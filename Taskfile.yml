version: "3"

vars:
  CLUSTER_NAME: '{{.CLUSTER_NAME | default "dfp-kind"}}'
  KIND_CONFIG: '{{.KIND_CONFIG | default "./infra/k8s/kind/kind-config.yaml"}}'
  KUSTOMIZE_DIR: '{{.KUSTOMIZE_DIR | default "./infra/k8s/kind/manifests"}}'
  NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
  PORT_FORWARD: '{{.PORT_FORWARD | default "1"}}'

tasks:
  tools:
    desc: Install pinned tooling via mise
    cmds:
      - mise install

  up:
    desc: Create kind cluster and deploy local services
    cmds:
      - task: kind:up
      - task: spark-operator:install
      - task: spark-image

  down:
    desc: Delete kind cluster
    cmds:
      - task: kind:down

  status:
    desc: Show kind cluster and namespace status
    cmds:
      - task: kind:status

  logs:
    desc: Tail logs for a deployment (APP=minio|redis|lakefs|mlflow|lakefs-postgres)
    vars:
      APP: '{{.APP | default ""}}'
    cmds:
      - task: kind:logs
        vars:
          APP: "{{.APP}}"

  port-forward:
    desc: Port-forward local service endpoints to localhost
    cmds:
      - bash ./tools/scripts/kind_portforward.sh start

  port-forward:stop:
    desc: Stop port-forwards started by this repo
    cmds:
      - bash ./tools/scripts/kind_portforward.sh stop

  port-forward:status:
    desc: Show port-forward status
    cmds:
      - bash ./tools/scripts/kind_portforward.sh status

  port-forward:reset:
    desc: Kill all kubectl port-forwards and restart them fresh
    cmds:
      - pkill -f "kubectl.*port-forward" || true
      - sleep 0.5
      - bash ./tools/scripts/kind_portforward.sh start
  
  kubeflow:port-forward:
    desc: Port-forward Kubeflow Pipelines UI to localhost:8080
    vars:
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
    cmds:
      - 'echo "Forwarding KFP UI to http://localhost:8080 (press Ctrl+C to stop)"'
      - 'kubectl port-forward -n "{{.KFP_NAMESPACE}}" svc/ml-pipeline-ui 8080:80'

  lakefs:keys:
    desc: Reset LakeFS and generate new credentials (updates .env file)
    cmds:
      - |
        set -e
        NAMESPACE="{{.NAMESPACE}}"
        
        echo "Resetting LakeFS to generate new credentials..."
        echo ""
        
        # Step 1: Reset the LakeFS postgres database
        echo "Step 1: Resetting LakeFS database..."
        kubectl exec -n "${NAMESPACE}" deployment/lakefs-postgres -- \
          psql -U lakefs -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;" 2>/dev/null || {
          echo "Warning: Could not reset database, continuing anyway..."
        }
        
        # Step 2: Restart LakeFS deployment to pick up fresh state
        echo "Step 2: Restarting LakeFS deployment..."
        kubectl rollout restart -n "${NAMESPACE}" deployment/lakefs
        kubectl rollout status -n "${NAMESPACE}" deployment/lakefs --timeout=120s
        
        # Give it a moment to initialize
        sleep 3
        
        # Step 3: Run setup to create new admin credentials
        echo "Step 3: Running LakeFS setup..."
        OUTPUT=$(kubectl exec -n "${NAMESPACE}" deployment/lakefs -- \
          lakefs setup --user-name admin 2>&1)
        
        echo "$OUTPUT" | tail -5
        
        # Parse credentials from output
        # Format is YAML: access_key_id: AKIAJ...  or  secret_access_key: ...
        ACCESS_KEY_ID=$(echo "$OUTPUT" | grep "access_key_id:" | awk '{print $2}' | head -1)
        SECRET_ACCESS_KEY=$(echo "$OUTPUT" | grep "secret_access_key:" | awk '{print $2}' | head -1)
        
        if [ -z "$ACCESS_KEY_ID" ] || [ -z "$SECRET_ACCESS_KEY" ]; then
          echo ""
          echo "Could not parse credentials from output."
          echo "Full output:"
          echo "$OUTPUT"
          echo ""
          echo "Please manually copy the access_key_id and secret_access_key to .env"
          exit 1
        fi
        
        echo ""
        echo "New credentials generated!"
        echo "  Access Key ID:     $ACCESS_KEY_ID"
        echo "  Secret Access Key: ${SECRET_ACCESS_KEY:0:10}..."
        
        # Update .env file
        if [ -f .env ]; then
          if grep -q "^LAKEFS_ACCESS_KEY_ID=" .env; then
            sed -i '' "s|^LAKEFS_ACCESS_KEY_ID=.*|LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID|" .env
          else
            echo "LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID" >> .env
          fi
          
          if grep -q "^LAKEFS_SECRET_ACCESS_KEY=" .env; then
            sed -i '' "s|^LAKEFS_SECRET_ACCESS_KEY=.*|LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY|" .env
          else
            echo "LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY" >> .env
          fi
          
          echo ""
          echo ".env file updated with new LakeFS credentials"
        else
          echo "LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID" > .env
          echo "LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY" >> .env
          echo ".env file created with LakeFS credentials"
        fi
        
        echo ""
        echo "Done! LakeFS has been reset with new credentials."
        echo ""
        echo "NOTE: All existing repositories and data in LakeFS have been deleted."
        echo "You may need to restart port-forwards: task port-forward:reset"

  kfp:clear:
    desc: Clear and delete all Kubeflow pipelines, runs, experiments, and kronodroid pods/services
    cmds:
      - uv run --with kfp,kubernetes tools/scripts/clear_kfp.py
    env:
      NAMESPACE: "{{.NAMESPACE}}"

  kind:up:
    desc: Bootstrap kind + apply kustomize overlay
    env:
      CLUSTER_NAME: "{{.CLUSTER_NAME}}"
      NAMESPACE: "{{.NAMESPACE}}"
      KIND_CONFIG: "{{.KIND_CONFIG}}"
      KUSTOMIZE_DIR: "{{.KUSTOMIZE_DIR}}"
      PORT_FORWARD: "{{.PORT_FORWARD}}"
    cmds:
      - bash ./tools/scripts/kind_bootstrap.sh

  kind:down:
    desc: Delete kind cluster
    cmds:
      - kind delete cluster --name "{{.CLUSTER_NAME}}"

  kind:status:
    desc: Show deployments/pods/services in the dfp namespace
    cmds:
      - |
          if ! kind get clusters | grep -qx "{{.CLUSTER_NAME}}"; then
            echo "kind cluster '{{.CLUSTER_NAME}}' not found"
            exit 0
          fi
      - kubectl -n "{{.NAMESPACE}}" get deploy,po,svc

  kind:logs:
    desc: Tail logs for APP (deployment name)
    vars:
      APP: '{{.APP | default ""}}'
    cmds:
      - |
          if [ -z "{{.APP}}" ]; then
            echo "Set APP=... (minio|redis|lakefs|mlflow|lakefs-postgres)"
            kubectl -n "{{.NAMESPACE}}" get deployments
            exit 0
          fi
      - kubectl -n "{{.NAMESPACE}}" logs deployment/"{{.APP}}" -f --tail=200

  spark-image:build:
    desc: Build the dfp-spark Docker image
    vars:
      SPARK_IMAGE: '{{.SPARK_IMAGE | default "dfp-spark:latest"}}'
    cmds:
      - docker build -t "{{.SPARK_IMAGE}}" -f tools/docker/Dockerfile.spark .

  spark-image:load:
    desc: Load dfp-spark image into kind cluster
    vars:
      SPARK_IMAGE: '{{.SPARK_IMAGE | default "dfp-spark:latest"}}'
    cmds:
      - kind load docker-image "{{.SPARK_IMAGE}}" --name "{{.CLUSTER_NAME}}"

  spark-image:
    desc: Build and load dfp-spark image into kind
    cmds:
      - task: spark-image:build
      - task: spark-image:load

  spark-operator:install:
    desc: Install Spark Operator via Helm
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
      SPARK_OPERATOR_VALUES: '{{.SPARK_OPERATOR_VALUES | default "./infra/k8s/spark-operator/values.yaml"}}'
    cmds:
      - |
          if ! command -v helm >/dev/null 2>&1; then
            echo "helm is required. Install from https://helm.sh/docs/intro/install/" >&2
            exit 1
          fi
      - helm repo add spark-operator https://kubeflow.github.io/spark-operator 2>/dev/null || true
      - helm repo update spark-operator
      - |
          if helm status spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" >/dev/null 2>&1; then
            echo "Spark Operator already installed, upgrading..."
            helm upgrade spark-operator spark-operator/spark-operator \
              --namespace "{{.SPARK_OPERATOR_NAMESPACE}}" \
              --values "{{.SPARK_OPERATOR_VALUES}}"
          else
            echo "Installing Spark Operator..."
            helm install spark-operator spark-operator/spark-operator \
              --namespace "{{.SPARK_OPERATOR_NAMESPACE}}" \
              --create-namespace \
              --values "{{.SPARK_OPERATOR_VALUES}}"
          fi
      - kubectl -n "{{.SPARK_OPERATOR_NAMESPACE}}" rollout status deployment/spark-operator-controller --timeout=120s
      - echo "Spark Operator installed successfully"

  spark-operator:uninstall:
    desc: Uninstall Spark Operator
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
    cmds:
      - helm uninstall spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" || true
      - kubectl delete namespace "{{.SPARK_OPERATOR_NAMESPACE}}" --ignore-not-found

  spark-operator:status:
    desc: Show Spark Operator status
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
    cmds:
      - |
          if helm status spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" >/dev/null 2>&1; then
            echo "Spark Operator is installed"
            kubectl -n "{{.SPARK_OPERATOR_NAMESPACE}}" get pods
          else
            echo "Spark Operator is not installed"
          fi
      - echo ""
      - echo "SparkApplications in {{.NAMESPACE}}:"
      - kubectl -n "{{.NAMESPACE}}" get sparkapplications 2>/dev/null || echo "  (none or CRD not installed)"

  spark:logs:
    desc: Tail logs for the latest Spark job driver pod
    cmds:
      - |
        POD=$(kubectl get pods -n "{{.NAMESPACE}}" -l spark-role=driver --sort-by=.metadata.creationTimestamp -o name | tail -n 1)
        if [ -z "$POD" ]; then
          echo "No Spark driver pods found in namespace {{.NAMESPACE}}"
          exit 0
        fi
        POD_NAME=${POD#pod/}
        echo "Tailing logs for $POD_NAME..."
        kubectl logs -n "{{.NAMESPACE}}" "$POD" -f --tail=200
