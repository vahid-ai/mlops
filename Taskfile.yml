version: "3"

vars:
  CLUSTER_NAME: '{{.CLUSTER_NAME | default "dfp-kind"}}'
  KIND_CONFIG: '{{.KIND_CONFIG | default "./infra/k8s/kind/kind-config.yaml"}}'
  KUSTOMIZE_DIR: '{{.KUSTOMIZE_DIR | default "./infra/k8s/kind/manifests"}}'
  KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
  KFP_HOST: '{{.KFP_HOST | default "http://localhost:8080/pipeline"}}'
  NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
  PORT_FORWARD: '{{.PORT_FORWARD | default "1"}}'

tasks:
  tools:
    desc: Install pinned tooling via mise
    cmds:
      - mise install

  # ─────────────────────────────────────────────────────────────────────────────
  # Python Environment Management (uv)
  # ─────────────────────────────────────────────────────────────────────────────

  uv:sync:
    desc: Sync default Python environment (Python 3.12)
    cmds:
      - |
        echo "Syncing Python 3.12 environment..."
        uv sync --python 3.12
        echo ""
        echo "Environment ready. Run 'source .venv/bin/activate' or use 'uv run <command>'"

  uv:sync:dev:
    desc: Sync environment with dev dependencies (linting, testing)
    cmds:
      - |
        echo "Syncing Python 3.12 environment with dev dependencies..."
        uv sync --python 3.12 --group dev
        echo ""
        echo "Dev environment ready with pyrefly, torchfix, pytest"

  uv:sync:android:
    desc: Sync environment for ExecuTorch/Android development
    cmds:
      - |
        echo "Syncing Python 3.12 environment with ExecuTorch..."
        uv sync --python 3.12 --group android
        echo ""
        echo "Android/ExecuTorch environment ready"

  uv:sync:kubeflow:
    desc: Sync environment for Kubeflow pipeline development
    cmds:
      - |
        echo "Syncing Python 3.12 environment with Kubeflow dependencies..."
        uv sync --python 3.12 --group kubeflow
        echo ""
        echo "Kubeflow environment ready"

  uv:sync:all:
    desc: Sync environment with all dependency groups
    cmds:
      - |
        echo "Syncing Python 3.12 environment with all dependency groups..."
        uv sync --python 3.12 --all-groups
        echo ""
        echo "Full environment ready with all dependencies"

  uv:info:
    desc: Show Python environment information
    cmds:
      - |
        echo "=== Python Environment Info ==="
        echo ""
        echo "uv version:"
        uv --version
        echo ""
        echo "Project Python constraint:"
        grep "requires-python" pyproject.toml || echo "Not specified"
        echo ""
        echo ".python-version file:"
        cat .python-version 2>/dev/null || echo "Not found"
        echo ""
        echo "Current virtual environment:"
        if [ -d ".venv" ]; then
          .venv/bin/python --version
          PKGCOUNT=$(uv pip list 2>/dev/null | wc -l | tr -d ' ')
          echo "Packages: $((PKGCOUNT - 2)) installed"
        else
          echo "No .venv found - run 'task uv:sync' to create"
        fi
        echo ""
        echo "Key package versions:"
        uv run python -c "import torch; print(f'  PyTorch: {torch.__version__}')" 2>/dev/null || echo "  PyTorch: not installed"
        uv run python -c "import pandas; print(f'  Pandas: {pandas.__version__}')" 2>/dev/null || echo "  Pandas: not installed"
        uv run python -c "from executorch.exir import to_edge; print('  ExecuTorch: installed')" 2>/dev/null || echo "  ExecuTorch: not installed (run task uv:sync:android)"

  uv:lock:
    desc: Update uv.lock file for reproducible installs
    cmds:
      - |
        echo "Updating uv.lock..."
        uv lock
        echo "Lock file updated. Commit uv.lock for reproducible builds."

  uv:clean:
    desc: Remove virtual environment and cache
    cmds:
      - |
        echo "Removing .venv..."
        rm -rf .venv
        echo "Removing .uv-cache..."
        rm -rf .uv-cache
        echo "Clean complete. Run 'task uv:sync' to recreate."

  bazel:install:
    desc: Install Bazel via Bazelisk (cross-platform version manager)
    vars:
      BAZELISK_VERSION: '{{.BAZELISK_VERSION | default "v1.25.0"}}'
      INSTALL_DIR: '{{.INSTALL_DIR | default "/usr/local/bin"}}'
    cmds:
      - |
        set -e

        # Detect OS
        OS="$(uname -s | tr '[:upper:]' '[:lower:]')"
        case "$OS" in
          darwin) OS="darwin" ;;
          linux) OS="linux" ;;
          mingw*|msys*|cygwin*) OS="windows" ;;
          *)
            echo "Unsupported OS: $OS"
            exit 1
            ;;
        esac

        # Detect architecture
        ARCH="$(uname -m)"
        case "$ARCH" in
          x86_64|amd64) ARCH="amd64" ;;
          aarch64|arm64) ARCH="arm64" ;;
          *)
            echo "Unsupported architecture: $ARCH"
            exit 1
            ;;
        esac

        echo "Detected platform: ${OS}-${ARCH}"
        echo "Installing Bazelisk {{.BAZELISK_VERSION}}..."

        # Set binary name and extension
        if [ "$OS" = "windows" ]; then
          BINARY_NAME="bazelisk-${OS}-${ARCH}.exe"
          TARGET_NAME="bazel.exe"
        else
          BINARY_NAME="bazelisk-${OS}-${ARCH}"
          TARGET_NAME="bazel"
        fi

        # Download URL
        URL="https://github.com/bazelbuild/bazelisk/releases/download/{{.BAZELISK_VERSION}}/${BINARY_NAME}"

        echo "Downloading from: $URL"

        # Create temp directory
        TEMP_DIR=$(mktemp -d)
        trap "rm -rf $TEMP_DIR" EXIT

        # Download Bazelisk
        if command -v curl >/dev/null 2>&1; then
          curl -fsSL -o "${TEMP_DIR}/${TARGET_NAME}" "$URL"
        elif command -v wget >/dev/null 2>&1; then
          wget -q -O "${TEMP_DIR}/${TARGET_NAME}" "$URL"
        else
          echo "Error: curl or wget required"
          exit 1
        fi

        # Make executable (not needed on Windows)
        if [ "$OS" != "windows" ]; then
          chmod +x "${TEMP_DIR}/${TARGET_NAME}"
        fi

        # Install to target directory
        INSTALL_PATH="{{.INSTALL_DIR}}/${TARGET_NAME}"

        echo "Installing to: $INSTALL_PATH"

        # Check if we need sudo
        if [ -w "{{.INSTALL_DIR}}" ]; then
          mv "${TEMP_DIR}/${TARGET_NAME}" "$INSTALL_PATH"
        else
          echo "Requires sudo to install to {{.INSTALL_DIR}}"
          sudo mv "${TEMP_DIR}/${TARGET_NAME}" "$INSTALL_PATH"
        fi

        # Verify installation
        echo ""
        echo "Bazelisk installed successfully!"
        echo ""
        "$INSTALL_PATH" --version
        echo ""
        echo "Bazelisk will automatically download the correct Bazel version"
        echo "when you run 'bazel' based on .bazelversion file (if present)"

  bazel:version:
    desc: Show installed Bazel/Bazelisk version
    cmds:
      - |
        if command -v bazel >/dev/null 2>&1; then
          echo "Bazelisk version:"
          bazel --version
          echo ""
          echo "To see actual Bazel version, run: bazel version"
        else
          echo "Bazel/Bazelisk not found. Run: task bazel:install"
        fi

  up:
    desc: Create kind cluster and deploy local services
    cmds:
      - task: kind:up
      - task: spark-operator:install
      - task: spark-image

  up:full:
    desc: Complete from-scratch setup - creates cluster, configures everything, and starts port-forwarding
    cmds:
      - |
        set -e
        echo "=== Starting complete MLOps platform setup ==="
        echo ""
        
        echo "Step 1/8: Tearing down any existing cluster..."
        kind delete cluster --name "{{.CLUSTER_NAME}}" || true
        task port-forward:stop || true
        sleep 2
        
        echo ""
        echo "Step 2/8: Creating kind cluster and deploying services..."
        task kind:up
        
        echo ""
        echo "Step 3/8: Installing Kubeflow Pipelines..."
        PIPELINE_VERSION=2.15.0
        kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
        kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io || true
        kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-emissary?ref=$PIPELINE_VERSION"
        
        echo ""
        echo "Step 4/8: Installing Spark Operator..."
        task spark-operator:install || {
          echo "Spark Operator install timed out, waiting for pods to be ready..."
          kubectl -n spark-operator wait --for=condition=ready pod -l app.kubernetes.io/name=spark-operator --timeout=300s || true
        }
        
        echo ""
        echo "Step 5/8: Building and loading Spark image..."
        task spark-image

        echo ""
        echo "Step 5b/8: Building and loading LakeFS component image..."
        task lakefs-component-image
        
        echo ""
        echo "Step 6/8: Waiting for all pods to be ready..."
        echo "Waiting for dfp namespace pods..."
        kubectl wait --for=condition=ready pod -l app=minio -n "{{.NAMESPACE}}" --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=lakefs -n "{{.NAMESPACE}}" --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=mlflow -n "{{.NAMESPACE}}" --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=redis -n "{{.NAMESPACE}}" --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=lakefs-postgres -n "{{.NAMESPACE}}" --timeout=180s || true
        
        echo "Waiting for kubeflow namespace pods (this may take a few minutes)..."
        # Wait for critical KFP pods with retries
        for i in 1 2 3; do
          kubectl wait --for=condition=ready pod -l app=ml-pipeline -n kubeflow --timeout=180s && break || sleep 10
        done
        kubectl wait --for=condition=ready pod -l app=ml-pipeline-ui -n kubeflow --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=seaweedfs -n kubeflow --timeout=180s || true
        kubectl wait --for=condition=ready pod -l app=mysql -n kubeflow --timeout=180s || true
        echo "Core KFP pods ready!"
        
        echo ""
        echo "Step 7/8: Generating LakeFS credentials..."
        task lakefs:keys
        
        echo ""
        echo "Step 8/8: Starting port-forwarding..."
        task port-forward:reset
        
        echo ""
        echo "====================================================="
        echo "  MLOps Platform Setup Complete!"
        echo "====================================================="
        echo ""
        echo "Services available at:"
        echo "  - Kubeflow Pipelines UI: http://localhost:8081"
        echo "  - Kubeflow Pipelines API: http://localhost:8080"
        echo "  - LakeFS:                 http://localhost:8000"
        echo "  - MinIO Console:          http://localhost:19001"
        echo "  - MinIO API:              http://localhost:19000"
        echo "  - MLflow:                 http://localhost:5050"
        echo "  - Redis:                  localhost:16379"
        echo ""
        echo "LakeFS credentials have been saved to .env"
        echo ""
        echo "You're ready to run Kubeflow pipelines!"


  down:
    desc: Delete kind cluster
    cmds:
      - task: kind:down

  status:
    desc: Show kind cluster and namespace status
    cmds:
      - task: kind:status

  logs:
    desc: Tail logs for a deployment (APP=minio|redis|lakefs|mlflow|lakefs-postgres)
    vars:
      APP: '{{.APP | default ""}}'
    cmds:
      - task: kind:logs
        vars:
          APP: "{{.APP}}"

  port-forward:
    desc: Port-forward local service endpoints to localhost
    cmds:
      - bash ./tools/scripts/kind_portforward.sh start

  port-forward:stop:
    desc: Stop port-forwards started by this repo
    cmds:
      - bash ./tools/scripts/kind_portforward.sh stop

  port-forward:status:
    desc: Show port-forward status
    cmds:
      - bash ./tools/scripts/kind_portforward.sh status

  port-forward:reset:
    desc: Kill all kubectl port-forwards and restart them fresh
    cmds:
      - pkill -f "kubectl.*port-forward" || true
      - sleep 0.5
      - bash ./tools/scripts/kind_portforward.sh start
  
  kubeflow:port-forward:
    desc: Port-forward Kubeflow Pipelines UI to localhost:8080
    vars:
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
    cmds:
      - 'echo "Forwarding KFP UI to http://localhost:8080 (press Ctrl+C to stop)"'
      - 'kubectl port-forward -n "{{.KFP_NAMESPACE}}" svc/ml-pipeline-ui 8080:80'

  lakefs:keys:
    desc: Reset LakeFS and generate new credentials (updates .env file)
    cmds:
      - |
        set -e
        NAMESPACE="{{.NAMESPACE}}"
        
        echo "Resetting LakeFS to generate new credentials..."
        echo ""
        
        # Step 1: Reset the LakeFS postgres database
        echo "Step 1: Resetting LakeFS database..."
        kubectl exec -n "${NAMESPACE}" deployment/lakefs-postgres -- \
          psql -U lakefs -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;" 2>/dev/null || {
          echo "Warning: Could not reset database, continuing anyway..."
        }
        
        # Step 2: Restart LakeFS deployment to pick up fresh state
        echo "Step 2: Restarting LakeFS deployment..."
        kubectl rollout restart -n "${NAMESPACE}" deployment/lakefs
        kubectl rollout status -n "${NAMESPACE}" deployment/lakefs --timeout=120s
        
        # Give it a moment to initialize
        sleep 3
        
        # Step 3: Run setup to create new admin credentials
        echo "Step 3: Running LakeFS setup..."
        OUTPUT=$(kubectl exec -n "${NAMESPACE}" deployment/lakefs -- \
          lakefs setup --user-name admin 2>&1)
        
        echo "$OUTPUT" | tail -5
        
        # Parse credentials from output
        # Format is YAML: access_key_id: AKIAJ...  or  secret_access_key: ...
        ACCESS_KEY_ID=$(echo "$OUTPUT" | grep "access_key_id:" | awk '{print $2}' | head -1)
        SECRET_ACCESS_KEY=$(echo "$OUTPUT" | grep "secret_access_key:" | awk '{print $2}' | head -1)
        
        if [ -z "$ACCESS_KEY_ID" ] || [ -z "$SECRET_ACCESS_KEY" ]; then
          echo ""
          echo "Could not parse credentials from output."
          echo "Full output:"
          echo "$OUTPUT"
          echo ""
          echo "Please manually copy the access_key_id and secret_access_key to .env"
          exit 1
        fi
        
        echo ""
        echo "New credentials generated!"
        echo "  Access Key ID:     $ACCESS_KEY_ID"
        echo "  Secret Access Key: ${SECRET_ACCESS_KEY:0:10}..."
        
        # Update .env file
        if [ -f .env ]; then
          if grep -q "^LAKEFS_ACCESS_KEY_ID=" .env; then
            sed -i '' "s|^LAKEFS_ACCESS_KEY_ID=.*|LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID|" .env
          else
            echo "LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID" >> .env
          fi
          
          if grep -q "^LAKEFS_SECRET_ACCESS_KEY=" .env; then
            sed -i '' "s|^LAKEFS_SECRET_ACCESS_KEY=.*|LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY|" .env
          else
            echo "LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY" >> .env
          fi
          
          echo ""
          echo ".env file updated with new LakeFS credentials"
        else
          echo "LAKEFS_ACCESS_KEY_ID=$ACCESS_KEY_ID" > .env
          echo "LAKEFS_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY" >> .env
          echo ".env file created with LakeFS credentials"
        fi
        
        echo ""
        echo "Done! LakeFS has been reset with new credentials."
        echo ""
        echo "NOTE: All existing repositories and data in LakeFS have been deleted."
        echo "You may need to restart port-forwards: task port-forward:reset"

  kfp:clear:
    desc: Clear and delete all Kubeflow pipelines, runs, experiments, and kronodroid pods/services
    cmds:
      - uv run --with kfp,kubernetes tools/scripts/clear_kfp.py
    env:
      KFP_CLEAR_ALL_WORKFLOWS: "1"
      KFP_HOST: "{{.KFP_HOST}}"
      KFP_NAMESPACE: "{{.KFP_NAMESPACE}}"
      NAMESPACE: "{{.NAMESPACE}}"

  kfp:reset:
    desc: Delete all Argo workflow pods and resources in kubeflow and dfp namespaces
    vars:
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
      NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
    cmds:
      - |
        echo "Deleting workflow pods in {{.KFP_NAMESPACE}} namespace..."
        kubectl delete pods -n {{.KFP_NAMESPACE}} -l workflows.argoproj.io/workflow --ignore-not-found=true
        
        echo "Deleting completed pods in {{.KFP_NAMESPACE}} namespace..."
        kubectl delete pods -n {{.KFP_NAMESPACE}} --field-selector=status.phase=Succeeded --ignore-not-found=true
        
        echo "Deleting failed pods in {{.KFP_NAMESPACE}} namespace..."
        kubectl delete pods -n {{.KFP_NAMESPACE}} --field-selector=status.phase=Failed --ignore-not-found=true
        
        echo "Deleting all Argo workflows in {{.KFP_NAMESPACE}} namespace..."
        kubectl delete workflows -n {{.KFP_NAMESPACE}} --all --ignore-not-found=true
                
        echo "KFP reset complete."

  kind:up:
    desc: Bootstrap kind + apply kustomize overlay
    env:
      CLUSTER_NAME: "{{.CLUSTER_NAME}}"
      NAMESPACE: "{{.NAMESPACE}}"
      KIND_CONFIG: "{{.KIND_CONFIG}}"
      KUSTOMIZE_DIR: "{{.KUSTOMIZE_DIR}}"
      PORT_FORWARD: "{{.PORT_FORWARD}}"
    cmds:
      - bash ./tools/scripts/kind_bootstrap.sh

  kind:down:
    desc: Delete kind cluster
    cmds:
      - kind delete cluster --name "{{.CLUSTER_NAME}}"

  kind:status:
    desc: Show deployments/pods/services across all namespaces and service URLs
    vars:
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
    cmds:
      - |
          if ! kind get clusters | grep -qx "{{.CLUSTER_NAME}}"; then
            echo "kind cluster '{{.CLUSTER_NAME}}' not found"
            exit 0
          fi
          
          echo "=============================================="
          echo "  Namespace: {{.NAMESPACE}} (Core Services)"
          echo "=============================================="
          kubectl -n "{{.NAMESPACE}}" get deploy,po,svc
          
          echo ""
          echo "=============================================="
          echo "  Namespace: {{.KFP_NAMESPACE}} (Kubeflow Pipelines)"
          echo "=============================================="
          kubectl -n "{{.KFP_NAMESPACE}}" get deploy,po,svc 2>/dev/null || echo "  (namespace not found)"
          
          echo ""
          echo "=============================================="
          echo "  Namespace: {{.SPARK_OPERATOR_NAMESPACE}} (Spark Operator)"
          echo "=============================================="
          kubectl -n "{{.SPARK_OPERATOR_NAMESPACE}}" get deploy,po,svc 2>/dev/null || echo "  (namespace not found)"
          
          echo ""
          echo "=============================================="
          echo "  SparkApplications in {{.NAMESPACE}}"
          echo "=============================================="
          kubectl -n "{{.NAMESPACE}}" get sparkapplications 2>/dev/null || echo "  (none or CRD not installed)"
          
          echo ""
          echo "=============================================="
          echo "  Service Access URLs / Port-Forwards"
          echo "=============================================="
          echo ""
          echo "  DFP Namespace Services:"
          echo "    MinIO Console:          http://localhost:19001  (admin/minioadmin)"
          echo "    MinIO API:              http://localhost:19000"
          echo "    LakeFS:                 http://localhost:8000"
          echo "    MLflow:                 http://localhost:5050"
          echo "    Redis:                  localhost:16379"
          echo ""
          echo "  Kubeflow Namespace Services:"
          echo "    Kubeflow Pipelines UI:  http://localhost:8081"
          echo "    Kubeflow Pipelines API: http://localhost:8080"
          echo ""
          echo "  Run 'task port-forward:status' to check if port-forwards are active."
          echo ""

  kind:logs:
    desc: Tail logs for APP (deployment name)
    vars:
      APP: '{{.APP | default ""}}'
    cmds:
      - |
          if [ -z "{{.APP}}" ]; then
            echo "Set APP=... (minio|redis|lakefs|mlflow|lakefs-postgres)"
            kubectl -n "{{.NAMESPACE}}" get deployments
            exit 0
          fi
      - kubectl -n "{{.NAMESPACE}}" logs deployment/"{{.APP}}" -f --tail=200

  spark-image:build:
    desc: Build the dfp-spark Docker image
    vars:
      SPARK_IMAGE: '{{.SPARK_IMAGE | default "dfp-spark:latest"}}'
    cmds:
      - docker build -t "{{.SPARK_IMAGE}}" -f tools/docker/Dockerfile.spark .

  spark-image:load:
    desc: Load dfp-spark image into kind cluster
    vars:
      SPARK_IMAGE: '{{.SPARK_IMAGE | default "dfp-spark:latest"}}'
    cmds:
      - kind load docker-image "{{.SPARK_IMAGE}}" --name "{{.CLUSTER_NAME}}"

  spark-image:
    desc: Build and load dfp-spark image into kind
    cmds:
      - task: spark-image:build
      - task: spark-image:load

  lakefs-component-image:build:
    desc: Build the dfp-lakefs-component Docker image
    vars:
      LAKEFS_COMPONENT_IMAGE: '{{.LAKEFS_COMPONENT_IMAGE | default "dfp-lakefs-component:v2"}}'
    cmds:
      - docker build -t "{{.LAKEFS_COMPONENT_IMAGE}}" -f tools/docker/Dockerfile.lakefs_component .

  lakefs-component-image:load:
    desc: Load dfp-lakefs-component image into kind cluster
    vars:
      LAKEFS_COMPONENT_IMAGE: '{{.LAKEFS_COMPONENT_IMAGE | default "dfp-lakefs-component:v2"}}'
    cmds:
      - kind load docker-image "{{.LAKEFS_COMPONENT_IMAGE}}" --name "{{.CLUSTER_NAME}}"

  lakefs-component-image:
    desc: Build and load dfp-lakefs-component image into kind
    cmds:
      - task: lakefs-component-image:build
      - task: lakefs-component-image:load

  spark-operator:install:
    desc: Install Spark Operator via Helm
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
      SPARK_OPERATOR_VALUES: '{{.SPARK_OPERATOR_VALUES | default "./infra/k8s/spark-operator/values.yaml"}}'
    cmds:
      - |
          if ! command -v helm >/dev/null 2>&1; then
            echo "helm is required. Install from https://helm.sh/docs/intro/install/" >&2
            exit 1
          fi
      - helm repo add spark-operator https://kubeflow.github.io/spark-operator 2>/dev/null || true
      - helm repo update spark-operator
      - |
          if helm status spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" >/dev/null 2>&1; then
            echo "Spark Operator already installed, upgrading..."
            helm upgrade spark-operator spark-operator/spark-operator \
              --namespace "{{.SPARK_OPERATOR_NAMESPACE}}" \
              --values "{{.SPARK_OPERATOR_VALUES}}"
          else
            echo "Installing Spark Operator..."
            helm install spark-operator spark-operator/spark-operator \
              --namespace "{{.SPARK_OPERATOR_NAMESPACE}}" \
              --create-namespace \
              --values "{{.SPARK_OPERATOR_VALUES}}"
          fi
      - kubectl -n "{{.SPARK_OPERATOR_NAMESPACE}}" rollout status deployment/spark-operator-controller --timeout=300s
      - echo "Spark Operator installed successfully"

  spark-operator:uninstall:
    desc: Uninstall Spark Operator
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
    cmds:
      - helm uninstall spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" || true
      - kubectl delete namespace "{{.SPARK_OPERATOR_NAMESPACE}}" --ignore-not-found

  spark-operator:status:
    desc: Show Spark Operator status
    vars:
      SPARK_OPERATOR_NAMESPACE: '{{.SPARK_OPERATOR_NAMESPACE | default "spark-operator"}}'
    cmds:
      - |
          if helm status spark-operator -n "{{.SPARK_OPERATOR_NAMESPACE}}" >/dev/null 2>&1; then
            echo "Spark Operator is installed"
            kubectl -n "{{.SPARK_OPERATOR_NAMESPACE}}" get pods
          else
            echo "Spark Operator is not installed"
          fi
      - echo ""
      - echo "SparkApplications in {{.NAMESPACE}}:"
      - kubectl -n "{{.NAMESPACE}}" get sparkapplications 2>/dev/null || echo "  (none or CRD not installed)"

  spark:logs:
    desc: Tail logs for the latest Spark job driver pod
    cmds:
      - |
        POD=$(kubectl get pods -n "{{.NAMESPACE}}" -l spark-role=driver --sort-by=.metadata.creationTimestamp -o name | tail -n 1)
        if [ -z "$POD" ]; then
          echo "No Spark driver pods found in namespace {{.NAMESPACE}}"
          exit 0
        fi
        POD_NAME=${POD#pod/}
        echo "Tailing logs for $POD_NAME..."
        kubectl logs -n "{{.NAMESPACE}}" "$POD" -f --tail=200

  # ===========================================================================
  # Model Validation Tasks
  # ===========================================================================

  model:validate:
    desc: Run full model validation (PyTorch → ExecuTorch → Android → Compare)
    vars:
      MODEL_NAME: '{{.MODEL_NAME | default "kronodroid_autoencoder"}}'
      MODEL_VERSION: '{{.MODEL_VERSION | default "latest"}}'
      TEST_DATA: '{{.TEST_DATA | default "./data/test_samples.npy"}}'
      TOLERANCE: '{{.TOLERANCE | default "0.01"}}'
      MLFLOW_URI: '{{.MLFLOW_URI | default "http://localhost:5000"}}'
    env:
      # MinIO/S3 credentials for MLflow artifact access
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://localhost:9000
    cmds:
      - |
        uv run python tools/model_validation/validate_model.py \
          --model-name "{{.MODEL_NAME}}" \
          --model-version "{{.MODEL_VERSION}}" \
          --test-data "{{.TEST_DATA}}" \
          --tolerance "{{.TOLERANCE}}" \
          --mlflow-uri "{{.MLFLOW_URI}}"

  model:validate:no-android:
    desc: Run model validation without Android emulator (host-only comparison)
    vars:
      MODEL_NAME: '{{.MODEL_NAME | default "kronodroid_autoencoder"}}'
      MODEL_VERSION: '{{.MODEL_VERSION | default "latest"}}'
      TEST_DATA: '{{.TEST_DATA | default "./data/test_samples.npy"}}'
      TOLERANCE: '{{.TOLERANCE | default "0.01"}}'
    env:
      # MinIO/S3 credentials for MLflow artifact access
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://localhost:9000
    cmds:
      - |
        uv run python tools/model_validation/validate_model.py \
          --model-name "{{.MODEL_NAME}}" \
          --model-version "{{.MODEL_VERSION}}" \
          --test-data "{{.TEST_DATA}}" \
          --tolerance "{{.TOLERANCE}}" \
          --skip-android

  model:export:
    desc: Export PyTorch model from MLflow to ExecuTorch format
    vars:
      MODEL_NAME: '{{.MODEL_NAME}}'
      MODEL_VERSION: '{{.MODEL_VERSION | default "latest"}}'
      OUTPUT: '{{.OUTPUT | default "./models/model.pte"}}'
      BACKEND: '{{.BACKEND | default "xnnpack"}}'
      QUANTIZATION: '{{.QUANTIZATION | default "none"}}'
    env:
      # MinIO/S3 credentials for MLflow artifact access
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://localhost:9000
    cmds:
      - |
        if [ -z "{{.MODEL_NAME}}" ]; then
          echo "Error: MODEL_NAME is required"
          echo "Usage: task model:export MODEL_NAME=my_model"
          exit 1
        fi
        uv run python -m runtimes.executorch.export.cli export \
          --model-name "{{.MODEL_NAME}}" \
          --model-version "{{.MODEL_VERSION}}" \
          --output "{{.OUTPUT}}" \
          --backend "{{.BACKEND}}" \
          --quantization "{{.QUANTIZATION}}" \
          --register

  model:export:list-quantization:
    desc: List available quantization configurations
    cmds:
      - uv run python -m runtimes.executorch.export.cli list-quantization

  model:export:test:
    desc: Test ExecuTorch export pipeline with a simple model (no MLflow required)
    cmds:
      - uv run tools/model_validation/test_export.py

  model:compare:
    desc: Compare inference results from different runtimes
    vars:
      PYTORCH_RESULTS: '{{.PYTORCH_RESULTS}}'
      EXECUTORCH_RESULTS: '{{.EXECUTORCH_RESULTS | default ""}}'
      ANDROID_RESULTS: '{{.ANDROID_RESULTS | default ""}}'
      TOLERANCE: '{{.TOLERANCE | default "0.01"}}'
    cmds:
      - |
        ARGS="--pytorch-results {{.PYTORCH_RESULTS}} --tolerance {{.TOLERANCE}}"
        if [ -n "{{.EXECUTORCH_RESULTS}}" ]; then
          ARGS="$ARGS --executorch-results {{.EXECUTORCH_RESULTS}}"
        fi
        if [ -n "{{.ANDROID_RESULTS}}" ]; then
          ARGS="$ARGS --android-results {{.ANDROID_RESULTS}}"
        fi
        uv run python tools/model_validation/compare_results.py $ARGS

  # ===========================================================================
  # Android Build Tasks
  # ===========================================================================

  android:build:
    desc: Build Android apps with Bazel
    vars:
      CONFIG: '{{.CONFIG | default "android_debug"}}'
    cmds:
      - bazel build --config={{.CONFIG}} //apps/android/...

  android:build:main:
    desc: Build main Android app
    vars:
      CONFIG: '{{.CONFIG | default "android_debug"}}'
    cmds:
      - bazel build --config={{.CONFIG}} //apps/android/main_app:main_app

  android:build:runtime:
    desc: Build runtime comparison app
    vars:
      CONFIG: '{{.CONFIG | default "android_debug"}}'
    cmds:
      - bazel build --config={{.CONFIG}} //apps/android/runtime_comparison_app:runtime_comparison_app

  android:install:
    desc: Build and install Android app on connected device/emulator
    vars:
      APP: '{{.APP | default "main_app"}}'
    cmds:
      - bazel mobile-install //apps/android/{{.APP}}:{{.APP}}

  android:test:
    desc: Run Android unit tests (Robolectric)
    cmds:
      - bazel test --config=robolectric //apps/android/...

  android:test:device:
    desc: Run Android instrumentation tests on device/emulator
    cmds:
      - |
        echo "Building and installing test APK..."
        bazel build --config=android_debug //apps/android/runtime_comparison_app:runtime_comparison_app

        APK="bazel-bin/apps/android/runtime_comparison_app/runtime_comparison_app.apk"
        if [ ! -f "$APK" ]; then
          echo "Error: APK not found at $APK"
          exit 1
        fi

        echo "Installing APK..."
        adb install -r "$APK"

        echo "Running instrumentation test..."
        adb shell am instrument -w \
          -e class com.dfp.runtimeapp.ModelAccuracyTest \
          com.dfp.runtimeapp.test/androidx.test.runner.AndroidJUnitRunner

  android:emulator:start:
    desc: Start Android emulator
    vars:
      AVD: '{{.AVD | default "Pixel_6_API_34"}}'
    cmds:
      - |
        if adb devices | grep -q "emulator"; then
          echo "Emulator already running"
        else
          echo "Starting emulator: {{.AVD}}"
          emulator -avd "{{.AVD}}" -no-snapshot-load &
          echo "Waiting for emulator to boot..."
          adb wait-for-device
          adb shell 'while [[ -z $(getprop sys.boot_completed) ]]; do sleep 1; done'
          echo "Emulator ready!"
        fi

  android:emulator:stop:
    desc: Stop Android emulator
    cmds:
      - adb -s emulator-5554 emu kill || true

  android:results:pull:
    desc: Pull benchmark results from Android device
    vars:
      OUTPUT_DIR: '{{.OUTPUT_DIR | default "./results"}}'
    cmds:
      - |
        mkdir -p "{{.OUTPUT_DIR}}"
        echo "Pulling benchmark results..."
        adb pull /sdcard/Android/data/com.dfp.runtimeapp/files/ "{{.OUTPUT_DIR}}/" || {
          echo "Could not pull results. Make sure the app has run and generated results."
          exit 1
        }
        echo "Results saved to {{.OUTPUT_DIR}}/"
        ls -la "{{.OUTPUT_DIR}}/"

  # ===========================================================================
  # Secrets Management Tasks
  # ===========================================================================

  secrets:sync:
    desc: Sync secrets from dfp namespace to kubeflow namespace (and vice versa)
    vars:
      DFP_NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
    cmds:
      - |
        set -e
        echo "=== Syncing secrets between namespaces ==="
        echo ""

        # Load credentials from .env file
        if [ -f .env ]; then
          source .env
        fi

        # MinIO credentials (hardcoded in manifests)
        MINIO_ACCESS_KEY="${MINIO_ACCESS_KEY_ID:-minioadmin}"
        MINIO_SECRET_KEY="${MINIO_SECRET_ACCESS_KEY:-minioadmin}"

        # LakeFS credentials (from .env or environment)
        LAKEFS_ACCESS_KEY="${LAKEFS_ACCESS_KEY_ID:-}"
        LAKEFS_SECRET_KEY="${LAKEFS_SECRET_ACCESS_KEY:-}"

        if [ -z "$LAKEFS_ACCESS_KEY" ] || [ -z "$LAKEFS_SECRET_KEY" ]; then
          echo "ERROR: LAKEFS_ACCESS_KEY_ID and LAKEFS_SECRET_ACCESS_KEY must be set"
          echo "Run 'task lakefs:keys' to generate new credentials"
          exit 1
        fi

        echo "Creating/updating secrets in {{.DFP_NAMESPACE}} namespace..."

        # Create minio-credentials in dfp namespace
        kubectl create secret generic minio-credentials \
          --namespace="{{.DFP_NAMESPACE}}" \
          --from-literal=MINIO_ACCESS_KEY_ID="$MINIO_ACCESS_KEY" \
          --from-literal=MINIO_SECRET_ACCESS_KEY="$MINIO_SECRET_KEY" \
          --dry-run=client -o yaml | kubectl apply -f -

        # Create lakefs-credentials in dfp namespace
        kubectl create secret generic lakefs-credentials \
          --namespace="{{.DFP_NAMESPACE}}" \
          --from-literal=LAKEFS_ACCESS_KEY_ID="$LAKEFS_ACCESS_KEY" \
          --from-literal=LAKEFS_SECRET_ACCESS_KEY="$LAKEFS_SECRET_KEY" \
          --dry-run=client -o yaml | kubectl apply -f -

        echo ""
        echo "Creating/updating secrets in {{.KFP_NAMESPACE}} namespace..."

        # Create minio-credentials in kubeflow namespace
        kubectl create secret generic minio-credentials \
          --namespace="{{.KFP_NAMESPACE}}" \
          --from-literal=MINIO_ACCESS_KEY_ID="$MINIO_ACCESS_KEY" \
          --from-literal=MINIO_SECRET_ACCESS_KEY="$MINIO_SECRET_KEY" \
          --dry-run=client -o yaml | kubectl apply -f -

        # Create lakefs-credentials in kubeflow namespace
        kubectl create secret generic lakefs-credentials \
          --namespace="{{.KFP_NAMESPACE}}" \
          --from-literal=LAKEFS_ACCESS_KEY_ID="$LAKEFS_ACCESS_KEY" \
          --from-literal=LAKEFS_SECRET_ACCESS_KEY="$LAKEFS_SECRET_KEY" \
          --dry-run=client -o yaml | kubectl apply -f -

        echo ""
        echo "=== Secrets synced successfully ==="
        task secrets:verify

  secrets:verify:
    desc: Verify that required secrets exist in both dfp and kubeflow namespaces
    vars:
      DFP_NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
      KFP_NAMESPACE: '{{.KFP_NAMESPACE | default "kubeflow"}}'
    cmds:
      - |
        echo "=== Verifying secrets ==="
        echo ""

        ERRORS=0

        for NS in "{{.DFP_NAMESPACE}}" "{{.KFP_NAMESPACE}}"; do
          echo "Namespace: $NS"

          # Check minio-credentials
          if kubectl get secret minio-credentials -n "$NS" >/dev/null 2>&1; then
            KEYS=$(kubectl get secret minio-credentials -n "$NS" -o jsonpath='{.data}' | jq -r 'keys | join(", ")')
            echo "  ✓ minio-credentials: $KEYS"
          else
            echo "  ✗ minio-credentials: NOT FOUND"
            ERRORS=$((ERRORS + 1))
          fi

          # Check lakefs-credentials
          if kubectl get secret lakefs-credentials -n "$NS" >/dev/null 2>&1; then
            KEYS=$(kubectl get secret lakefs-credentials -n "$NS" -o jsonpath='{.data}' | jq -r 'keys | join(", ")')
            echo "  ✓ lakefs-credentials: $KEYS"
          else
            echo "  ✗ lakefs-credentials: NOT FOUND"
            ERRORS=$((ERRORS + 1))
          fi

          echo ""
        done

        if [ $ERRORS -gt 0 ]; then
          echo "ERROR: $ERRORS missing secrets. Run 'task secrets:sync' to create them."
          exit 1
        fi

        echo "All secrets verified successfully!"

  secrets:show:
    desc: Show decoded secret values (for debugging)
    vars:
      SECRET: '{{.SECRET | default ""}}'
      NAMESPACE: '{{.NAMESPACE | default "dfp"}}'
    cmds:
      - |
        if [ -z "{{.SECRET}}" ]; then
          echo "Usage: task secrets:show SECRET=<secret-name> [NAMESPACE=<namespace>]"
          echo ""
          echo "Available secrets in {{.NAMESPACE}}:"
          kubectl get secrets -n "{{.NAMESPACE}}" --no-headers | awk '{print "  - "$1}'
          exit 0
        fi

        echo "Secret: {{.SECRET}} (namespace: {{.NAMESPACE}})"
        echo ""
        kubectl get secret "{{.SECRET}}" -n "{{.NAMESPACE}}" -o json | \
          jq -r '.data | to_entries[] | "\(.key): \(.value | @base64d)"'

  # ===========================================================================
  # Pipeline Docker Image Tasks
  # ===========================================================================

  autoencoder-image:build:
    desc: Build the autoencoder training Docker image
    vars:
      IMAGE_TAG: '{{.IMAGE_TAG | default "v8"}}'
    cmds:
      - docker build -t "dfp-autoencoder-train:{{.IMAGE_TAG}}" -f tools/docker/Dockerfile.autoencoder_train .

  autoencoder-image:load:
    desc: Load autoencoder training image into kind cluster
    vars:
      IMAGE_TAG: '{{.IMAGE_TAG | default "v8"}}'
    cmds:
      - kind load docker-image "dfp-autoencoder-train:{{.IMAGE_TAG}}" --name "{{.CLUSTER_NAME}}"

  autoencoder-image:
    desc: Build and load autoencoder training image into kind
    cmds:
      - task: autoencoder-image:build
      - task: autoencoder-image:load

  pipeline-images:
    desc: Build and load all pipeline images (spark + lakefs component + autoencoder)
    cmds:
      - task: spark-image
      - task: lakefs-component-image
      - task: autoencoder-image

  pipeline-images:verify:
    desc: Verify pipeline images are loaded in kind cluster
    cmds:
      - |
        echo "=== Checking pipeline images in kind cluster ==="
        echo ""

        IMAGES=("dfp-spark:latest" "dfp-lakefs-component:v2" "dfp-autoencoder-train:v8")
        ERRORS=0

        for IMG in "${IMAGES[@]}"; do
          if docker exec "{{.CLUSTER_NAME}}-control-plane" crictl images | grep -q "${IMG%:*}"; then
            echo "  ✓ $IMG: loaded"
          else
            echo "  ✗ $IMG: NOT FOUND"
            ERRORS=$((ERRORS + 1))
          fi
        done

        echo ""
        if [ $ERRORS -gt 0 ]; then
          echo "Run 'task pipeline-images' to build and load missing images."
          exit 1
        fi
        echo "All pipeline images verified!"

  # ===========================================================================
  # Pipeline Execution Tasks
  # ===========================================================================

  pipeline:spark:
    desc: Run Spark Iceberg transformation pipeline (direct SparkApplication)
    vars:
      BRANCH: '{{.BRANCH | default "dev"}}'
      DESTINATION: '{{.DESTINATION | default "lakefs"}}'
      TEST_MODE: '{{.TEST_MODE | default "false"}}'
    cmds:
      - |
        ARGS="--branch {{.BRANCH}} --transform-engine kubeflow --destination {{.DESTINATION}}"

        if [ "{{.TEST_MODE}}" = "true" ]; then
          ARGS="$ARGS --test-mode"
        fi

        # Skip ingestion if data already exists
        ARGS="$ARGS --skip-ingestion --skip-feast"

        echo "Running Spark transformation pipeline..."
        echo "  Branch: {{.BRANCH}}"
        echo "  Destination: {{.DESTINATION}}"
        echo ""

        uv run tools/scripts/run_kronodroid_pipeline.py $ARGS

  pipeline:spark:kfp:
    desc: Run Spark Iceberg transformation pipeline via KFP client
    vars:
      BRANCH: '{{.BRANCH | default "dev"}}'
      DESTINATION: '{{.DESTINATION | default "lakefs"}}'
    cmds:
      - |
        echo "Ensuring secrets are synced..."
        task secrets:sync

        echo ""
        echo "Running Spark transformation pipeline via KFP..."
        echo "  Branch: {{.BRANCH}}"
        echo "  Destination: {{.DESTINATION}}"
        echo ""

        uv run tools/scripts/run_kronodroid_pipeline.py \
          --branch "{{.BRANCH}}" \
          --transform-engine kubeflow \
          --destination "{{.DESTINATION}}" \
          --use-kfp-client \
          --skip-ingestion \
          --skip-feast

  pipeline:full:
    desc: Run full pipeline (ingest -> transform -> feast)
    vars:
      BRANCH: '{{.BRANCH | default "dev"}}'
      DESTINATION: '{{.DESTINATION | default "lakefs"}}'
    cmds:
      - |
        echo "Ensuring secrets are synced..."
        task secrets:sync

        echo ""
        echo "Running full pipeline..."
        uv run tools/scripts/run_kronodroid_pipeline.py \
          --branch "{{.BRANCH}}" \
          --transform-engine kubeflow \
          --destination "{{.DESTINATION}}"

  pipeline:setup:
    desc: Complete pipeline setup (sync secrets, build images, verify)
    cmds:
      - task: secrets:sync
      - task: pipeline-images
      - task: pipeline-images:verify
      - task: secrets:verify
      - |
        echo ""
        echo "=== Pipeline setup complete ==="
        echo ""
        echo "You can now run pipelines with:"
        echo "  task pipeline:spark BRANCH=dev          # Direct SparkApplication"
        echo "  task pipeline:spark:kfp BRANCH=dev      # Via KFP client"
        echo "  task pipeline:full BRANCH=dev           # Full pipeline"
