{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Spark & Feast Testing Notebook\n",
                "\n",
                "This notebook provides a complete environment for testing the integration between **Spark**, **LakeFS (Iceberg)**, and **Feast**.\n",
                "\n",
                "## Objectives:\n",
                "1.  **Initialize Spark** with Iceberg and S3A (LakeFS) configurations.\n",
                "2.  **Explore Data** in LakeFS Iceberg tables.\n",
                "3.  **Interact with Feast** to manage and retrieve features.\n",
                "4.  **Validate Integration** between Spark offline store and Feast."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration\n",
                "\n",
                "First, we'll set up the environment variables and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pandas as pd\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "# Set environment variables for LakeFS and Feast if they aren't already set\n",
                "# Update these values to match your local environment if necessary\n",
                "os.environ[\"LAKEFS_ENDPOINT_URL\"] = os.environ.get(\"LAKEFS_ENDPOINT_URL\", \"http://localhost:8000\")\n",
                "os.environ[\"LAKEFS_ACCESS_KEY_ID\"] = os.environ.get(\"LAKEFS_ACCESS_KEY_ID\", \"AKIAJWAE4BUBMLQESYDQ\")\n",
                "os.environ[\"LAKEFS_SECRET_ACCESS_KEY\"] = os.environ.get(\"LAKEFS_SECRET_ACCESS_KEY\", \"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\")\n",
                "os.environ[\"LAKEFS_REPOSITORY\"] = os.environ.get(\"LAKEFS_REPOSITORY\", \"kronodroid\")\n",
                "os.environ[\"LAKEFS_BRANCH\"] = os.environ.get(\"LAKEFS_BRANCH\", \"main\")\n",
                "os.environ[\"REDIS_CONNECTION_STRING\"] = os.environ.get(\"REDIS_CONNECTION_STRING\", \"redis://localhost:6379\")\n",
                "\n",
                "print(f\"LakeFS Repository: {os.environ['LAKEFS_REPOSITORY']}\")\n",
                "print(f\"LakeFS Branch:     {os.environ['LAKEFS_BRANCH']}\")\n",
                "print(f\"LakeFS Endpoint:   {os.environ['LAKEFS_ENDPOINT_URL']}\")\n",
                "print(f\"Redis Connection:  {os.environ['REDIS_CONNECTION_STRING']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Spark\n",
                "\n",
                "We'll initialize a Spark session configured to use the LakeFS S3 gateway and the Iceberg catalog."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "repo = os.environ[\"LAKEFS_REPOSITORY\"]\n",
                "branch = os.environ[\"LAKEFS_BRANCH\"]\n",
                "endpoint = os.environ[\"LAKEFS_ENDPOINT_URL\"]\n",
                "access_key = os.environ[\"LAKEFS_ACCESS_KEY_ID\"]\n",
                "secret_key = os.environ[\"LAKEFS_SECRET_ACCESS_KEY\"]\n",
                "\n",
                "spark = (SparkSession.builder\n",
                "    .appName(\"Spark Feast Test\")\n",
                "    # Iceberg extensions\n",
                "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
                "    # LakeFS Iceberg catalog (Hadoop-based)\n",
                "    .config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
                "    .config(\"spark.sql.catalog.lakefs.type\", \"hadoop\")\n",
                "    .config(\"spark.sql.catalog.lakefs.warehouse\", f\"s3a://{repo}/{branch}/iceberg\")\n",
                "    # S3A filesystem for LakeFS S3 gateway\n",
                "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
                "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
                "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
                "    # Per-bucket config for LakeFS repository\n",
                "    .config(f\"spark.hadoop.fs.s3a.bucket.{repo}.endpoint\", endpoint)\n",
                "    .config(f\"spark.hadoop.fs.s3a.bucket.{repo}.access.key\", access_key)\n",
                "    .config(f\"spark.hadoop.fs.s3a.bucket.{repo}.secret.key\", secret_key)\n",
                "    # Maven packages for Iceberg + S3A (compatible with Spark 3.5.0)\n",
                "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
                "    .getOrCreate())\n",
                "\n",
                "print(\"✅ Spark session initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Explore LakeFS Iceberg Tables\n",
                "\n",
                "Let's see what tables are available in the LakeFS catalog."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List databases in lakefs catalog\n",
                "spark.sql(\"SHOW NAMESPACES IN lakefs\").show()\n",
                "\n",
                "# List tables in kronodroid database (if it exists)\n",
                "try:\n",
                "    spark.sql(\"SHOW TABLES IN lakefs.kronodroid\").show()\n",
                "except Exception as e:\n",
                "    print(f\"Could not list tables in lakefs.kronodroid: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data from fct_training_dataset if it exists\n",
                "try:\n",
                "    df = spark.table(\"lakefs.kronodroid.fct_training_dataset\")\n",
                "    print(f\"Total records in fct_training_dataset: {df.count()}\")\n",
                "    df.limit(5).toPandas().display()\n",
                "except Exception as e:\n",
                "    print(f\"Could not read lakefs.kronodroid.fct_training_dataset: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feast Integration\n",
                "\n",
                "Now we'll initialize the Feast Feature Store and verify that it can read data from Spark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from feast import FeatureStore\n",
                "import os\n",
                "\n",
                "# The location of your feature_store.yaml or feature_store_spark.yaml\n",
                "repo_path = os.path.join(os.getcwd(), \"../feature_stores/feast_store\")\n",
                "fs = FeatureStore(repo_path=repo_path)\n",
                "\n",
                "print(f\"✅ Feast FeatureStore initialized from: {repo_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1. List Feature Views & Entities\n",
                "\n",
                "Let's see what features are defined in Feast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Entities:\")\n",
                "for entity in fs.list_entities():\n",
                "    print(f\"  - {entity.name}\")\n",
                "\n",
                "print(\"\\nFeature Views:\")\n",
                "for fv in fs.list_feature_views():\n",
                "    print(f\"  - {fv.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2. Retrieval: Historical Features (Offline Store)\n",
                "\n",
                "We'll retrieve historical features using Spark as the offline store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from datetime import datetime\n",
                "\n",
                "# Define an entity dataframe for retrieval\n",
                "# In a real scenario, this would be your training data with timestamps\n",
                "entity_df = pd.DataFrame.from_dict({\n",
                "    \"sample_hash\": [\n",
                "        \"0000ed700543e4776114ebca3eb0df04\",\n",
                "        \"00018f2f4c39c4a56c4d8ce7b30cd0f9\",\n",
                "        \"0002ba7c18001d9f829f0ce645c9df5e\"\n",
                "    ],\n",
                "    \"event_timestamp\": [\n",
                "        datetime.now(),\n",
                "        datetime.now(),\n",
                "        datetime.now()\n",
                "    ]\n",
                "})\n",
                "\n",
                "try:\n",
                "    # Get historical features\n",
                "    retrieval_job = fs.get_historical_features(\n",
                "        entity_df=entity_df,\n",
                "        features=[\n",
                "            \"malware_sample_features:label\",\n",
                "            \"malware_sample_features:syscall_total\",\n",
                "            \"malware_sample_features:syscall_mean\",\n",
                "        ],\n",
                "    )\n",
                "    \n",
                "    # Convert to pandas to see results\n",
                "    result_df = retrieval_job.to_df()\n",
                "    # Use display() if in a notebook environment, else print\n",
                "    if 'IPython' in sys.modules:\n",
                "        from IPython.display import display\n",
                "        display(result_df)\n",
                "    else:\n",
                "        print(result_df)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Failed to get historical features: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3. Retrieval: Online Features (Online Store)\n",
                "\n",
                "First, we would normally materialize features to the online store. \n",
                "Then we can retrieve them for low-latency serving."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: materializing latest features (uncomment to run)\n",
                "# fs.materialize_incremental(end_date=datetime.now())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Get online features\n",
                "    online_features = fs.get_online_features(\n",
                "        features=[\n",
                "            \"malware_sample_features:label\",\n",
                "            \"malware_sample_features:syscall_total\",\n",
                "        ],\n",
                "        entity_rows=[\n",
                "            {\"sample_hash\": \"0000ed700543e4776114ebca3eb0df04\"}\n",
                "        ],\n",
                "    )\n",
                "    \n",
                "    print(\"Online Features Result:\")\n",
                "    print(online_features.to_dict())\n",
                "except Exception as e:\n",
                "    print(f\"❌ Failed to get online features: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Cleanup\n",
                "\n",
                "Stop the Spark session when finished."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()\n",
                "print(\"✅ Spark session stopped\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}