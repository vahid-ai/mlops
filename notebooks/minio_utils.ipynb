{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinIO Utilities\n",
    "\n",
    "Utility functions for managing MinIO buckets and objects.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- MinIO server running (locally: `task up` or `docker-compose up`)\n",
    "- Port-forwarding enabled for local kind cluster (`task port-forward`)\n",
    "- boto3 installed (`pip install boto3` or `uv pip install boto3`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO Endpoint: http://localhost:19000\n",
      "Access Key: minioadmin\n"
     ]
    }
   ],
   "source": [
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT = os.getenv('MINIO_ENDPOINT_URL', 'http://localhost:19000')\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ACCESS_KEY_ID', 'minioadmin')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_SECRET_ACCESS_KEY', 'minioadmin')\n",
    "\n",
    "print(f\"MinIO Endpoint: {MINIO_ENDPOINT}\")\n",
    "print(f\"Access Key: {MINIO_ACCESS_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MinIO client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client and resource\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "print(\"✓ MinIO client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_buckets() -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    List all buckets in MinIO.\n",
    "    \n",
    "    Returns:\n",
    "        List of bucket dictionaries with 'Name' and 'CreationDate'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_buckets()\n",
    "        buckets = response.get('Buckets', [])\n",
    "        print(f\"Found {len(buckets)} bucket(s):\\n\")\n",
    "        \n",
    "        for bucket in buckets:\n",
    "            name = bucket['Name']\n",
    "            created = bucket['CreationDate'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"  • {name:30s} (created: {created})\")\n",
    "        \n",
    "        return buckets\n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing buckets: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name: str, region: Optional[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Create a new bucket in MinIO.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to create\n",
    "        region: AWS region (optional, not required for MinIO)\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        else:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        \n",
    "        print(f\"✓ Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"⚠ Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        elif e.response['Error']['Code'] == 'BucketAlreadyExists':\n",
    "            print(f\"⚠ Bucket '{bucket_name}' already exists\")\n",
    "        else:\n",
    "            print(f\"Error creating bucket: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket(bucket_name: str, force: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a bucket from MinIO.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to delete\n",
    "        force: If True, delete all objects first. If False, bucket must be empty.\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        \n",
    "        if force:\n",
    "            print(f\"⚠ Force deleting bucket '{bucket_name}' and all its contents...\")\n",
    "            # Delete all objects and versions\n",
    "            bucket.objects.all().delete()\n",
    "            bucket.object_versions.all().delete()\n",
    "            print(f\"  ✓ All objects deleted\")\n",
    "        \n",
    "        # Delete the bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"✓ Bucket '{bucket_name}' deleted successfully\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchBucket':\n",
    "            print(f\"⚠ Bucket '{bucket_name}' does not exist\")\n",
    "        elif error_code == 'BucketNotEmpty':\n",
    "            print(f\"⚠ Bucket '{bucket_name}' is not empty. Use force=True to delete contents.\")\n",
    "        else:\n",
    "            print(f\"Error deleting bucket: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_exists(bucket_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a bucket exists.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to check\n",
    "    \n",
    "    Returns:\n",
    "        True if bucket exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_size(bucket_name: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Get the total size and object count of a bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'total_size_bytes', 'total_size_mb', 'object_count'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        total_size = 0\n",
    "        object_count = 0\n",
    "        \n",
    "        for obj in bucket.objects.all():\n",
    "            total_size += obj.size\n",
    "            object_count += 1\n",
    "        \n",
    "        total_size_mb = total_size / (1024 * 1024)\n",
    "        total_size_gb = total_size / (1024 * 1024 * 1024)\n",
    "        \n",
    "        result = {\n",
    "            'bucket_name': bucket_name,\n",
    "            'object_count': object_count,\n",
    "            'total_size_bytes': total_size,\n",
    "            'total_size_mb': round(total_size_mb, 2),\n",
    "            'total_size_gb': round(total_size_gb, 2)\n",
    "        }\n",
    "        \n",
    "        print(f\"Bucket: {bucket_name}\")\n",
    "        print(f\"  Objects: {object_count}\")\n",
    "        print(f\"  Size: {result['total_size_mb']} MB ({result['total_size_gb']} GB)\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting bucket size: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_objects(bucket_name: str, prefix: str = '', max_keys: int = 1000) -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    List objects in a bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        prefix: Filter objects by prefix (folder path)\n",
    "        max_keys: Maximum number of objects to return\n",
    "    \n",
    "    Returns:\n",
    "        List of object dictionaries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=max_keys\n",
    "        )\n",
    "        \n",
    "        objects = response.get('Contents', [])\n",
    "        \n",
    "        if not objects:\n",
    "            print(f\"No objects found in '{bucket_name}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(objects)} object(s) in '{bucket_name}' (prefix: '{prefix}'):\\n\")\n",
    "        \n",
    "        for obj in objects[:20]:  # Show first 20\n",
    "            key = obj['Key']\n",
    "            size = obj['Size']\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"  • {key:50s} {size_mb:8.2f} MB  {modified}\")\n",
    "        \n",
    "        if len(objects) > 20:\n",
    "            print(f\"\\n  ... and {len(objects) - 20} more objects\")\n",
    "        \n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing objects: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_object(bucket_name: str, object_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a single object from a bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key (path) of the object to delete\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=object_key)\n",
    "        print(f\"✓ Deleted: {object_key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting object: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_objects_by_prefix(bucket_name: str, prefix: str, dry_run: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Delete all objects with a given prefix (folder).\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        prefix: Prefix (folder path) to delete\n",
    "        dry_run: If True, only list objects without deleting\n",
    "    \n",
    "    Returns:\n",
    "        Number of objects deleted (or would be deleted in dry_run mode)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        objects_to_delete = list(bucket.objects.filter(Prefix=prefix))\n",
    "        \n",
    "        if not objects_to_delete:\n",
    "            print(f\"No objects found with prefix '{prefix}' in bucket '{bucket_name}'\")\n",
    "            return 0\n",
    "        \n",
    "        count = len(objects_to_delete)\n",
    "        \n",
    "        if dry_run:\n",
    "            print(f\"DRY RUN: Would delete {count} object(s) with prefix '{prefix}':\\n\")\n",
    "            for obj in objects_to_delete[:20]:\n",
    "                print(f\"  • {obj.key}\")\n",
    "            if count > 20:\n",
    "                print(f\"  ... and {count - 20} more objects\")\n",
    "            print(f\"\\nSet dry_run=False to actually delete these objects\")\n",
    "        else:\n",
    "            print(f\"⚠ Deleting {count} object(s) with prefix '{prefix}'...\")\n",
    "            for obj in objects_to_delete:\n",
    "                obj.delete()\n",
    "            print(f\"✓ Deleted {count} object(s)\")\n",
    "        \n",
    "        return count\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting objects: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_objects(bucket_name: str, dry_run: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Delete ALL objects in a bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        dry_run: If True, only list objects without deleting\n",
    "    \n",
    "    Returns:\n",
    "        Number of objects deleted (or would be deleted in dry_run mode)\n",
    "    \"\"\"\n",
    "    return delete_objects_by_prefix(bucket_name, '', dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_object(source_bucket: str, source_key: str, dest_bucket: str, dest_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Copy an object from one location to another.\n",
    "    \n",
    "    Args:\n",
    "        source_bucket: Source bucket name\n",
    "        source_key: Source object key\n",
    "        dest_bucket: Destination bucket name\n",
    "        dest_key: Destination object key\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        copy_source = {'Bucket': source_bucket, 'Key': source_key}\n",
    "        s3_client.copy_object(\n",
    "            CopySource=copy_source,\n",
    "            Bucket=dest_bucket,\n",
    "            Key=dest_key\n",
    "        )\n",
    "        print(f\"✓ Copied: {source_bucket}/{source_key} → {dest_bucket}/{dest_key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error copying object: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_object(bucket_name: str, object_key: str, local_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download an object to a local file.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key of the object to download\n",
    "        local_path: Local file path to save to\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, object_key, local_path)\n",
    "        print(f\"✓ Downloaded: {bucket_name}/{object_key} → {local_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading object: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(local_path: str, bucket_name: str, object_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload a local file to MinIO.\n",
    "    \n",
    "    Args:\n",
    "        local_path: Path to local file\n",
    "        bucket_name: Name of the destination bucket\n",
    "        object_key: Key (path) for the object in the bucket\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_path, bucket_name, object_key)\n",
    "        print(f\"✓ Uploaded: {local_path} → {bucket_name}/{object_key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {local_path}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_bucket_stats() -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Get size and object count statistics for all buckets.\n",
    "    \n",
    "    Returns:\n",
    "        List of bucket statistics\n",
    "    \"\"\"\n",
    "    buckets = list_buckets()\n",
    "    stats = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Bucket Statistics\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for bucket in buckets:\n",
    "        bucket_name = bucket['Name']\n",
    "        stat = get_bucket_size(bucket_name)\n",
    "        stats.append(stat)\n",
    "        print()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_empty_buckets(dry_run: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Delete all empty buckets.\n",
    "    \n",
    "    Args:\n",
    "        dry_run: If True, only list empty buckets without deleting\n",
    "    \n",
    "    Returns:\n",
    "        Number of buckets deleted (or would be deleted in dry_run mode)\n",
    "    \"\"\"\n",
    "    buckets = list_buckets()\n",
    "    empty_buckets = []\n",
    "    \n",
    "    for bucket in buckets:\n",
    "        bucket_name = bucket['Name']\n",
    "        stat = get_bucket_size(bucket_name)\n",
    "        if stat.get('object_count', 0) == 0:\n",
    "            empty_buckets.append(bucket_name)\n",
    "    \n",
    "    if not empty_buckets:\n",
    "        print(\"No empty buckets found\")\n",
    "        return 0\n",
    "    \n",
    "    count = len(empty_buckets)\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"\\nDRY RUN: Would delete {count} empty bucket(s):\\n\")\n",
    "        for bucket_name in empty_buckets:\n",
    "            print(f\"  • {bucket_name}\")\n",
    "        print(f\"\\nSet dry_run=False to actually delete these buckets\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Deleting {count} empty bucket(s)...\\n\")\n",
    "        for bucket_name in empty_buckets:\n",
    "            delete_bucket(bucket_name, force=False)\n",
    "        print(f\"\\n✓ Deleted {count} empty bucket(s)\")\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: List all buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Get statistics for all buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_all_bucket_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: List objects in a specific bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your-bucket-name' with an actual bucket name\n",
    "# objects = list_objects('your-bucket-name')\n",
    "\n",
    "# Or with a prefix to filter by folder\n",
    "# objects = list_objects('your-bucket-name', prefix='data/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to create a test bucket\n",
    "# create_bucket('test-bucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Delete objects by prefix (DRY RUN first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a dry run to see what would be deleted\n",
    "# delete_objects_by_prefix('your-bucket-name', 'old-data/', dry_run=True)\n",
    "\n",
    "# Then actually delete (uncomment and run after reviewing)\n",
    "# delete_objects_by_prefix('your-bucket-name', 'old-data/', dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Delete a bucket (with force option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete empty bucket\n",
    "# delete_bucket('test-bucket')\n",
    "\n",
    "# Force delete bucket with all contents (DANGEROUS!)\n",
    "# delete_bucket('test-bucket', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Clean up empty buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 bucket(s):\n",
      "\n",
      "  • dlt-data                       (created: 2025-12-16 06:57:13)\n",
      "  • lakefs-data                    (created: 2025-12-17 23:54:38)\n",
      "Bucket: dlt-data\n",
      "  Objects: 0\n",
      "  Size: 0.0 MB (0.0 GB)\n",
      "Bucket: lakefs-data\n",
      "  Objects: 0\n",
      "  Size: 0.0 MB (0.0 GB)\n",
      "\n",
      "DRY RUN: Would delete 2 empty bucket(s):\n",
      "\n",
      "  • dlt-data\n",
      "  • lakefs-data\n",
      "\n",
      "Set dry_run=False to actually delete these buckets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First do a dry run\n",
    "# cleanup_empty_buckets(dry_run=True)\n",
    "\n",
    "# Then actually delete (uncomment after reviewing)\n",
    "cleanup_empty_buckets(dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Upload and download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a file\n",
    "# upload_file('/path/to/local/file.txt', 'your-bucket-name', 'data/file.txt')\n",
    "\n",
    "# Download a file\n",
    "# download_object('your-bucket-name', 'data/file.txt', '/path/to/download/file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Copy objects between buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy an object\n",
    "# copy_object('source-bucket', 'data/file.txt', 'dest-bucket', 'backup/file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Reminders\n",
    "\n",
    "⚠️ **IMPORTANT SAFETY NOTES:**\n",
    "\n",
    "1. **Always use `dry_run=True` first** when deleting objects or buckets in bulk\n",
    "2. **Deletions are irreversible** - make sure you have backups if needed\n",
    "3. **Check dependencies** - deleting buckets used by LakeFS or other services will break them\n",
    "4. **Use prefixes carefully** - deleting by prefix can remove entire folder structures\n",
    "5. **Test on development** - test destructive operations on dev/test buckets first\n",
    "\n",
    "### Common LakeFS-related buckets (DO NOT DELETE unless you know what you're doing):\n",
    "- Buckets with names containing `lakefs`, `iceberg`, `warehouse`\n",
    "- Any bucket referenced in your LakeFS configuration\n",
    "- Any bucket containing Iceberg table metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
