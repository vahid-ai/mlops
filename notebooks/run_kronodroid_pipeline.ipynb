{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Kronodroid Pipeline Runner (Notebook)\n",
    "\n",
    "Notebook equivalent of `tools/scripts/run_kronodroid_pipeline.py`.\n",
    "\n",
    "Data flow:\n",
    "```\n",
    "Kaggle → dlt → Parquet/Avro → MinIO → Spark + dbt → Iceberg (LakeFS) → Feast → (optional) LakeFS commit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1) Environment setup\n",
    "\n",
    "- Loads `.env` from repo root (same behavior as the script).\n",
    "- Ensures the repo is importable from `notebooks/`.\n",
    "- Sets `LAKEFS_BRANCH` from the parameters cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repo root is importable when running from the notebooks directory.\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from tools.scripts import run_kronodroid_pipeline as krono\n",
    "\n",
    "# Load env vars from `.env` (if present)\n",
    "krono.load_env_file(REPO_ROOT / \".env\")\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"LAKEFS_REPOSITORY: {os.getenv('LAKEFS_REPOSITORY', 'kronodroid')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e422ee",
   "metadata": {},
   "source": [
    "## 2) Run parameters\n",
    "\n",
    "Mirror the CLI flags from `tools/scripts/run_kronodroid_pipeline.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086faewjqlec",
   "metadata": {},
   "source": [
    "**Important Note on Spark Connectivity:**\n",
    "\n",
    "When using `TRANSFORM_RUNNER = \"spark-operator\"`:\n",
    "- The transformations run in the Kind cluster using Spark Operator (no local Spark needed)\n",
    "- However, **Feast with Spark offline store still requires Spark connectivity** to register features and materialize\n",
    "- Setting `SKIP_SPARK_CHECK = True` bypasses the connectivity check, but Feast may still fail if Spark is unavailable\n",
    "\n",
    "**Options:**\n",
    "1. **Use Spark Thrift Server** (recommended): Deploy it even when using spark-operator\n",
    "   ```bash\n",
    "   kubectl apply -k infra/k8s/kind/addons/spark-thrift/\n",
    "   kubectl -n dfp wait --for=condition=ready pod -l app=spark-thrift-server --timeout=120s\n",
    "   ```\n",
    "2. **Skip Feast steps**: Set `SKIP_FEAST_APPLY = True` and `SKIP_MATERIALIZE = True`\n",
    "3. **Accept Feast failures**: Let `SKIP_SPARK_CHECK = True` and catch any Feast errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "BRANCH = \"main\"\n",
    "TRANSFORM_RUNNER = \"spark-operator\"  # \"dbt\" or \"spark-operator\"\n",
    "DBT_TARGET = \"dev\"  # \"dev\" (embedded Spark) or \"thrift\" (Spark server)\n",
    "K8S_NAMESPACE = \"dfp\"  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "SPARK_IMAGE = \"apache/spark:3.5.7-python3\"  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "SPARK_TIMEOUT_SECONDS = 60 * 30  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "FILE_FORMAT = \"parquet\"  # \"parquet\" or \"avro\" (dlt loader uses parquet for avro requests)\n",
    "\n",
    "# Step toggles\n",
    "SKIP_INGESTION = False\n",
    "SKIP_DBT = False\n",
    "SKIP_FEAST_APPLY = False\n",
    "SKIP_MATERIALIZE = False\n",
    "SKIP_COMMIT = False\n",
    "\n",
    "# Spark check: Set to False when using spark-operator (Feast will try to connect anyway)\n",
    "# If Feast fails, you may need to start Spark Thrift Server or skip Feast steps\n",
    "SKIP_SPARK_CHECK = True if TRANSFORM_RUNNER == \"spark-operator\" else False\n",
    "\n",
    "# Materialize-only mode (equivalent to `--materialize-only`)\n",
    "MATERIALIZE_ONLY = False\n",
    "MATERIALIZE_DAYS = 30\n",
    "\n",
    "os.environ[\"LAKEFS_BRANCH\"] = BRANCH\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"BRANCH\": BRANCH,\n",
    "        \"TRANSFORM_RUNNER\": TRANSFORM_RUNNER,\n",
    "        \"DBT_TARGET\": DBT_TARGET,\n",
    "        \"FILE_FORMAT\": FILE_FORMAT,\n",
    "        \"SKIP_SPARK_CHECK\": SKIP_SPARK_CHECK,\n",
    "        \"MATERIALIZE_ONLY\": MATERIALIZE_ONLY,\n",
    "        \"MATERIALIZE_DAYS\": MATERIALIZE_DAYS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de648110",
   "metadata": {},
   "source": [
    "## 3) Optional: quick dependency check\n",
    "\n",
    "The pipeline uses the `dbt` and `feast` CLIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print({\"dbt\": shutil.which(\"dbt\"), \"feast\": shutil.which(\"feast\"), \"kubectl\": shutil.which(\"kubectl\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark-operator-check",
   "metadata": {},
   "source": [
    "## 4) Spark Operator Status Check (when using spark-operator)\n",
    "\n",
    "These cells use the same monitoring functions as `spark_operator_test.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-spark-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Spark Operator deployment (only relevant when using spark-operator)\n",
    "if TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    print(\"Checking Spark Operator deployment...\\n\")\n",
    "    \n",
    "    deployment = krono.kubectl_get_json(\"deployment\", \"spark-operator\", K8S_NAMESPACE)\n",
    "    \n",
    "    if deployment:\n",
    "        containers = deployment.get(\"spec\", {}).get(\"template\", {}).get(\"spec\", {}).get(\"containers\", [])\n",
    "        if containers:\n",
    "            image = containers[0].get(\"image\", \"unknown\")\n",
    "            krono.print_status(\"success\", f\"Spark Operator image: {image}\")\n",
    "        \n",
    "        status = deployment.get(\"status\", {})\n",
    "        ready = status.get(\"readyReplicas\", 0)\n",
    "        desired = status.get(\"replicas\", 0)\n",
    "        \n",
    "        if ready >= 1:\n",
    "            krono.print_status(\"success\", f\"Replicas: {ready}/{desired} ready\")\n",
    "        else:\n",
    "            krono.print_status(\"warning\", f\"Replicas: {ready}/{desired} ready\")\n",
    "    else:\n",
    "        krono.print_status(\"error\", \"Spark Operator not found!\")\n",
    "        print(\"\\n  To install: kubectl apply -k infra/k8s/kind/addons/spark-operator/\")\n",
    "        print(\"  Or: task spark-operator:up\")\n",
    "else:\n",
    "    print(\"Using dbt runner - Spark Operator check skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-spark-apps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing SparkApplications\n",
    "if TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    apps = krono.list_spark_applications(K8S_NAMESPACE)\n",
    "    \n",
    "    if apps:\n",
    "        print(f\"Existing SparkApplications in namespace '{K8S_NAMESPACE}':\\n\")\n",
    "        for app in apps:\n",
    "            state_icon = {\n",
    "                'COMPLETED': '[OK]',\n",
    "                'RUNNING': '[RUN]',\n",
    "                'SUBMITTED': '[SUB]',\n",
    "                'FAILED': '[FAIL]',\n",
    "                'SUBMISSION_FAILED': '[FAIL]'\n",
    "            }.get(app['state'], '[?]')\n",
    "            print(f\"  {state_icon:7s} {app['name']:50s} {app['created']}\")\n",
    "    else:\n",
    "        print(\"No SparkApplications found\")\n",
    "else:\n",
    "    print(\"Using dbt runner - skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f62eb",
   "metadata": {},
   "source": [
    "## 5) Run the pipeline step-by-step\n",
    "\n",
    "Run the cells below step-by-step, or run the \"Run full pipeline\" cell to mirror the script's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcef05",
   "metadata": {},
   "source": [
    "### Step 1: dlt ingestion (Kaggle → MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c264ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_INGESTION:\n",
    "    ok = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"dlt ingestion failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15a899",
   "metadata": {},
   "source": [
    "### Step 2: transformations (MinIO → Iceberg on LakeFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs the transformations and stores the app name for monitoring\n",
    "SPARK_APP_NAME = None  # Will be set if using spark-operator\n",
    "\n",
    "if not MATERIALIZE_ONLY and not SKIP_DBT:\n",
    "    if TRANSFORM_RUNNER == \"dbt\":\n",
    "        ok = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "    else:\n",
    "        ok = krono.run_kubeflow_spark_operator_transformations(\n",
    "            branch=BRANCH,\n",
    "            namespace=K8S_NAMESPACE,\n",
    "            spark_image=SPARK_IMAGE,\n",
    "            timeout_seconds=SPARK_TIMEOUT_SECONDS,\n",
    "        )\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"transformations failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "### Step 2b: Monitor SparkApplication (interactive)\n",
    "\n",
    "Use these cells to monitor a running SparkApplication or debug failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor-app-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of a specific SparkApplication\n",
    "# Replace with actual app name from Step 2 output (e.g., \"kronodroid-transform-20250112-123456\")\n",
    "APP_NAME_TO_MONITOR = None  # Set to app name if you want to monitor\n",
    "\n",
    "if APP_NAME_TO_MONITOR and TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    status = krono.get_spark_application_status(APP_NAME_TO_MONITOR, K8S_NAMESPACE)\n",
    "    \n",
    "    if status['found']:\n",
    "        state = status['state']\n",
    "        state_icons = {\n",
    "            'COMPLETED': 'success',\n",
    "            'RUNNING': 'info',\n",
    "            'SUBMITTED': 'pending',\n",
    "            'FAILED': 'error',\n",
    "            'SUBMISSION_FAILED': 'error',\n",
    "        }\n",
    "        icon = state_icons.get(state, 'info')\n",
    "        krono.print_status(icon, f\"State: {state}\")\n",
    "        \n",
    "        if status['spark_application_id']:\n",
    "            print(f\"    Spark Application ID: {status['spark_application_id']}\")\n",
    "        \n",
    "        if status['driver_info']:\n",
    "            driver = status['driver_info']\n",
    "            print(f\"    Driver Pod: {driver.get('podName', 'N/A')}\")\n",
    "            if driver.get('webUIAddress'):\n",
    "                print(f\"    Spark UI: {driver.get('webUIAddress')}\")\n",
    "        \n",
    "        if status['error_message']:\n",
    "            print(f\"    Error: {status['error_message']}\")\n",
    "    else:\n",
    "        krono.print_status('error', f\"SparkApplication '{APP_NAME_TO_MONITOR}' not found\")\n",
    "else:\n",
    "    print(\"Set APP_NAME_TO_MONITOR to monitor a specific SparkApplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-driver-logs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get driver logs for a SparkApplication\n",
    "if APP_NAME_TO_MONITOR and TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    print(f\"Driver logs for {APP_NAME_TO_MONITOR} (last 100 lines):\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    logs = krono.get_driver_logs(APP_NAME_TO_MONITOR, K8S_NAMESPACE, tail=100)\n",
    "    if logs:\n",
    "        print(logs)\n",
    "    else:\n",
    "        print(\"No logs available (driver pod may not exist yet or has been cleaned up)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"Set APP_NAME_TO_MONITOR to view driver logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "live-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live monitor a SparkApplication until completion\n",
    "# This will block until the app completes, fails, or times out\n",
    "\n",
    "LIVE_MONITOR_APP = None  # Set to app name to monitor live\n",
    "MONITOR_TIMEOUT = 300  # seconds\n",
    "\n",
    "if LIVE_MONITOR_APP and TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    print(f\"Monitoring {LIVE_MONITOR_APP} (timeout: {MONITOR_TIMEOUT}s)...\\n\")\n",
    "    result = krono.monitor_spark_application_status(\n",
    "        LIVE_MONITOR_APP,\n",
    "        namespace=K8S_NAMESPACE,\n",
    "        timeout_seconds=MONITOR_TIMEOUT,\n",
    "        poll_interval=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"\\nResult: {result}\")\n",
    "else:\n",
    "    print(\"Set LIVE_MONITOR_APP to monitor a SparkApplication live\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407645a",
   "metadata": {},
   "source": [
    "### Step 3: Feast apply (register feature definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_FEAST_APPLY:\n",
    "    check_spark = not SKIP_SPARK_CHECK\n",
    "    ok = krono.run_feast_apply(check_spark=check_spark)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"feast apply failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f488802",
   "metadata": {},
   "source": [
    "### Step 4: Feast materialize (offline → online store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_MATERIALIZE:\n",
    "    check_spark = not SKIP_SPARK_CHECK\n",
    "    ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "    if not ok:\n",
    "        print(\"WARNING: feature materialization failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bcbcf",
   "metadata": {},
   "source": [
    "### Step 5: Commit to LakeFS (Iceberg tables → commit)\n",
    "\n",
    "Uses `engines.spark_engine.dfp_spark.iceberg_catalog.commit_iceberg_changes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed13d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not MATERIALIZE_ONLY and not SKIP_COMMIT:\n",
    "    krono.commit_to_lakefs(\n",
    "        branch=BRANCH,\n",
    "        message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7dfca",
   "metadata": {},
   "source": [
    "## 6) Run full pipeline (script-like)\n",
    "\n",
    "Convenience cell mirroring the script's `main()` flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046da518",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = True\n",
    "check_spark = not SKIP_SPARK_CHECK\n",
    "\n",
    "if MATERIALIZE_ONLY:\n",
    "    success = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "else:\n",
    "    if not SKIP_INGESTION:\n",
    "        success = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at dlt ingestion\")\n",
    "\n",
    "    if not SKIP_DBT:\n",
    "        if TRANSFORM_RUNNER == \"dbt\":\n",
    "            success = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "        else:\n",
    "            success = krono.run_kubeflow_spark_operator_transformations(\n",
    "                branch=BRANCH,\n",
    "                namespace=K8S_NAMESPACE,\n",
    "                spark_image=SPARK_IMAGE,\n",
    "                timeout_seconds=SPARK_TIMEOUT_SECONDS,\n",
    "            )\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at transformations\")\n",
    "\n",
    "    if not SKIP_FEAST_APPLY:\n",
    "        success = krono.run_feast_apply(check_spark=check_spark)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at feast apply\")\n",
    "\n",
    "    if not SKIP_MATERIALIZE:\n",
    "        ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "        if not ok:\n",
    "            print(\"WARNING: feature materialization failed\")\n",
    "\n",
    "    if not SKIP_COMMIT:\n",
    "        krono.commit_to_lakefs(\n",
    "            branch=BRANCH,\n",
    "            message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "        )\n",
    "\n",
    "print({\"success\": bool(success)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## 7) Cleanup SparkApplications\n",
    "\n",
    "Use these cells to clean up completed or failed SparkApplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-completed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all SparkApplications with their states\n",
    "if TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    apps = krono.list_spark_applications(K8S_NAMESPACE)\n",
    "    \n",
    "    terminal_states = {'COMPLETED', 'FAILED', 'SUBMISSION_FAILED'}\n",
    "    to_cleanup = [app for app in apps if app['state'] in terminal_states]\n",
    "    \n",
    "    if to_cleanup:\n",
    "        print(f\"SparkApplications available for cleanup in '{K8S_NAMESPACE}':\\n\")\n",
    "        for app in to_cleanup:\n",
    "            print(f\"  - {app['name']} ({app['state']})\")\n",
    "        print(f\"\\nTotal: {len(to_cleanup)} application(s)\")\n",
    "    else:\n",
    "        print(\"No completed/failed SparkApplications to clean up\")\n",
    "else:\n",
    "    print(\"Using dbt runner - skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific SparkApplication\n",
    "APP_TO_DELETE = None  # Set to app name to delete\n",
    "\n",
    "if APP_TO_DELETE and TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    print(f\"Deleting SparkApplication: {APP_TO_DELETE}\")\n",
    "    if krono.delete_spark_application(APP_TO_DELETE, K8S_NAMESPACE):\n",
    "        krono.print_status('success', f\"Deleted {APP_TO_DELETE}\")\n",
    "    else:\n",
    "        krono.print_status('error', f\"Failed to delete {APP_TO_DELETE}\")\n",
    "else:\n",
    "    print(\"Set APP_TO_DELETE to delete a SparkApplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ALL completed/failed SparkApplications (use with caution!)\n",
    "CLEANUP_ALL = False  # Set to True to delete all completed/failed apps\n",
    "\n",
    "if CLEANUP_ALL and TRANSFORM_RUNNER == \"spark-operator\":\n",
    "    apps = krono.list_spark_applications(K8S_NAMESPACE)\n",
    "    terminal_states = {'COMPLETED', 'FAILED', 'SUBMISSION_FAILED'}\n",
    "    to_cleanup = [app for app in apps if app['state'] in terminal_states]\n",
    "    \n",
    "    if to_cleanup:\n",
    "        print(f\"Deleting {len(to_cleanup)} SparkApplication(s)...\\n\")\n",
    "        for app in to_cleanup:\n",
    "            if krono.delete_spark_application(app['name'], K8S_NAMESPACE):\n",
    "                krono.print_status('success', f\"Deleted {app['name']}\")\n",
    "            else:\n",
    "                krono.print_status('error', f\"Failed to delete {app['name']}\")\n",
    "    else:\n",
    "        print(\"No applications to clean up\")\n",
    "else:\n",
    "    print(\"Set CLEANUP_ALL = True to delete all completed/failed SparkApplications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-reference",
   "metadata": {},
   "source": [
    "## Quick Reference Commands\n",
    "\n",
    "Useful kubectl commands for debugging:\n",
    "\n",
    "```bash\n",
    "# List all SparkApplications\n",
    "kubectl -n dfp get sparkapplication\n",
    "\n",
    "# Describe a SparkApplication\n",
    "kubectl -n dfp describe sparkapplication <app-name>\n",
    "\n",
    "# View driver logs\n",
    "kubectl -n dfp logs <app-name>-driver\n",
    "\n",
    "# Follow driver logs\n",
    "kubectl -n dfp logs -f <app-name>-driver\n",
    "\n",
    "# View Spark Operator logs\n",
    "kubectl -n dfp logs deployment/spark-operator\n",
    "\n",
    "# Delete a SparkApplication\n",
    "kubectl -n dfp delete sparkapplication <app-name>\n",
    "\n",
    "# Pre-load Spark image into kind\n",
    "docker pull apache/spark:3.5.7-python3\n",
    "kind load docker-image apache/spark:3.5.7-python3 --name dfp-kind\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
