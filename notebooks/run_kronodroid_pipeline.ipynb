{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronodroid Pipeline Runner (Notebook)\n",
    "\n",
    "Notebook equivalent of `tools/scripts/run_kronodroid_pipeline.py`.\n",
    "\n",
    "Data flow:\n",
    "```\n",
    "Kaggle → dlt → Parquet/Avro → MinIO → Spark + dbt → Iceberg (LakeFS) → Feast → (optional) LakeFS commit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Environment setup\n",
    "\n",
    "- Loads `.env` from repo root (same behavior as the script).\n",
    "- Ensures the repo is importable from `notebooks/`.\n",
    "- Sets `LAKEFS_BRANCH` from the parameters cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repo root is importable when running from the notebooks directory.\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from tools.scripts import run_kronodroid_pipeline as krono\n",
    "\n",
    "# Load env vars from `.env` (if present)\n",
    "krono.load_env_file(REPO_ROOT / \".env\")\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"LAKEFS_REPOSITORY: {os.getenv('LAKEFS_REPOSITORY', 'kronodroid')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run parameters\n",
    "\n",
    "Mirror the CLI flags from `tools/scripts/run_kronodroid_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "BRANCH = \"main\"\n",
    "DBT_TARGET = \"dev\"  # \"dev\" (embedded Spark) or \"thrift\" (Spark server)\n",
    "FILE_FORMAT = \"parquet\"  # \"parquet\" or \"avro\" (dlt loader uses parquet for avro requests)\n",
    "\n",
    "# Step toggles\n",
    "SKIP_INGESTION = False\n",
    "SKIP_DBT = False\n",
    "SKIP_FEAST_APPLY = False\n",
    "SKIP_MATERIALIZE = False\n",
    "SKIP_COMMIT = False\n",
    "\n",
    "# Materialize-only mode (equivalent to `--materialize-only`)\n",
    "MATERIALIZE_ONLY = False\n",
    "MATERIALIZE_DAYS = 30\n",
    "\n",
    "os.environ[\"LAKEFS_BRANCH\"] = BRANCH\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"BRANCH\": BRANCH,\n",
    "        \"DBT_TARGET\": DBT_TARGET,\n",
    "        \"FILE_FORMAT\": FILE_FORMAT,\n",
    "        \"MATERIALIZE_ONLY\": MATERIALIZE_ONLY,\n",
    "        \"MATERIALIZE_DAYS\": MATERIALIZE_DAYS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Optional: quick dependency check\n",
    "\n",
    "The pipeline uses the `dbt` and `feast` CLIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print({\"dbt\": shutil.which(\"dbt\"), \"feast\": shutil.which(\"feast\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run the pipeline\n",
    "\n",
    "Run the cells below step-by-step, or run the \"Run full pipeline\" cell to mirror the script's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: dlt ingestion (Kaggle → MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_INGESTION:\n",
    "    ok = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"dlt ingestion failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: dbt-spark transformations (MinIO → Iceberg on LakeFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_DBT:\n",
    "    ok = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"dbt transformations failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feast apply (register feature definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_FEAST_APPLY:\n",
    "    ok = krono.run_feast_apply()\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"feast apply failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feast materialize (offline → online store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_MATERIALIZE:\n",
    "    ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS)\n",
    "    if not ok:\n",
    "        print(\"WARNING: feature materialization failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Commit to LakeFS (Iceberg tables → commit)\n",
    "\n",
    "Uses `engines.spark_engine.dfp_spark.iceberg_catalog.commit_iceberg_changes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not MATERIALIZE_ONLY and not SKIP_COMMIT:\n",
    "    krono.commit_to_lakefs(\n",
    "        branch=BRANCH,\n",
    "        message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run full pipeline (script-like)\n",
    "\n",
    "Convenience cell mirroring the script's `main()` flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = True\n",
    "\n",
    "if MATERIALIZE_ONLY:\n",
    "    success = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS)\n",
    "else:\n",
    "    if not SKIP_INGESTION:\n",
    "        success = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at dlt ingestion\")\n",
    "\n",
    "    if not SKIP_DBT:\n",
    "        success = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at dbt transformations\")\n",
    "\n",
    "    if not SKIP_FEAST_APPLY:\n",
    "        success = krono.run_feast_apply()\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at feast apply\")\n",
    "\n",
    "    if not SKIP_MATERIALIZE:\n",
    "        ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS)\n",
    "        if not ok:\n",
    "            print(\"WARNING: feature materialization failed\")\n",
    "\n",
    "    if not SKIP_COMMIT:\n",
    "        krono.commit_to_lakefs(\n",
    "            branch=BRANCH,\n",
    "            message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "        )\n",
    "\n",
    "print({\"success\": bool(success)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
