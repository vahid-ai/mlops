{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronodroid Pipeline Runner (Notebook)\n",
    "\n",
    "Notebook equivalent of `tools/scripts/run_kronodroid_pipeline.py`.\n",
    "\n",
    "Data flow:\n",
    "```\n",
    "Kaggle → dlt → Parquet/Avro → MinIO → Spark + dbt → Iceberg (LakeFS) → Feast → (optional) LakeFS commit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Environment setup\n",
    "\n",
    "- Loads `.env` from repo root (same behavior as the script).\n",
    "- Ensures the repo is importable from `notebooks/`.\n",
    "- Sets `LAKEFS_BRANCH` from the parameters cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde6df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/benjaminbrown/Documents/GitHub/mlops\n",
      "LAKEFS_REPOSITORY: kronodroid\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repo root is importable when running from the notebooks directory.\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from tools.scripts import run_kronodroid_pipeline as krono\n",
    "\n",
    "# Load env vars from `.env` (if present)\n",
    "krono.load_env_file(REPO_ROOT / \".env\")\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"LAKEFS_REPOSITORY: {os.getenv('LAKEFS_REPOSITORY', 'kronodroid')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e422ee",
   "metadata": {},
   "source": [
    "## 2) Run parameters\n",
    "\n",
    "Mirror the CLI flags from `tools/scripts/run_kronodroid_pipeline.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086faewjqlec",
   "metadata": {},
   "source": [
    "**Important Note on Spark Connectivity:**\n",
    "\n",
    "When using `TRANSFORM_RUNNER = \"spark-operator\"`:\n",
    "- The transformations run in the Kind cluster using Spark Operator (no local Spark needed)\n",
    "- However, **Feast with Spark offline store still requires Spark connectivity** to register features and materialize\n",
    "- Setting `SKIP_SPARK_CHECK = True` bypasses the connectivity check, but Feast may still fail if Spark is unavailable\n",
    "\n",
    "**Options:**\n",
    "1. **Use Spark Thrift Server** (recommended): Deploy it even when using spark-operator\n",
    "   ```bash\n",
    "   kubectl apply -k infra/k8s/kind/addons/spark-thrift/\n",
    "   kubectl -n dfp wait --for=condition=ready pod -l app=spark-thrift-server --timeout=120s\n",
    "   ```\n",
    "2. **Skip Feast steps**: Set `SKIP_FEAST_APPLY = True` and `SKIP_MATERIALIZE = True`\n",
    "3. **Accept Feast failures**: Let `SKIP_SPARK_CHECK = True` and catch any Feast errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42d901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BRANCH': 'main', 'TRANSFORM_RUNNER': 'spark-operator', 'DBT_TARGET': 'dev', 'FILE_FORMAT': 'parquet', 'SKIP_SPARK_CHECK': True, 'MATERIALIZE_ONLY': False, 'MATERIALIZE_DAYS': 30}\n"
     ]
    }
   ],
   "source": [
    "# Core parameters\n",
    "BRANCH = \"main\"\n",
    "TRANSFORM_RUNNER = \"spark-operator\"  # \"dbt\" or \"spark-operator\"\n",
    "DBT_TARGET = \"dev\"  # \"dev\" (embedded Spark) or \"thrift\" (Spark server)\n",
    "K8S_NAMESPACE = \"dfp\"  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "SPARK_IMAGE = \"apache/spark:3.5.7-python3\"  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "SPARK_TIMEOUT_SECONDS = 60 * 30  # used when TRANSFORM_RUNNER == \"spark-operator\"\n",
    "FILE_FORMAT = \"parquet\"  # \"parquet\" or \"avro\" (dlt loader uses parquet for avro requests)\n",
    "\n",
    "# Step toggles\n",
    "SKIP_INGESTION = False\n",
    "SKIP_DBT = False\n",
    "SKIP_FEAST_APPLY = False\n",
    "SKIP_MATERIALIZE = False\n",
    "SKIP_COMMIT = False\n",
    "\n",
    "# Spark check: Set to False when using spark-operator (Feast will try to connect anyway)\n",
    "# If Feast fails, you may need to start Spark Thrift Server or skip Feast steps\n",
    "SKIP_SPARK_CHECK = True if TRANSFORM_RUNNER == \"spark-operator\" else False\n",
    "\n",
    "# Materialize-only mode (equivalent to `--materialize-only`)\n",
    "MATERIALIZE_ONLY = False\n",
    "MATERIALIZE_DAYS = 30\n",
    "\n",
    "os.environ[\"LAKEFS_BRANCH\"] = BRANCH\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"BRANCH\": BRANCH,\n",
    "        \"TRANSFORM_RUNNER\": TRANSFORM_RUNNER,\n",
    "        \"DBT_TARGET\": DBT_TARGET,\n",
    "        \"FILE_FORMAT\": FILE_FORMAT,\n",
    "        \"SKIP_SPARK_CHECK\": SKIP_SPARK_CHECK,\n",
    "        \"MATERIALIZE_ONLY\": MATERIALIZE_ONLY,\n",
    "        \"MATERIALIZE_DAYS\": MATERIALIZE_DAYS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de648110",
   "metadata": {},
   "source": [
    "## 3) Optional: quick dependency check\n",
    "\n",
    "The pipeline uses the `dbt` and `feast` CLIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9cd2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dbt': '/Users/benjaminbrown/Documents/GitHub/mlops/.venv/bin/dbt', 'feast': '/Users/benjaminbrown/Documents/GitHub/mlops/.venv/bin/feast', 'kubectl': '/usr/local/bin/kubectl'}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "print({\"dbt\": shutil.which(\"dbt\"), \"feast\": shutil.which(\"feast\"), \"kubectl\": shutil.which(\"kubectl\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f62eb",
   "metadata": {},
   "source": [
    "## 4) Run the pipeline\n",
    "\n",
    "Run the cells below step-by-step, or run the \"Run full pipeline\" cell to mirror the script's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcef05",
   "metadata": {},
   "source": [
    "### Step 1: dlt ingestion (Kaggle → MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c264ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 1: Running dlt ingestion from Kaggle → Parquet → MinIO\n",
      "============================================================\n",
      "Created bucket: dlt-data\n",
      "Dataset URL: https://www.kaggle.com/datasets/dhoogla/kronodroid-2021\n",
      "Pipeline completed: Pipeline kronodroid_minio load step completed in 0.61 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset kronodroid_raw\n",
      "The filesystem destination used s3://dlt-data location to store data\n",
      "Load package 1767916086.435871 is LOADED and contains no failed jobs\n",
      "  - Format requested: parquet\n",
      "  - Loader format used: parquet\n",
      "  - Dataset: kronodroid_raw\n",
      "dlt pipeline completed successfully\n",
      "  - Dataset: kronodroid_raw\n",
      "  - Format: parquet\n",
      "  - Destination: MinIO\n"
     ]
    }
   ],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_INGESTION:\n",
    "    ok = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"dlt ingestion failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15a899",
   "metadata": {},
   "source": [
    "### Step 2: transformations (MinIO → Iceberg on LakeFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e2b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 2 (alt): Running Spark Operator transformations → Iceberg tables\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     ok = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     ok = \u001b[43mkrono\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_kubeflow_spark_operator_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBRANCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK8S_NAMESPACE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspark_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSPARK_IMAGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSPARK_TIMEOUT_SECONDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtransformations failed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mlops/tools/scripts/run_kronodroid_pipeline.py:230\u001b[39m, in \u001b[36mrun_kubeflow_spark_operator_transformations\u001b[39m\u001b[34m(branch, namespace, spark_image, timeout_seconds)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01morchestration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkubeflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdfp_kfp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkronodroid_spark_operator_transform_component\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    205\u001b[39m     KronodroidSparkOperatorConfig,\n\u001b[32m    206\u001b[39m     run,\n\u001b[32m    207\u001b[39m )\n\u001b[32m    209\u001b[39m cfg = KronodroidSparkOperatorConfig(\n\u001b[32m    210\u001b[39m     namespace=namespace,\n\u001b[32m    211\u001b[39m     spark_image=spark_image,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     lakefs_secret_access_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mLAKEFS_SECRET_ACCESS_KEY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    228\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m ok = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark Operator transformations completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mlops/orchestration/kubeflow/dfp_kfp/components/kronodroid_spark_operator_transform_component.py:247\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(cfg, application_name)\u001b[39m\n\u001b[32m    243\u001b[39m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    245\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    250\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimed out waiting for SparkApplication \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapplication_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.timeout_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    251\u001b[39m     )\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_DBT:\n",
    "    if TRANSFORM_RUNNER == \"dbt\":\n",
    "        ok = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "    else:\n",
    "        ok = krono.run_kubeflow_spark_operator_transformations(\n",
    "            branch=BRANCH,\n",
    "            namespace=K8S_NAMESPACE,\n",
    "            spark_image=SPARK_IMAGE,\n",
    "            timeout_seconds=SPARK_TIMEOUT_SECONDS,\n",
    "        )\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"transformations failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407645a",
   "metadata": {},
   "source": [
    "### Step 3: Feast apply (register feature definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MATERIALIZE_ONLY and not SKIP_FEAST_APPLY:\n",
    "    check_spark = not SKIP_SPARK_CHECK\n",
    "    ok = krono.run_feast_apply(check_spark=check_spark)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"feast apply failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f488802",
   "metadata": {},
   "source": [
    "### Step 4: Feast materialize (offline → online store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_MATERIALIZE:\n",
    "    check_spark = not SKIP_SPARK_CHECK\n",
    "    ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "    if not ok:\n",
    "        print(\"WARNING: feature materialization failed\")\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bcbcf",
   "metadata": {},
   "source": [
    "### Step 5: Commit to LakeFS (Iceberg tables → commit)\n",
    "\n",
    "Uses `engines.spark_engine.dfp_spark.iceberg_catalog.commit_iceberg_changes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed13d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not MATERIALIZE_ONLY and not SKIP_COMMIT:\n",
    "    krono.commit_to_lakefs(\n",
    "        branch=BRANCH,\n",
    "        message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7dfca",
   "metadata": {},
   "source": [
    "## Run full pipeline (script-like)\n",
    "\n",
    "Convenience cell mirroring the script's `main()` flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046da518",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = True\n",
    "check_spark = not SKIP_SPARK_CHECK\n",
    "\n",
    "if MATERIALIZE_ONLY:\n",
    "    success = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "else:\n",
    "    if not SKIP_INGESTION:\n",
    "        success = krono.run_dlt_ingestion(file_format=FILE_FORMAT)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at dlt ingestion\")\n",
    "\n",
    "    if not SKIP_DBT:\n",
    "        if TRANSFORM_RUNNER == \"dbt\":\n",
    "            success = krono.run_dbt_spark_transformations(target=DBT_TARGET)\n",
    "        else:\n",
    "            success = krono.run_kubeflow_spark_operator_transformations(\n",
    "                branch=BRANCH,\n",
    "                namespace=K8S_NAMESPACE,\n",
    "                spark_image=SPARK_IMAGE,\n",
    "                timeout_seconds=SPARK_TIMEOUT_SECONDS,\n",
    "            )\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at transformations\")\n",
    "\n",
    "    if not SKIP_FEAST_APPLY:\n",
    "        success = krono.run_feast_apply(check_spark=check_spark)\n",
    "        if not success:\n",
    "            raise RuntimeError(\"Pipeline failed at feast apply\")\n",
    "\n",
    "    if not SKIP_MATERIALIZE:\n",
    "        ok = krono.run_feast_materialize(days_back=MATERIALIZE_DAYS, check_spark=check_spark)\n",
    "        if not ok:\n",
    "            print(\"WARNING: feature materialization failed\")\n",
    "\n",
    "    if not SKIP_COMMIT:\n",
    "        krono.commit_to_lakefs(\n",
    "            branch=BRANCH,\n",
    "            message=f\"Notebook run: Iceberg tables updated {datetime.now().isoformat()}\",\n",
    "        )\n",
    "\n",
    "print({\"success\": bool(success)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
