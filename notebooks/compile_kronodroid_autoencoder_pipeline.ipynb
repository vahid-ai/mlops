{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Kronodroid Autoencoder Pipeline\n",
    "\n",
    "This notebook compiles the Kronodroid Autoencoder training pipeline to a YAML file\n",
    "that can be submitted to Kubeflow Pipelines.\n",
    "\n",
    "## Pipeline Variants\n",
    "\n",
    "There are two pipeline variants:\n",
    "\n",
    "1. **Training Pipeline** (`kronodroid_autoencoder_training_pipeline`): Training-only, assumes data is already in LakeFS\n",
    "2. **Full Pipeline** (`kronodroid_autoencoder_full_pipeline`): Spark transform -> LakeFS commit -> Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Project Path\n",
    "\n",
    "**Run this cell first** to ensure the project modules can be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root (directory containing pyproject.toml)\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "# Verify we found the right directory\n",
    "if not (project_root / \"pyproject.toml\").exists():\n",
    "    # Try current directory\n",
    "    if (notebook_dir / \"pyproject.toml\").exists():\n",
    "        project_root = notebook_dir\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot find project root. Current dir: {notebook_dir}\")\n",
    "\n",
    "# Add to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root for consistent paths\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes project: {str(project_root) in sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check KFP Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "print(f\"KFP version: {kfp.__version__}\")\n",
    "\n",
    "# Verify kfp-kubernetes is available\n",
    "try:\n",
    "    from kfp import kubernetes\n",
    "    print(\"kfp-kubernetes: available\")\n",
    "except ImportError:\n",
    "    print(\"kfp-kubernetes: NOT INSTALLED - run 'uv add kfp-kubernetes'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.kubeflow.dfp_kfp.pipelines.kronodroid_autoencoder_pipeline import (\n",
    "    kronodroid_autoencoder_training_pipeline,\n",
    "    kronodroid_autoencoder_full_pipeline,\n",
    "    compile_kronodroid_autoencoder_pipeline,\n",
    ")\n",
    "\n",
    "print(\"Pipelines imported successfully!\")\n",
    "print(f\"Training pipeline: {kronodroid_autoencoder_training_pipeline.name}\")\n",
    "print(f\"Full pipeline: {kronodroid_autoencoder_full_pipeline.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Training Pipeline\n",
    "\n",
    "This pipeline assumes data is already transformed and available in LakeFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# Output path for the training pipeline\n",
    "training_output = Path(\"kronodroid_autoencoder_pipeline.yaml\")\n",
    "\n",
    "# Compile the training-only pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=kronodroid_autoencoder_training_pipeline,\n",
    "    package_path=str(training_output),\n",
    ")\n",
    "\n",
    "print(f\"Training pipeline compiled to: {training_output.absolute()}\")\n",
    "print(f\"File size: {training_output.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Full Pipeline (Optional)\n",
    "\n",
    "This pipeline includes Spark transformation, LakeFS commit, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output path for the full pipeline\n",
    "full_output = Path(\"kronodroid_autoencoder_full_pipeline.yaml\")\n",
    "\n",
    "# Compile the full pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=kronodroid_autoencoder_full_pipeline,\n",
    "    package_path=str(full_output),\n",
    ")\n",
    "\n",
    "print(f\"Full pipeline compiled to: {full_output.absolute()}\")\n",
    "print(f\"File size: {full_output.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Compiled YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 80 lines of compiled training pipeline\n",
    "with open(training_output) as f:\n",
    "    lines = f.readlines()[:80]\n",
    "    print(f\"First 80 lines of {training_output.name}:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kubeflow Pipelines (Optional)\n",
    "\n",
    "If you have a KFP server running, you can submit the pipeline directly.\n",
    "\n",
    "First, start the KFP port-forward:\n",
    "```bash\n",
    "task kfp:port-forward\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "KFP_HOST = \"http://localhost:8080\"\n",
    "SUBMIT_PIPELINE = False  # Set to True to submit\n",
    "\n",
    "if SUBMIT_PIPELINE:\n",
    "    client = kfp.Client(host=KFP_HOST)\n",
    "    \n",
    "    # Create a run with default parameters\n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        kronodroid_autoencoder_training_pipeline,\n",
    "        arguments={\n",
    "            \"lakefs_ref\": \"main\",\n",
    "            \"max_epochs\": 5,\n",
    "            \"batch_size\": 256,\n",
    "        },\n",
    "        experiment_name=\"kronodroid-autoencoder\",\n",
    "        run_name=\"autoencoder-training-test\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Submitted run: {run.run_id}\")\n",
    "    print(f\"View at: {KFP_HOST}/#/runs/details/{run.run_id}\")\n",
    "else:\n",
    "    print(\"Set SUBMIT_PIPELINE = True to submit the pipeline\")\n",
    "    print(f\"Or submit via CLI: kfp run create -f {training_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Parameters Reference\n",
    "\n",
    "### Training Pipeline Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `lakefs_endpoint` | `http://lakefs:8000` | LakeFS endpoint |\n",
    "| `lakefs_repository` | `kronodroid` | LakeFS repository |\n",
    "| `lakefs_ref` | `main` | LakeFS branch/commit to read from |\n",
    "| `mlflow_tracking_uri` | `http://mlflow:5000` | MLflow tracking server |\n",
    "| `mlflow_experiment_name` | `kronodroid-autoencoder` | MLflow experiment |\n",
    "| `mlflow_model_name` | `kronodroid_autoencoder` | Registered model name |\n",
    "| `latent_dim` | `16` | Autoencoder latent dimension |\n",
    "| `hidden_dims_json` | `[128, 64]` | Hidden layer dimensions |\n",
    "| `batch_size` | `512` | Training batch size |\n",
    "| `max_epochs` | `10` | Maximum training epochs |\n",
    "| `seed` | `1337` | Random seed |\n",
    "\n",
    "### Full Pipeline Additional Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `minio_endpoint` | `http://minio:9000` | MinIO endpoint for raw data |\n",
    "| `minio_bucket` | `dlt-data` | MinIO bucket |\n",
    "| `minio_prefix` | `kronodroid_raw` | Raw data prefix |\n",
    "| `spark_image` | `dfp-spark:latest` | Spark job Docker image |\n",
    "| `target_branch` | `main` | LakeFS branch to merge into |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
