{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Spark Operator Test Notebook\n",
    "\n",
    "Interactive notebook for testing and debugging the Kubeflow Spark Operator.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Kind cluster running with Spark Operator deployed (`task spark-operator:up`)\n",
    "- kubectl configured to access the cluster\n",
    "- Python environment with subprocess support\n",
    "\n",
    "## What This Notebook Tests\n",
    "\n",
    "1. **Spark Operator Deployment Status** - Verify the operator is running\n",
    "2. **CRD Availability** - Check SparkApplication CRD is installed\n",
    "3. **RBAC Configuration** - Verify service accounts and permissions\n",
    "4. **SparkApplication Submission** - Submit a test Pi calculation job\n",
    "5. **Application Monitoring** - Track job progress and state changes\n",
    "6. **Log Retrieval** - Get driver and executor logs\n",
    "7. **Troubleshooting** - Diagnose common issues\n",
    "8. **Cleanup** - Remove test resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace: dfp\n",
      "Spark Image: apache/spark:3.5.7-python3\n",
      "Spark Version: 3.5.7\n",
      "Service Account: spark-operator\n",
      "Cluster Name: dfp-kind\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "NAMESPACE = os.getenv('SPARK_NAMESPACE', 'dfp')\n",
    "SPARK_IMAGE = os.getenv('SPARK_IMAGE', 'apache/spark:3.5.7-python3')\n",
    "SPARK_VERSION = '3.5.7'\n",
    "SERVICE_ACCOUNT = 'spark-operator'\n",
    "CLUSTER_NAME = os.getenv('CLUSTER_NAME', 'dfp-kind')\n",
    "\n",
    "print(f\"Namespace: {NAMESPACE}\")\n",
    "print(f\"Spark Image: {SPARK_IMAGE}\")\n",
    "print(f\"Spark Version: {SPARK_VERSION}\")\n",
    "print(f\"Service Account: {SERVICE_ACCOUNT}\")\n",
    "print(f\"Cluster Name: {CLUSTER_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kubectl(args: List[str], *, check: bool = True, capture: bool = True) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run a kubectl command and return the result.\"\"\"\n",
    "    cmd = ['kubectl'] + args\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            check=check,\n",
    "            capture_output=capture,\n",
    "            text=True\n",
    "        )\n",
    "        return result\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {' '.join(cmd)}\")\n",
    "        if e.stdout:\n",
    "            print(f\"stdout: {e.stdout}\")\n",
    "        if e.stderr:\n",
    "            print(f\"stderr: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def kubectl_get_json(resource: str, name: str = '', namespace: str = NAMESPACE) -> Optional[Dict]:\n",
    "    \"\"\"Get a Kubernetes resource as JSON.\"\"\"\n",
    "    try:\n",
    "        args = ['-n', namespace, 'get', resource]\n",
    "        if name:\n",
    "            args.append(name)\n",
    "        args.extend(['-o', 'json'])\n",
    "        result = run_kubectl(args)\n",
    "        return json.loads(result.stdout)\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def print_status(status: str, message: str):\n",
    "    \"\"\"Print a status message with icon.\"\"\"\n",
    "    icons = {\n",
    "        'success': '[OK]',\n",
    "        'warning': '[WARN]',\n",
    "        'error': '[ERROR]',\n",
    "        'info': '[INFO]',\n",
    "        'pending': '[...]'\n",
    "    }\n",
    "    icon = icons.get(status, '[?]')\n",
    "    print(f\"{icon} {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Cluster Connectivity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cluster connectivity...\n",
      "\n",
      "[OK] Current context: kind-dfp-kind\n",
      "[OK] Cluster is accessible\n",
      "    Kubernetes control plane is running at https://127.0.0.1:54595\n",
      "[OK] Namespace 'dfp' exists\n"
     ]
    }
   ],
   "source": [
    "def check_cluster_connectivity() -> bool:\n",
    "    \"\"\"Verify kubectl can connect to the cluster.\"\"\"\n",
    "    print(\"Checking cluster connectivity...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get current context\n",
    "        result = run_kubectl(['config', 'current-context'])\n",
    "        context = result.stdout.strip()\n",
    "        print_status('success', f\"Current context: {context}\")\n",
    "        \n",
    "        # Check cluster info\n",
    "        result = run_kubectl(['cluster-info'], check=False)\n",
    "        if result.returncode == 0:\n",
    "            print_status('success', \"Cluster is accessible\")\n",
    "            # Extract control plane URL\n",
    "            for line in result.stdout.split('\\n'):\n",
    "                if 'control plane' in line.lower() or 'master' in line.lower():\n",
    "                    print(f\"    {line.strip()}\")\n",
    "        else:\n",
    "            print_status('error', \"Cannot connect to cluster\")\n",
    "            print(f\"    stderr: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        # Check namespace exists\n",
    "        result = run_kubectl(['-n', NAMESPACE, 'get', 'namespace', NAMESPACE], check=False)\n",
    "        if result.returncode == 0:\n",
    "            print_status('success', f\"Namespace '{NAMESPACE}' exists\")\n",
    "        else:\n",
    "            print_status('warning', f\"Namespace '{NAMESPACE}' does not exist\")\n",
    "            print(f\"    Create it with: kubectl create namespace {NAMESPACE}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print_status('error', \"kubectl not found in PATH\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "cluster_ok = check_cluster_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Spark Operator Deployment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Spark Operator deployment...\n",
      "\n",
      "[OK] Spark Operator image: ghcr.io/kubeflow/spark-operator:v1beta2-1.4.3-3.5.0\n",
      "[OK] Replicas: 1/1 ready\n",
      "[OK] Pod: spark-operator-6bf7947df-7tkbg (Running)\n",
      "[OK] Operator supports Spark 3.5.x\n"
     ]
    }
   ],
   "source": [
    "def check_spark_operator_deployment() -> Dict[str, Any]:\n",
    "    \"\"\"Check if the Spark Operator is deployed and running.\"\"\"\n",
    "    print(\"Checking Spark Operator deployment...\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'installed': False,\n",
    "        'ready': False,\n",
    "        'image': None,\n",
    "        'replicas': {'desired': 0, 'ready': 0},\n",
    "        'pods': []\n",
    "    }\n",
    "    \n",
    "    # Check deployment\n",
    "    deployment = kubectl_get_json('deployment', 'spark-operator')\n",
    "    \n",
    "    if not deployment:\n",
    "        print_status('error', \"Spark Operator deployment not found\")\n",
    "        print(f\"\\n    To install Spark Operator:\")\n",
    "        print(f\"    kubectl apply -k infra/k8s/kind/addons/spark-operator/\")\n",
    "        print(f\"    # or: task spark-operator:up\")\n",
    "        return result\n",
    "    \n",
    "    result['installed'] = True\n",
    "    \n",
    "    # Get image\n",
    "    containers = deployment.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "    if containers:\n",
    "        result['image'] = containers[0].get('image', 'unknown')\n",
    "        print_status('success', f\"Spark Operator image: {result['image']}\")\n",
    "    \n",
    "    # Check replicas\n",
    "    status = deployment.get('status', {})\n",
    "    result['replicas']['desired'] = status.get('replicas', 0)\n",
    "    result['replicas']['ready'] = status.get('readyReplicas', 0)\n",
    "    \n",
    "    if result['replicas']['ready'] >= 1:\n",
    "        result['ready'] = True\n",
    "        print_status('success', f\"Replicas: {result['replicas']['ready']}/{result['replicas']['desired']} ready\")\n",
    "    else:\n",
    "        print_status('warning', f\"Replicas: {result['replicas']['ready']}/{result['replicas']['desired']} ready\")\n",
    "    \n",
    "    # Get pod details\n",
    "    pods = kubectl_get_json('pods', '', NAMESPACE)\n",
    "    if pods:\n",
    "        for pod in pods.get('items', []):\n",
    "            name = pod.get('metadata', {}).get('name', '')\n",
    "            if 'spark-operator' in name:\n",
    "                phase = pod.get('status', {}).get('phase', 'Unknown')\n",
    "                result['pods'].append({'name': name, 'phase': phase})\n",
    "                status_icon = 'success' if phase == 'Running' else 'warning'\n",
    "                print_status(status_icon, f\"Pod: {name} ({phase})\")\n",
    "    \n",
    "    # Check Spark version compatibility\n",
    "    if result['image']:\n",
    "        if '3.5' in result['image']:\n",
    "            print_status('success', \"Operator supports Spark 3.5.x\")\n",
    "        else:\n",
    "            print_status('warning', \"Operator version may not match Spark 3.5.x\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "operator_status = check_spark_operator_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_operator_logs(tail: int = 50) -> str:\n",
    "    \"\"\"Get recent logs from the Spark Operator.\"\"\"\n",
    "    print(f\"Spark Operator logs (last {tail} lines):\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        result = run_kubectl(['-n', NAMESPACE, 'logs', 'deployment/spark-operator', f'--tail={tail}'])\n",
    "        print(result.stdout)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to get logs: {e.stderr}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Uncomment to view operator logs\n",
    "# get_spark_operator_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. CRD and RBAC Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Spark Operator CRDs...\n",
      "\n",
      "[OK] CRD installed: scheduledsparkapplications.sparkoperator.k8s.io\n",
      "[OK] CRD installed: sparkapplications.sparkoperator.k8s.io\n"
     ]
    }
   ],
   "source": [
    "def check_spark_crds() -> Dict[str, bool]:\n",
    "    \"\"\"Check if SparkApplication CRDs are installed.\"\"\"\n",
    "    print(\"Checking Spark Operator CRDs...\\n\")\n",
    "    \n",
    "    crds = {\n",
    "        'sparkapplications.sparkoperator.k8s.io': False,\n",
    "        'scheduledsparkapplications.sparkoperator.k8s.io': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = run_kubectl(['get', 'crd', '-o', 'json'])\n",
    "        crd_list = json.loads(result.stdout)\n",
    "        \n",
    "        for item in crd_list.get('items', []):\n",
    "            name = item.get('metadata', {}).get('name', '')\n",
    "            if name in crds:\n",
    "                crds[name] = True\n",
    "                print_status('success', f\"CRD installed: {name}\")\n",
    "        \n",
    "        for crd, installed in crds.items():\n",
    "            if not installed:\n",
    "                print_status('error', f\"CRD missing: {crd}\")\n",
    "        \n",
    "        return crds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Failed to check CRDs: {e}\")\n",
    "        return crds\n",
    "\n",
    "\n",
    "crd_status = check_spark_crds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking RBAC configuration...\n",
      "\n",
      "[OK] ServiceAccount 'spark-operator' exists\n",
      "[OK] ClusterRole 'spark-operator' exists\n",
      "[OK] ClusterRoleBinding 'spark-operator' exists\n"
     ]
    }
   ],
   "source": [
    "def check_rbac_configuration() -> Dict[str, Any]:\n",
    "    \"\"\"Check RBAC configuration for Spark Operator.\"\"\"\n",
    "    print(\"Checking RBAC configuration...\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'service_account': False,\n",
    "        'cluster_role': False,\n",
    "        'cluster_role_binding': False\n",
    "    }\n",
    "    \n",
    "    # Check ServiceAccount\n",
    "    try:\n",
    "        sa = kubectl_get_json('serviceaccount', SERVICE_ACCOUNT)\n",
    "        if sa:\n",
    "            result['service_account'] = True\n",
    "            print_status('success', f\"ServiceAccount '{SERVICE_ACCOUNT}' exists\")\n",
    "        else:\n",
    "            print_status('error', f\"ServiceAccount '{SERVICE_ACCOUNT}' not found\")\n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Failed to check ServiceAccount: {e}\")\n",
    "    \n",
    "    # Check ClusterRole\n",
    "    try:\n",
    "        cr_result = run_kubectl(['get', 'clusterrole', 'spark-operator', '-o', 'json'], check=False)\n",
    "        if cr_result.returncode == 0:\n",
    "            result['cluster_role'] = True\n",
    "            print_status('success', \"ClusterRole 'spark-operator' exists\")\n",
    "        else:\n",
    "            print_status('warning', \"ClusterRole 'spark-operator' not found (may use different name)\")\n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Failed to check ClusterRole: {e}\")\n",
    "    \n",
    "    # Check ClusterRoleBinding\n",
    "    try:\n",
    "        crb_result = run_kubectl(['get', 'clusterrolebinding', 'spark-operator', '-o', 'json'], check=False)\n",
    "        if crb_result.returncode == 0:\n",
    "            result['cluster_role_binding'] = True\n",
    "            print_status('success', \"ClusterRoleBinding 'spark-operator' exists\")\n",
    "        else:\n",
    "            print_status('warning', \"ClusterRoleBinding 'spark-operator' not found (may use different name)\")\n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Failed to check ClusterRoleBinding: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "rbac_status = check_rbac_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Submit Test SparkApplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test application name: spark-test-20260112-125105\n"
     ]
    }
   ],
   "source": [
    "def generate_test_spark_application(app_name: str) -> str:\n",
    "    \"\"\"Generate a simple test SparkApplication YAML (Pi calculation).\"\"\"\n",
    "    return f\"\"\"apiVersion: sparkoperator.k8s.io/v1beta2\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: {app_name}\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  type: Scala\n",
    "  mode: cluster\n",
    "  sparkVersion: \"{SPARK_VERSION}\"\n",
    "  image: \"{SPARK_IMAGE}\"\n",
    "  imagePullPolicy: IfNotPresent\n",
    "  mainClass: org.apache.spark.examples.SparkPi\n",
    "  mainApplicationFile: \"local:///opt/spark/examples/jars/spark-examples_2.12-{SPARK_VERSION}.jar\"\n",
    "  arguments:\n",
    "    - \"100\"\n",
    "  sparkConf:\n",
    "    \"spark.jars.ivy\": \"/tmp/.ivy2\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  driver:\n",
    "    cores: 1\n",
    "    memory: \"512m\"\n",
    "    serviceAccount: {SERVICE_ACCOUNT}\n",
    "    env:\n",
    "      - name: HOME\n",
    "        value: \"/tmp\"\n",
    "  executor:\n",
    "    instances: 1\n",
    "    cores: 1\n",
    "    memory: \"512m\"\n",
    "    serviceAccount: {SERVICE_ACCOUNT}\n",
    "    env:\n",
    "      - name: HOME\n",
    "        value: \"/tmp\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_python_test_spark_application(app_name: str) -> str:\n",
    "    \"\"\"Generate a Python test SparkApplication YAML.\"\"\"\n",
    "    return f\"\"\"apiVersion: sparkoperator.k8s.io/v1beta2\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: {app_name}\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  type: Python\n",
    "  mode: cluster\n",
    "  pythonVersion: \"3\"\n",
    "  sparkVersion: \"{SPARK_VERSION}\"\n",
    "  image: \"{SPARK_IMAGE}\"\n",
    "  imagePullPolicy: IfNotPresent\n",
    "  mainApplicationFile: \"local:///opt/spark/examples/src/main/python/pi.py\"\n",
    "  arguments:\n",
    "    - \"100\"\n",
    "  sparkConf:\n",
    "    \"spark.jars.ivy\": \"/tmp/.ivy2\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  driver:\n",
    "    cores: 1\n",
    "    memory: \"512m\"\n",
    "    serviceAccount: {SERVICE_ACCOUNT}\n",
    "    env:\n",
    "      - name: HOME\n",
    "        value: \"/tmp\"\n",
    "  executor:\n",
    "    instances: 1\n",
    "    cores: 1\n",
    "    memory: \"512m\"\n",
    "    serviceAccount: {SERVICE_ACCOUNT}\n",
    "    env:\n",
    "      - name: HOME\n",
    "        value: \"/tmp\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Generate a unique application name\n",
    "TEST_APP_NAME = f\"spark-test-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "print(f\"Test application name: {TEST_APP_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting SparkApplication: spark-test-20260112-125105\n",
      "\n",
      "[INFO] Using Python Pi example\n",
      "\n",
      "SparkApplication YAML:\n",
      "\n",
      "------------------------------------------------------------\n",
      "apiVersion: sparkoperator.k8s.io/v1beta2\n",
      "kind: SparkApplication\n",
      "metadata:\n",
      "  name: spark-test-20260112-125105\n",
      "  namespace: dfp\n",
      "spec:\n",
      "  type: Python\n",
      "  mode: cluster\n",
      "  pythonVersion: \"3\"\n",
      "  sparkVersion: \"3.5.7\"\n",
      "  image: \"apache/spark:3.5.7-python3\"\n",
      "  imagePullPolicy: IfNotPresent\n",
      "  mainApplicationFile: \"local:///opt/spark/examples/src/main/python/pi.py\"\n",
      "  arguments:\n",
      "    - \"100\"\n",
      "  sparkConf:\n",
      "    \"spark.jars.ivy\": \"/tmp/.ivy2\"\n",
      "  restartPolicy:\n",
      "    type: Never\n",
      "  driver:\n",
      "    cores: 1\n",
      "    memory: \"512m\"\n",
      "    serviceAccount: spark-operator\n",
      "    env:\n",
      "      - name: HOME\n",
      "        value: \"/tmp\"\n",
      "  executor:\n",
      "    instances: 1\n",
      "    cores: 1\n",
      "    memory: \"512m\"\n",
      "    serviceAccount: spark-operator\n",
      "    env:\n",
      "      - name: HOME\n",
      "        value: \"/tmp\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "[OK] SparkApplication created: spark-test-20260112-125105\n",
      "    sparkapplication.sparkoperator.k8s.io/spark-test-20260112-125105 created\n"
     ]
    }
   ],
   "source": [
    "def submit_spark_application(app_name: str, use_python: bool = True) -> bool:\n",
    "    \"\"\"Submit a test SparkApplication to the cluster.\"\"\"\n",
    "    print(f\"Submitting SparkApplication: {app_name}\\n\")\n",
    "    \n",
    "    # Generate YAML\n",
    "    if use_python:\n",
    "        yaml_content = generate_python_test_spark_application(app_name)\n",
    "        print_status('info', \"Using Python Pi example\")\n",
    "    else:\n",
    "        yaml_content = generate_test_spark_application(app_name)\n",
    "        print_status('info', \"Using Scala SparkPi example\")\n",
    "    \n",
    "    print(f\"\\nSparkApplication YAML:\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    print(yaml_content)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Apply the YAML\n",
    "        result = subprocess.run(\n",
    "            ['kubectl', 'apply', '-f', '-'],\n",
    "            input=yaml_content,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            capture_output=True\n",
    "        )\n",
    "        print_status('success', f\"SparkApplication created: {app_name}\")\n",
    "        print(f\"    {result.stdout.strip()}\")\n",
    "        return True\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print_status('error', f\"Failed to create SparkApplication\")\n",
    "        print(f\"    stderr: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Submit the test application (Python by default)\n",
    "# Set use_python=False to use Scala SparkPi instead\n",
    "submitted = submit_spark_application(TEST_APP_NAME, use_python=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Monitor SparkApplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current status of spark-test-20260112-125105:\n",
      "\n",
      "[INFO] State: UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "def get_spark_application_status(app_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get the current status of a SparkApplication.\"\"\"\n",
    "    app = kubectl_get_json('sparkapplication', app_name)\n",
    "    \n",
    "    if not app:\n",
    "        return {'found': False, 'state': 'NOT_FOUND'}\n",
    "    \n",
    "    status = app.get('status', {})\n",
    "    app_state = status.get('applicationState', {})\n",
    "    \n",
    "    return {\n",
    "        'found': True,\n",
    "        'state': app_state.get('state', 'UNKNOWN'),\n",
    "        'error_message': app_state.get('errorMessage', ''),\n",
    "        'driver_info': status.get('driverInfo', {}),\n",
    "        'executor_state': status.get('executorState', {}),\n",
    "        'last_submission_attempt_time': status.get('lastSubmissionAttemptTime', ''),\n",
    "        'termination_time': status.get('terminationTime', ''),\n",
    "        'spark_application_id': status.get('sparkApplicationId', '')\n",
    "    }\n",
    "\n",
    "\n",
    "def print_spark_application_status(app_name: str):\n",
    "    \"\"\"Print formatted SparkApplication status.\"\"\"\n",
    "    status = get_spark_application_status(app_name)\n",
    "    \n",
    "    if not status['found']:\n",
    "        print_status('error', f\"SparkApplication '{app_name}' not found\")\n",
    "        return status\n",
    "    \n",
    "    state = status['state']\n",
    "    state_icons = {\n",
    "        'COMPLETED': 'success',\n",
    "        'RUNNING': 'info',\n",
    "        'SUBMITTED': 'pending',\n",
    "        'PENDING_RERUN': 'pending',\n",
    "        'FAILED': 'error',\n",
    "        'SUBMISSION_FAILED': 'error',\n",
    "        'FAILING': 'warning',\n",
    "        'INVALIDATING': 'warning'\n",
    "    }\n",
    "    \n",
    "    icon = state_icons.get(state, 'info')\n",
    "    print_status(icon, f\"State: {state}\")\n",
    "    \n",
    "    if status['spark_application_id']:\n",
    "        print(f\"    Spark Application ID: {status['spark_application_id']}\")\n",
    "    \n",
    "    if status['driver_info']:\n",
    "        driver = status['driver_info']\n",
    "        print(f\"    Driver Pod: {driver.get('podName', 'N/A')}\")\n",
    "        if driver.get('webUIAddress'):\n",
    "            print(f\"    Spark UI: {driver.get('webUIAddress')}\")\n",
    "    \n",
    "    if status['error_message']:\n",
    "        print(f\"    Error: {status['error_message']}\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "\n",
    "# Check current status\n",
    "print(f\"\\nCurrent status of {TEST_APP_NAME}:\\n\")\n",
    "current_status = print_spark_application_status(TEST_APP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring SparkApplication: spark-test-20260112-125105\n",
      "Timeout: 300s, Poll interval: 5s\n",
      "\n",
      "[12:52:26] [0s] State: UNKNOWN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     47\u001b[39m         time.sleep(poll_interval)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Monitor the test application\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m result = \u001b[43mmonitor_spark_application\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_APP_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mmonitor_spark_application\u001b[39m\u001b[34m(app_name, timeout_seconds, poll_interval)\u001b[39m\n\u001b[32m     44\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus[\u001b[33m'\u001b[39m\u001b[33merror_message\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m: state, \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m: status[\u001b[33m'\u001b[39m\u001b[33merror_message\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m'\u001b[39m: elapsed}\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def monitor_spark_application(app_name: str, timeout_seconds: int = 300, poll_interval: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Monitor a SparkApplication until completion or timeout.\"\"\"\n",
    "    print(f\"Monitoring SparkApplication: {app_name}\")\n",
    "    print(f\"Timeout: {timeout_seconds}s, Poll interval: {poll_interval}s\\n\")\n",
    "    \n",
    "    terminal_states = {'COMPLETED', 'FAILED', 'SUBMISSION_FAILED'}\n",
    "    start_time = time.time()\n",
    "    last_state = None\n",
    "    \n",
    "    while True:\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        \n",
    "        if elapsed > timeout_seconds:\n",
    "            print_status('warning', f\"Timeout after {elapsed}s\")\n",
    "            return {'success': False, 'reason': 'TIMEOUT', 'last_state': last_state}\n",
    "        \n",
    "        status = get_spark_application_status(app_name)\n",
    "        \n",
    "        if not status['found']:\n",
    "            print_status('warning', f\"[{elapsed}s] SparkApplication not found, waiting...\")\n",
    "            time.sleep(poll_interval)\n",
    "            continue\n",
    "        \n",
    "        state = status['state']\n",
    "        \n",
    "        # Print state changes\n",
    "        if state != last_state:\n",
    "            timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "            print(f\"[{timestamp}] [{elapsed}s] State: {state}\")\n",
    "            \n",
    "            if status['driver_info'].get('podName'):\n",
    "                print(f\"           Driver: {status['driver_info']['podName']}\")\n",
    "            \n",
    "            last_state = state\n",
    "        \n",
    "        # Check for terminal state\n",
    "        if state in terminal_states:\n",
    "            if state == 'COMPLETED':\n",
    "                print_status('success', f\"SparkApplication completed in {elapsed}s\")\n",
    "                return {'success': True, 'state': state, 'elapsed': elapsed}\n",
    "            else:\n",
    "                print_status('error', f\"SparkApplication {state} after {elapsed}s\")\n",
    "                if status['error_message']:\n",
    "                    print(f\"    Error: {status['error_message']}\")\n",
    "                return {'success': False, 'state': state, 'error': status['error_message'], 'elapsed': elapsed}\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "# Monitor the test application\n",
    "result = monitor_spark_application(TEST_APP_NAME, timeout_seconds=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Log Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver logs for spark-test-20260112-125105 (last 100 lines):\n",
      "\n",
      "================================================================================\n",
      "26/01/12 20:51:43 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 71 ms on 10.244.0.37 (executor 1) (90/100)\n",
      "26/01/12 20:51:43 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (10.244.0.37, executor 1, partition 91, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:43 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 70 ms on 10.244.0.37 (executor 1) (91/100)\n",
      "26/01/12 20:51:43 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (10.244.0.37, executor 1, partition 92, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:43 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 72 ms on 10.244.0.37 (executor 1) (92/100)\n",
      "26/01/12 20:51:43 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (10.244.0.37, executor 1, partition 93, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:43 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 71 ms on 10.244.0.37 (executor 1) (93/100)\n",
      "26/01/12 20:51:43 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (10.244.0.37, executor 1, partition 94, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:43 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 70 ms on 10.244.0.37 (executor 1) (94/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (10.244.0.37, executor 1, partition 95, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 70 ms on 10.244.0.37 (executor 1) (95/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (10.244.0.37, executor 1, partition 96, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 70 ms on 10.244.0.37 (executor 1) (96/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (10.244.0.37, executor 1, partition 97, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 71 ms on 10.244.0.37 (executor 1) (97/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (10.244.0.37, executor 1, partition 98, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 72 ms on 10.244.0.37 (executor 1) (98/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (10.244.0.37, executor 1, partition 99, PROCESS_LOCAL, 8998 bytes) \n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 71 ms on 10.244.0.37 (executor 1) (99/100)\n",
      "26/01/12 20:51:44 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 71 ms on 10.244.0.37 (executor 1) (100/100)\n",
      "26/01/12 20:51:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "26/01/12 20:51:44 INFO DAGScheduler: ResultStage 0 (reduce at /opt/spark/examples/src/main/python/pi.py:42) finished in 7.562 s\n",
      "26/01/12 20:51:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "26/01/12 20:51:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "26/01/12 20:51:44 INFO DAGScheduler: Job 0 finished: reduce at /opt/spark/examples/src/main/python/pi.py:42, took 7.579730 s\n",
      "Pi is roughly 3.136320\n",
      "26/01/12 20:51:44 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "26/01/12 20:51:44 INFO SparkUI: Stopped Spark web UI at http://spark-test-20260112-125105-0282979bb3fa6d79-driver-svc.dfp.svc:4040\n",
      "26/01/12 20:51:44 INFO KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "26/01/12 20:51:44 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "26/01/12 20:51:44 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n",
      "26/01/12 20:51:44 ERROR Utils: Uncaught exception in thread Thread-4\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: DELETE at: https://kubernetes.default.svc/api/v1/namespaces/dfp/persistentvolumeclaims?labelSelector=spark-app-selector%3Dspark-bce4b18ed65e44b4b5ad88621ee7b92d. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. persistentvolumeclaims is forbidden: User \"system:serviceaccount:dfp:spark-operator\" cannot deletecollection resource \"persistentvolumeclaims\" in API group \"\" in the namespace \"dfp\".\n",
      "\tat io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:518)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:535)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleDelete(OperationSupport.java:320)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.BaseOperation.deleteAll(BaseOperation.java:504)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.BaseOperation.delete(BaseOperation.java:469)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.$anonfun$stop$6(KubernetesClusterSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.stop(KubernetesClusterSchedulerBackend.scala:153)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: DELETE at: https://kubernetes.default.svc/api/v1/namespaces/dfp/persistentvolumeclaims?labelSelector=spark-app-selector%3Dspark-bce4b18ed65e44b4b5ad88621ee7b92d. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. persistentvolumeclaims is forbidden: User \"system:serviceaccount:dfp:spark-operator\" cannot deletecollection resource \"persistentvolumeclaims\" in API group \"\" in the namespace \"dfp\".\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:671)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:651)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.assertResponseCode(OperationSupport.java:597)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$handleResponse$0(OperationSupport.java:560)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)\n",
      "\tat io.fabric8.kubernetes.client.http.StandardHttpClient.lambda$completeOrCancel$10(StandardHttpClient.java:140)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)\n",
      "\tat io.fabric8.kubernetes.client.http.ByteArrayBodyHandler.onBodyDone(ByteArrayBodyHandler.java:52)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)\n",
      "\tat io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$OkHttpAsyncBody.doConsume(OkHttpClientImpl.java:137)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "26/01/12 20:51:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "26/01/12 20:51:44 INFO MemoryStore: MemoryStore cleared\n",
      "26/01/12 20:51:44 INFO BlockManager: BlockManager stopped\n",
      "26/01/12 20:51:44 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "26/01/12 20:51:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "26/01/12 20:51:44 INFO SparkContext: Successfully stopped SparkContext\n",
      "26/01/12 20:51:45 INFO ShutdownHookManager: Shutdown hook called\n",
      "26/01/12 20:51:45 INFO ShutdownHookManager: Deleting directory /var/data/spark-bf033fac-1ee8-436a-b7c5-76960f159d61/spark-72e30241-6832-4692-8222-3de59c13f0ae\n",
      "26/01/12 20:51:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-96fa6685-db42-41ec-9048-96de9e8ecd0c\n",
      "26/01/12 20:51:45 INFO ShutdownHookManager: Deleting directory /var/data/spark-bf033fac-1ee8-436a-b7c5-76960f159d61/spark-72e30241-6832-4692-8222-3de59c13f0ae/pyspark-385f1f6b-7bbd-4c30-b046-da02bbd33c64\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def get_driver_logs(app_name: str, tail: int = 100) -> str:\n",
    "    \"\"\"Get logs from the driver pod.\"\"\"\n",
    "    print(f\"Driver logs for {app_name} (last {tail} lines):\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    driver_pod = f\"{app_name}-driver\"\n",
    "    \n",
    "    try:\n",
    "        result = run_kubectl(['-n', NAMESPACE, 'logs', driver_pod, f'--tail={tail}'], check=False)\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "            return result.stdout\n",
    "        else:\n",
    "            # Try with --previous flag for crashed containers\n",
    "            result = run_kubectl(['-n', NAMESPACE, 'logs', driver_pod, '--previous', f'--tail={tail}'], check=False)\n",
    "            if result.returncode == 0:\n",
    "                print(\"(Previous container logs)\")\n",
    "                print(result.stdout)\n",
    "                return result.stdout\n",
    "            else:\n",
    "                print(f\"No logs available: {result.stderr}\")\n",
    "                return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Get driver logs\n",
    "driver_logs = get_driver_logs(TEST_APP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_executor_pods(app_name: str) -> List[str]:\n",
    "    \"\"\"Get list of executor pod names.\"\"\"\n",
    "    try:\n",
    "        result = run_kubectl([\n",
    "            '-n', NAMESPACE, 'get', 'pods',\n",
    "            '-l', f'sparkoperator.k8s.io/app-name={app_name},spark-role=executor',\n",
    "            '-o', 'jsonpath={.items[*].metadata.name}'\n",
    "        ])\n",
    "        pods = result.stdout.strip().split()\n",
    "        return [p for p in pods if p]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_executor_logs(app_name: str, tail: int = 50) -> Dict[str, str]:\n",
    "    \"\"\"Get logs from all executor pods.\"\"\"\n",
    "    executor_pods = get_executor_pods(app_name)\n",
    "    \n",
    "    if not executor_pods:\n",
    "        print(\"No executor pods found\")\n",
    "        return {}\n",
    "    \n",
    "    logs = {}\n",
    "    for pod in executor_pods:\n",
    "        print(f\"\\nExecutor logs for {pod} (last {tail} lines):\")\n",
    "        print(\"-\" * 60)\n",
    "        try:\n",
    "            result = run_kubectl(['-n', NAMESPACE, 'logs', pod, f'--tail={tail}'], check=False)\n",
    "            if result.returncode == 0:\n",
    "                print(result.stdout)\n",
    "                logs[pod] = result.stdout\n",
    "            else:\n",
    "                print(f\"No logs available: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "# Uncomment to get executor logs\n",
    "# executor_logs = get_executor_logs(TEST_APP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_spark_application(app_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run comprehensive diagnostics on a SparkApplication.\"\"\"\n",
    "    print(f\"Diagnosing SparkApplication: {app_name}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    diagnosis = {\n",
    "        'app_status': None,\n",
    "        'pod_issues': [],\n",
    "        'events': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # 1. Get SparkApplication status\n",
    "    print(\"\\n[1] SparkApplication Status\")\n",
    "    print(\"-\" * 40)\n",
    "    status = get_spark_application_status(app_name)\n",
    "    diagnosis['app_status'] = status\n",
    "    \n",
    "    if not status['found']:\n",
    "        print_status('error', \"SparkApplication not found\")\n",
    "        diagnosis['recommendations'].append(\"Verify the application name and namespace\")\n",
    "        return diagnosis\n",
    "    \n",
    "    print(f\"State: {status['state']}\")\n",
    "    if status['error_message']:\n",
    "        print(f\"Error: {status['error_message']}\")\n",
    "    \n",
    "    # 2. Check driver pod\n",
    "    print(\"\\n[2] Driver Pod Status\")\n",
    "    print(\"-\" * 40)\n",
    "    driver_pod = f\"{app_name}-driver\"\n",
    "    \n",
    "    try:\n",
    "        result = run_kubectl(['-n', NAMESPACE, 'get', 'pod', driver_pod, '-o', 'json'], check=False)\n",
    "        if result.returncode == 0:\n",
    "            pod_data = json.loads(result.stdout)\n",
    "            pod_status = pod_data.get('status', {})\n",
    "            phase = pod_status.get('phase', 'Unknown')\n",
    "            print(f\"Pod: {driver_pod}\")\n",
    "            print(f\"Phase: {phase}\")\n",
    "            \n",
    "            # Check container statuses\n",
    "            for cs in pod_status.get('containerStatuses', []):\n",
    "                state = cs.get('state', {})\n",
    "                if 'waiting' in state:\n",
    "                    reason = state['waiting'].get('reason', 'Unknown')\n",
    "                    message = state['waiting'].get('message', '')\n",
    "                    print(f\"Container waiting: {reason}\")\n",
    "                    if message:\n",
    "                        print(f\"  Message: {message[:200]}\")\n",
    "                    \n",
    "                    if 'ImagePull' in reason:\n",
    "                        diagnosis['pod_issues'].append(f\"Image pull issue: {reason}\")\n",
    "                        diagnosis['recommendations'].append(\n",
    "                            f\"Pre-load image into kind: docker pull {SPARK_IMAGE} && \"\n",
    "                            f\"kind load docker-image {SPARK_IMAGE} --name {CLUSTER_NAME}\"\n",
    "                        )\n",
    "                elif 'terminated' in state:\n",
    "                    exit_code = state['terminated'].get('exitCode', -1)\n",
    "                    reason = state['terminated'].get('reason', 'Unknown')\n",
    "                    print(f\"Container terminated: {reason} (exit code: {exit_code})\")\n",
    "                    diagnosis['pod_issues'].append(f\"Container terminated: {reason}\")\n",
    "        else:\n",
    "            print(f\"Driver pod not found: {driver_pod}\")\n",
    "            diagnosis['recommendations'].append(\"Check if Spark Operator is running and has permissions\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking driver pod: {e}\")\n",
    "    \n",
    "    # 3. Get events\n",
    "    print(\"\\n[3] Recent Events\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        result = run_kubectl([\n",
    "            '-n', NAMESPACE, 'get', 'events',\n",
    "            '--field-selector', f'involvedObject.name={app_name}',\n",
    "            '--sort-by=.lastTimestamp'\n",
    "        ], check=False)\n",
    "        if result.stdout.strip():\n",
    "            print(result.stdout)\n",
    "            diagnosis['events'] = result.stdout.split('\\n')\n",
    "        else:\n",
    "            print(\"No events found for this application\")\n",
    "            \n",
    "        # Also check driver pod events\n",
    "        result = run_kubectl([\n",
    "            '-n', NAMESPACE, 'get', 'events',\n",
    "            '--field-selector', f'involvedObject.name={driver_pod}',\n",
    "            '--sort-by=.lastTimestamp'\n",
    "        ], check=False)\n",
    "        if result.stdout.strip():\n",
    "            print(f\"\\nDriver pod events:\")\n",
    "            print(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting events: {e}\")\n",
    "    \n",
    "    # 4. Recommendations\n",
    "    if diagnosis['recommendations']:\n",
    "        print(\"\\n[4] Recommendations\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(diagnosis['recommendations'], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    return diagnosis\n",
    "\n",
    "\n",
    "# Run diagnostics on the test application\n",
    "# diagnosis = diagnose_spark_application(TEST_APP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkApplications in namespace 'dfp':\n",
      "\n",
      "  [?]     spark-test-20260112-125105                         2026-01-12T20:51:31Z\n"
     ]
    }
   ],
   "source": [
    "def list_all_spark_applications() -> List[Dict]:\n",
    "    \"\"\"List all SparkApplications in the namespace.\"\"\"\n",
    "    print(f\"SparkApplications in namespace '{NAMESPACE}':\\n\")\n",
    "    \n",
    "    apps = kubectl_get_json('sparkapplication', '')\n",
    "    \n",
    "    if not apps or not apps.get('items'):\n",
    "        print(\"No SparkApplications found\")\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    for app in apps.get('items', []):\n",
    "        name = app.get('metadata', {}).get('name', 'unknown')\n",
    "        state = app.get('status', {}).get('applicationState', {}).get('state', 'UNKNOWN')\n",
    "        created = app.get('metadata', {}).get('creationTimestamp', 'N/A')\n",
    "        \n",
    "        result.append({'name': name, 'state': state, 'created': created})\n",
    "        \n",
    "        state_icon = {\n",
    "            'COMPLETED': '[OK]',\n",
    "            'RUNNING': '[RUN]',\n",
    "            'SUBMITTED': '[SUB]',\n",
    "            'FAILED': '[FAIL]',\n",
    "            'SUBMISSION_FAILED': '[FAIL]'\n",
    "        }.get(state, '[?]')\n",
    "        \n",
    "        print(f\"  {state_icon:7s} {name:50s} {created}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "all_apps = list_all_spark_applications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Resource Summary\n",
      "\n",
      "============================================================\n",
      "\n",
      "Nodes:\n",
      "  dfp-kind-control-plane: Ready=True, CPU=10, Memory=8025424Ki\n",
      "\n",
      "Pods in 'dfp': 8\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def check_cluster_resources() -> Dict[str, Any]:\n",
    "    \"\"\"Check available cluster resources.\"\"\"\n",
    "    print(\"Cluster Resource Summary\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = {'nodes': [], 'pods_in_namespace': 0}\n",
    "    \n",
    "    # Get node resources\n",
    "    try:\n",
    "        nodes = kubectl_get_json('nodes', '')\n",
    "        if nodes:\n",
    "            print(\"\\nNodes:\")\n",
    "            for node in nodes.get('items', []):\n",
    "                name = node.get('metadata', {}).get('name', 'unknown')\n",
    "                status = node.get('status', {})\n",
    "                allocatable = status.get('allocatable', {})\n",
    "                \n",
    "                cpu = allocatable.get('cpu', 'N/A')\n",
    "                memory = allocatable.get('memory', 'N/A')\n",
    "                \n",
    "                # Check conditions\n",
    "                ready = 'Unknown'\n",
    "                for cond in status.get('conditions', []):\n",
    "                    if cond.get('type') == 'Ready':\n",
    "                        ready = cond.get('status', 'Unknown')\n",
    "                \n",
    "                print(f\"  {name}: Ready={ready}, CPU={cpu}, Memory={memory}\")\n",
    "                result['nodes'].append({'name': name, 'ready': ready, 'cpu': cpu, 'memory': memory})\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting nodes: {e}\")\n",
    "    \n",
    "    # Get pod count in namespace\n",
    "    try:\n",
    "        pods = kubectl_get_json('pods', '')\n",
    "        if pods:\n",
    "            pod_count = len(pods.get('items', []))\n",
    "            result['pods_in_namespace'] = pod_count\n",
    "            print(f\"\\nPods in '{NAMESPACE}': {pod_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting pods: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Check cluster resources\n",
    "resources = check_cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spark_application(app_name: str, wait: bool = True) -> bool:\n",
    "    \"\"\"Delete a SparkApplication.\"\"\"\n",
    "    print(f\"Deleting SparkApplication: {app_name}\")\n",
    "    \n",
    "    try:\n",
    "        result = run_kubectl(['-n', NAMESPACE, 'delete', 'sparkapplication', app_name], check=False)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print_status('success', f\"SparkApplication '{app_name}' deleted\")\n",
    "            \n",
    "            if wait:\n",
    "                print(\"Waiting for resources to be cleaned up...\")\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # Verify pods are gone\n",
    "                pods = get_executor_pods(app_name)\n",
    "                driver_exists = False\n",
    "                try:\n",
    "                    run_kubectl(['-n', NAMESPACE, 'get', 'pod', f'{app_name}-driver'])\n",
    "                    driver_exists = True\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if not pods and not driver_exists:\n",
    "                    print_status('success', \"All pods cleaned up\")\n",
    "                else:\n",
    "                    print_status('warning', \"Some pods may still be terminating\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            if 'NotFound' in result.stderr:\n",
    "                print_status('warning', f\"SparkApplication '{app_name}' not found (already deleted?)\")\n",
    "            else:\n",
    "                print_status('error', f\"Failed to delete: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print_status('error', f\"Error deleting SparkApplication: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Delete the test application\n",
    "# Uncomment the line below to delete\n",
    "# delete_spark_application(TEST_APP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up completed/failed SparkApplications (dry_run=True)\n",
      "\n",
      "No completed/failed applications to clean up\n",
      "Cleaning up completed/failed SparkApplications (dry_run=False)\n",
      "\n",
      "No completed/failed applications to clean up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanup_completed_applications(dry_run: bool = True) -> int:\n",
    "    \"\"\"Delete all completed or failed SparkApplications.\"\"\"\n",
    "    print(f\"Cleaning up completed/failed SparkApplications (dry_run={dry_run})\\n\")\n",
    "    \n",
    "    apps = kubectl_get_json('sparkapplication', '')\n",
    "    \n",
    "    if not apps or not apps.get('items'):\n",
    "        print(\"No SparkApplications found\")\n",
    "        return 0\n",
    "    \n",
    "    to_delete = []\n",
    "    terminal_states = {'COMPLETED', 'FAILED', 'SUBMISSION_FAILED'}\n",
    "    \n",
    "    for app in apps.get('items', []):\n",
    "        name = app.get('metadata', {}).get('name', '')\n",
    "        state = app.get('status', {}).get('applicationState', {}).get('state', '')\n",
    "        \n",
    "        if state in terminal_states:\n",
    "            to_delete.append({'name': name, 'state': state})\n",
    "    \n",
    "    if not to_delete:\n",
    "        print(\"No completed/failed applications to clean up\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Found {len(to_delete)} application(s) to clean up:\")\n",
    "    for app in to_delete:\n",
    "        print(f\"  - {app['name']} ({app['state']})\")\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"\\nDRY RUN: Would delete {len(to_delete)} application(s)\")\n",
    "        print(\"Set dry_run=False to actually delete\")\n",
    "    else:\n",
    "        print(f\"\\nDeleting {len(to_delete)} application(s)...\")\n",
    "        for app in to_delete:\n",
    "            delete_spark_application(app['name'], wait=False)\n",
    "        print(f\"\\nDeleted {len(to_delete)} application(s)\")\n",
    "    \n",
    "    return len(to_delete)\n",
    "\n",
    "\n",
    "# Preview cleanup (dry run)\n",
    "cleanup_completed_applications(dry_run=True)\n",
    "\n",
    "# Uncomment to actually delete\n",
    "# cleanup_completed_applications(dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 10. Quick Reference Commands\n",
    "\n",
    "Useful kubectl commands for Spark Operator debugging:\n",
    "\n",
    "```bash\n",
    "# List all SparkApplications\n",
    "kubectl -n dfp get sparkapplication\n",
    "\n",
    "# Describe a SparkApplication\n",
    "kubectl -n dfp describe sparkapplication <app-name>\n",
    "\n",
    "# Get SparkApplication YAML\n",
    "kubectl -n dfp get sparkapplication <app-name> -o yaml\n",
    "\n",
    "# View driver logs\n",
    "kubectl -n dfp logs <app-name>-driver\n",
    "\n",
    "# Follow driver logs\n",
    "kubectl -n dfp logs -f <app-name>-driver\n",
    "\n",
    "# View Spark Operator logs\n",
    "kubectl -n dfp logs deployment/spark-operator\n",
    "\n",
    "# List all Spark-related pods\n",
    "kubectl -n dfp get pods -l sparkoperator.k8s.io/app-name\n",
    "\n",
    "# Delete a SparkApplication\n",
    "kubectl -n dfp delete sparkapplication <app-name>\n",
    "\n",
    "# Check CRDs\n",
    "kubectl get crd | grep spark\n",
    "\n",
    "# Pre-load Spark image into kind\n",
    "docker pull apache/spark:3.5.7-python3\n",
    "kind load docker-image apache/spark:3.5.7-python3 --name dfp-kind\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "### What We Tested\n",
    "\n",
    "1. Cluster connectivity and namespace existence\n",
    "2. Spark Operator deployment and readiness\n",
    "3. CRD installation (SparkApplication, ScheduledSparkApplication)\n",
    "4. RBAC configuration (ServiceAccount, ClusterRole, ClusterRoleBinding)\n",
    "5. SparkApplication submission and lifecycle monitoring\n",
    "6. Log retrieval from driver and executor pods\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| ImagePullBackOff | Pre-load image: `kind load docker-image <image> --name dfp-kind` |\n",
    "| SparkApplication stuck in SUBMITTED | Check operator logs: `kubectl -n dfp logs deployment/spark-operator` |\n",
    "| Permission denied | Verify ServiceAccount and RBAC permissions |\n",
    "| Driver OOMKilled | Increase driver memory in SparkApplication spec |\n",
    "| Executor not starting | Check resource availability with `kubectl describe node` |\n",
    "\n",
    "### For Production Workloads\n",
    "\n",
    "See `orchestration/kubeflow/dfp_kfp/components/kronodroid_spark_operator_transform_component.py` for a complete example with:\n",
    "- Iceberg integration\n",
    "- S3/MinIO connectivity\n",
    "- LakeFS catalog configuration\n",
    "- Proper dependency management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
