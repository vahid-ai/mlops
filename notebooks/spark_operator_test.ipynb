{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Operator Testing & Debugging\n",
    "\n",
    "This notebook helps test and debug the Spark Operator integration with Kubeflow in the Kind cluster.\n",
    "\n",
    "## What This Tests\n",
    "\n",
    "1. ✅ Spark Operator deployment status\n",
    "2. ✅ SparkApplication CRD availability\n",
    "3. ✅ Submit test SparkApplication\n",
    "4. ✅ Monitor SparkApplication lifecycle\n",
    "5. ✅ Retrieve driver and executor logs\n",
    "6. ✅ Verify MinIO/LakeFS connectivity from Spark\n",
    "7. ✅ Troubleshoot common issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configuration\n",
    "NAMESPACE = \"dfp\"\n",
    "CLUSTER_NAME = \"dfp-kind\"\n",
    "\n",
    "print(f\"Testing Spark Operator in namespace: {NAMESPACE}\")\n",
    "print(f\"Kind cluster: {CLUSTER_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kubectl(args: List[str], check: bool = True) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run kubectl command and return result.\"\"\"\n",
    "    cmd = [\"kubectl\"] + args\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    )\n",
    "    if check and result.returncode != 0:\n",
    "        print(f\"❌ Command failed: {' '.join(cmd)}\")\n",
    "        print(f\"   stderr: {result.stderr}\")\n",
    "        raise subprocess.CalledProcessError(result.returncode, cmd, result.stdout, result.stderr)\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "def print_json(data: dict):\n",
    "    \"\"\"Pretty print JSON data.\"\"\"\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"1. Checking Prerequisites\")\n",
    "\n",
    "# Check kubectl\n",
    "try:\n",
    "    result = run_kubectl([\"version\", \"--client\", \"-o\", \"json\"])\n",
    "    version_info = json.loads(result.stdout)\n",
    "    print(f\"✓ kubectl version: {version_info['clientVersion']['gitVersion']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ kubectl not found or not configured: {e}\")\n",
    "\n",
    "# Check cluster connection\n",
    "try:\n",
    "    result = run_kubectl([\"cluster-info\"])\n",
    "    print(f\"\\n✓ Cluster connection established\")\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Cannot connect to cluster: {e}\")\n",
    "\n",
    "# Check namespace\n",
    "try:\n",
    "    result = run_kubectl([\"get\", \"namespace\", NAMESPACE])\n",
    "    print(f\"✓ Namespace '{NAMESPACE}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Namespace '{NAMESPACE}' not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Spark Operator Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"2. Spark Operator Deployment Status\")\n",
    "\n",
    "try:\n",
    "    # Get deployment\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"deployment\", \"spark-operator\", \"-o\", \"json\"])\n",
    "    deployment = json.loads(result.stdout)\n",
    "    \n",
    "    # Extract key info\n",
    "    image = deployment[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"image\"]\n",
    "    replicas = deployment[\"spec\"][\"replicas\"]\n",
    "    available_replicas = deployment[\"status\"].get(\"availableReplicas\", 0)\n",
    "    ready_replicas = deployment[\"status\"].get(\"readyReplicas\", 0)\n",
    "    \n",
    "    print(f\"✓ Spark Operator Deployment Found\")\n",
    "    print(f\"  Image: {image}\")\n",
    "    print(f\"  Desired Replicas: {replicas}\")\n",
    "    print(f\"  Ready Replicas: {ready_replicas}\")\n",
    "    print(f\"  Available Replicas: {available_replicas}\")\n",
    "    \n",
    "    if ready_replicas == replicas:\n",
    "        print(f\"\\n✅ Spark Operator is READY\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Spark Operator is NOT READY\")\n",
    "        \n",
    "    # Get pod status\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"pods\", \"-l\", \"app=spark-operator\", \"-o\", \"json\"])\n",
    "    pods = json.loads(result.stdout)\n",
    "    \n",
    "    print(f\"\\nPods:\")\n",
    "    for pod in pods[\"items\"]:\n",
    "        name = pod[\"metadata\"][\"name\"]\n",
    "        phase = pod[\"status\"][\"phase\"]\n",
    "        print(f\"  • {name}: {phase}\")\n",
    "        \n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ Spark Operator deployment not found!\")\n",
    "    print(\"\\nTo install:\")\n",
    "    print(\"  kubectl apply -k infra/k8s/kind/addons/spark-operator/\")\n",
    "    print(\"  # or\")\n",
    "    print(\"  task spark-operator:up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check SparkApplication CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"3. SparkApplication Custom Resource Definition\")\n",
    "\n",
    "try:\n",
    "    # Check CRD exists\n",
    "    result = run_kubectl([\"get\", \"crd\", \"sparkapplications.sparkoperator.k8s.io\", \"-o\", \"json\"])\n",
    "    crd = json.loads(result.stdout)\n",
    "    \n",
    "    print(f\"✓ SparkApplication CRD Found\")\n",
    "    print(f\"  Name: {crd['metadata']['name']}\")\n",
    "    print(f\"  Group: {crd['spec']['group']}\")\n",
    "    print(f\"  Scope: {crd['spec']['scope']}\")\n",
    "    \n",
    "    print(f\"\\n  Versions:\")\n",
    "    for version in crd[\"spec\"][\"versions\"]:\n",
    "        served = \"✓\" if version[\"served\"] else \"✗\"\n",
    "        storage = \"(storage)\" if version[\"storage\"] else \"\"\n",
    "        print(f\"    {served} {version['name']} {storage}\")\n",
    "        \n",
    "    print(f\"\\n✅ SparkApplication CRD is available\")\n",
    "    \n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ SparkApplication CRD not found!\")\n",
    "    print(\"\\nThe CRD should be installed with the Spark Operator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List Existing SparkApplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"4. Existing SparkApplications\")\n",
    "\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"sparkapplications\", \"-o\", \"json\"])\n",
    "    apps = json.loads(result.stdout)\n",
    "    \n",
    "    if not apps[\"items\"]:\n",
    "        print(\"No SparkApplications found in namespace.\")\n",
    "    else:\n",
    "        print(f\"Found {len(apps['items'])} SparkApplication(s):\\n\")\n",
    "        for app in apps[\"items\"]:\n",
    "            name = app[\"metadata\"][\"name\"]\n",
    "            state = app.get(\"status\", {}).get(\"applicationState\", {}).get(\"state\", \"UNKNOWN\")\n",
    "            creation = app[\"metadata\"][\"creationTimestamp\"]\n",
    "            print(f\"  • {name}\")\n",
    "            print(f\"    State: {state}\")\n",
    "            print(f\"    Created: {creation}\")\n",
    "            print()\n",
    "            \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to list SparkApplications: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Test SparkApplication\n",
    "\n",
    "This creates a simple test SparkApplication that runs a PySpark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"5. Creating Test SparkApplication\")\n",
    "\n",
    "# Generate unique name\n",
    "test_app_name = f\"spark-test-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "print(f\"Creating SparkApplication: {test_app_name}\\n\")\n",
    "\n",
    "# Simple PySpark test job\n",
    "test_job_code = '''\nimport sys\nfrom pyspark.sql import SparkSession\n\nprint(\"=\"*70)\nprint(\"Starting Spark Test Job\")\nprint(\"=\"*70)\n\nspark = SparkSession.builder.appName(\"spark-test\").getOrCreate()\n\nprint(f\"Spark version: {spark.version}\")\nprint(f\"Python version: {sys.version}\")\n\n# Create test DataFrame\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\nprint(\"\\\\nTest DataFrame:\")\ndf.show()\n\nprint(\"\\\\nDataFrame count:\", df.count())\nprint(\"\\\\nDataFrame schema:\")\ndf.printSchema()\n\nprint(\"\\\\n\" + \"=\"*70)\nprint(\"✅ Spark Test Job Completed Successfully\")\nprint(\"=\"*70)\n\nspark.stop()\n'''\n\n# Create ConfigMap with test job\ntest_configmap = f\"\"\"\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {test_app_name}-job\n  namespace: {NAMESPACE}\ndata:\n  test_job.py: |\n{test_job_code}\n\"\"\"\n\nprint(\"Creating ConfigMap with test job...\")\ntry:\n    result = run_kubectl([\"apply\", \"-f\", \"-\"], check=True)\n    subprocess.run([\"kubectl\", \"apply\", \"-f\", \"-\"], input=test_configmap, text=True, check=True, capture_output=True)\n    print(\"✓ ConfigMap created\\n\")\nexcept Exception as e:\n    print(f\"❌ Failed to create ConfigMap: {e}\")\n\n# Create SparkApplication\ntest_spark_app = f\"\"\"\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: {test_app_name}\n  namespace: {NAMESPACE}\nspec:\n  type: Python\n  mode: cluster\n  pythonVersion: \"3\"\n  sparkVersion: \"3.5.3\"\n  image: \"bitnami/spark:3.5.3\"\n  imagePullPolicy: IfNotPresent\n  mainApplicationFile: \"local:///opt/spark/work-dir/test_job.py\"\n  \n  restartPolicy:\n    type: Never\n  \n  driver:\n    cores: 1\n    memory: \"512m\"\n    serviceAccount: spark-operator\n    labels:\n      version: \"3.5.3\"\n    volumeMounts:\n      - name: test-job\n        mountPath: /opt/spark/work-dir\n  \n  executor:\n    instances: 1\n    cores: 1\n    memory: \"512m\"\n    labels:\n      version: \"3.5.3\"\n    volumeMounts:\n      - name: test-job\n        mountPath: /opt/spark/work-dir\n  \n  volumes:\n    - name: test-job\n      configMap:\n        name: {test_app_name}-job\n\"\"\"\n\nprint(\"Creating SparkApplication...\")\ntry:\n    subprocess.run([\"kubectl\", \"apply\", \"-f\", \"-\"], input=test_spark_app, text=True, check=True, capture_output=True)\n    print(f\"✓ SparkApplication '{test_app_name}' created\\n\")\n    print(f\"Monitor with:\")\n    print(f\"  kubectl -n {NAMESPACE} get sparkapplication {test_app_name}\")\n    print(f\"  kubectl -n {NAMESPACE} logs -f {test_app_name}-driver\")\nexcept Exception as e:\n    print(f\"❌ Failed to create SparkApplication: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitor SparkApplication Status\n",
    "\n",
    "Run this cell multiple times to watch the status change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(f\"6. Monitoring SparkApplication: {test_app_name}\")\n",
    "\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"sparkapplication\", test_app_name, \"-o\", \"json\"])\n",
    "    app = json.loads(result.stdout)\n",
    "    \n",
    "    # Extract status info\n",
    "    state = app.get(\"status\", {}).get(\"applicationState\", {}).get(\"state\", \"UNKNOWN\")\n",
    "    submission_time = app.get(\"status\", {}).get(\"submissionTime\", \"N/A\")\n",
    "    termination_time = app.get(\"status\", {}).get(\"terminationTime\", \"N/A\")\n",
    "    driver_info = app.get(\"status\", {}).get(\"driverInfo\", {})\n",
    "    \n",
    "    print(f\"Application: {test_app_name}\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Submission Time: {submission_time}\")\n",
    "    print(f\"Termination Time: {termination_time}\")\n",
    "    \n",
    "    if driver_info:\n",
    "        print(f\"\\nDriver Info:\")\n",
    "        print(f\"  Pod Name: {driver_info.get('podName', 'N/A')}\")\n",
    "        print(f\"  Web UI Service: {driver_info.get('webUIServiceName', 'N/A')}\")\n",
    "    \n",
    "    # State-specific messages\n",
    "    if state == \"COMPLETED\":\n",
    "        print(f\"\\n✅ SparkApplication COMPLETED successfully!\")\n",
    "    elif state == \"FAILED\":\n",
    "        print(f\"\\n❌ SparkApplication FAILED!\")\n",
    "        error_message = app.get(\"status\", {}).get(\"applicationState\", {}).get(\"errorMessage\", \"\")\n",
    "        if error_message:\n",
    "            print(f\"   Error: {error_message}\")\n",
    "    elif state in [\"SUBMITTED\", \"RUNNING\"]:\n",
    "        print(f\"\\n⏳ SparkApplication is {state}...\")\n",
    "    elif state == \"PENDING\":\n",
    "        print(f\"\\n⏳ SparkApplication is pending submission...\")\n",
    "    \n",
    "    # Check pods\n",
    "    print(f\"\\nAssociated Pods:\")\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"pods\", \"-l\", f\"sparkoperator.k8s.io/app-name={test_app_name}\", \"-o\", \"json\"])\n",
    "    pods = json.loads(result.stdout)\n",
    "    \n",
    "    if not pods[\"items\"]:\n",
    "        print(\"  No pods created yet\")\n",
    "    else:\n",
    "        for pod in pods[\"items\"]:\n",
    "            name = pod[\"metadata\"][\"name\"]\n",
    "            phase = pod[\"status\"][\"phase\"]\n",
    "            role = pod[\"metadata\"][\"labels\"].get(\"spark-role\", \"unknown\")\n",
    "            print(f\"  • {name} ({role}): {phase}\")\n",
    "            \n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"❌ SparkApplication '{test_app_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wait for Completion\n",
    "\n",
    "This cell waits for the SparkApplication to complete (or fail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(f\"7. Waiting for SparkApplication to Complete\")\n",
    "\n",
    "print(f\"Waiting for {test_app_name}...\\n\")\n",
    "\n",
    "timeout = 300  # 5 minutes\n",
    "start_time = time.time()\n",
    "last_state = None\n",
    "\n",
    "while time.time() - start_time < timeout:\n",
    "    try:\n",
    "        result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"sparkapplication\", test_app_name, \"-o\", \"json\"])\n",
    "        app = json.loads(result.stdout)\n",
    "        state = app.get(\"status\", {}).get(\"applicationState\", {}).get(\"state\", \"UNKNOWN\")\n",
    "        \n",
    "        if state != last_state:\n",
    "            elapsed = int(time.time() - start_time)\n",
    "            print(f\"[{elapsed}s] State: {state}\")\n",
    "            last_state = state\n",
    "        \n",
    "        if state in [\"COMPLETED\", \"FAILED\", \"SUBMISSION_FAILED\"]:\n",
    "            print(f\"\\nFinal state reached: {state}\")\n",
    "            if state == \"COMPLETED\":\n",
    "                print(\"✅ SparkApplication completed successfully!\")\n",
    "            else:\n",
    "                print(f\"❌ SparkApplication {state}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"❌ Failed to get SparkApplication status\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"\\n⏱️ Timeout reached ({timeout}s)\")\n",
    "    print(f\"Last state: {last_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Driver Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"8. Driver Pod Logs\")\n",
    "\n",
    "try:\n",
    "    # Get driver pod name\n",
    "    result = run_kubectl([\n",
    "        \"-n\", NAMESPACE,\n",
    "        \"get\", \"pods\",\n",
    "        \"-l\", f\"sparkoperator.k8s.io/app-name={test_app_name},spark-role=driver\",\n",
    "        \"-o\", \"jsonpath={.items[0].metadata.name}\"\n",
    "    ])\n",
    "    driver_pod = result.stdout.strip()\n",
    "    \n",
    "    if not driver_pod:\n",
    "        print(\"❌ Driver pod not found\")\n",
    "    else:\n",
    "        print(f\"Driver pod: {driver_pod}\\n\")\n",
    "        print(\"Logs:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        result = run_kubectl([\"-n\", NAMESPACE, \"logs\", driver_pod, \"--tail=100\"])\n",
    "        print(result.stdout)\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to get logs: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Executor Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"9. Executor Pod Logs\")\n",
    "\n",
    "try:\n",
    "    # Get executor pods\n",
    "    result = run_kubectl([\n",
    "        \"-n\", NAMESPACE,\n",
    "        \"get\", \"pods\",\n",
    "        \"-l\", f\"sparkoperator.k8s.io/app-name={test_app_name},spark-role=executor\",\n",
    "        \"-o\", \"json\"\n",
    "    ])\n",
    "    pods = json.loads(result.stdout)\n",
    "    \n",
    "    if not pods[\"items\"]:\n",
    "        print(\"No executor pods found (may have been cleaned up)\")\n",
    "    else:\n",
    "        for pod in pods[\"items\"]:\n",
    "            pod_name = pod[\"metadata\"][\"name\"]\n",
    "            print(f\"\\nExecutor pod: {pod_name}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            result = run_kubectl([\"-n\", NAMESPACE, \"logs\", pod_name, \"--tail=50\"])\n",
    "            print(result.stdout)\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to get executor logs: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Describe SparkApplication\n",
    "\n",
    "Get detailed information including events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"10. SparkApplication Details\")\n",
    "\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"describe\", \"sparkapplication\", test_app_name])\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to describe SparkApplication: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Check Spark Operator Logs\n",
    "\n",
    "View the Spark Operator controller logs to see how it processed the SparkApplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"11. Spark Operator Controller Logs\")\n",
    "\n",
    "try:\n",
    "    result = run_kubectl([\n",
    "        \"-n\", NAMESPACE,\n",
    "        \"logs\",\n",
    "        \"deployment/spark-operator\",\n",
    "        \"--tail=100\"\n",
    "    ])\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to get operator logs: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Troubleshooting Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"12. Troubleshooting Information\")\n",
    "\n",
    "print(\"Common Issues and Solutions:\\n\")\n",
    "\n",
    "print(\"1. ImagePullBackOff:\")\n",
    "print(\"   - Check if Spark image exists\")\n",
    "print(\"   - For kind, load image: kind load docker-image bitnami/spark:3.5.3 --name dfp-kind\\n\")\n",
    "\n",
    "print(\"2. Pods stuck in Pending:\")\n",
    "print(\"   - Check node resources: kubectl top nodes\")\n",
    "print(\"   - Check pod events: kubectl -n dfp describe pod <pod-name>\\n\")\n",
    "\n",
    "print(\"3. Application stuck in SUBMITTED:\")\n",
    "print(\"   - Check Spark Operator logs (see cell above)\")\n",
    "print(\"   - Verify RBAC permissions\")\n",
    "print(\"   - Check if service account exists\\n\")\n",
    "\n",
    "print(\"4. Application FAILED:\")\n",
    "print(\"   - Check driver logs (see cell above)\")\n",
    "print(\"   - Check application state error message\")\n",
    "print(\"   - Verify job code syntax\\n\")\n",
    "\n",
    "print(\"\\nUseful Commands:\")\n",
    "print(f\"  # Watch SparkApplication\")\n",
    "print(f\"  kubectl -n {NAMESPACE} get sparkapplication {test_app_name} -w\")\n",
    "print(f\"\\n  # Follow driver logs\")\n",
    "print(f\"  kubectl -n {NAMESPACE} logs -f {test_app_name}-driver\")\n",
    "print(f\"\\n  # Check all pods\")\n",
    "print(f\"  kubectl -n {NAMESPACE} get pods -l sparkoperator.k8s.io/app-name={test_app_name}\")\n",
    "print(f\"\\n  # Delete SparkApplication\")\n",
    "print(f\"  kubectl -n {NAMESPACE} delete sparkapplication {test_app_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Check Service Account and RBAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"13. Service Account and RBAC Check\")\n",
    "\n",
    "# Check service account\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"get\", \"serviceaccount\", \"spark-operator\"])\n",
    "    print(\"✓ ServiceAccount 'spark-operator' exists\\n\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ ServiceAccount 'spark-operator' not found\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70 + \"\\n\")\n",
    "\n",
    "# Check ClusterRole\n",
    "try:\n",
    "    result = run_kubectl([\"get\", \"clusterrole\", \"spark-operator\"])\n",
    "    print(\"✓ ClusterRole 'spark-operator' exists\\n\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ ClusterRole 'spark-operator' not found\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70 + \"\\n\")\n",
    "\n",
    "# Check ClusterRoleBinding\n",
    "try:\n",
    "    result = run_kubectl([\"get\", \"clusterrolebinding\", \"spark-operator\"])\n",
    "    print(\"✓ ClusterRoleBinding 'spark-operator' exists\\n\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ ClusterRoleBinding 'spark-operator' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Clean Up Test Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"14. Clean Up Test Resources\")\n",
    "\n",
    "print(f\"Deleting test resources for: {test_app_name}\\n\")\n",
    "\n",
    "# Delete SparkApplication\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"delete\", \"sparkapplication\", test_app_name])\n",
    "    print(f\"✓ Deleted SparkApplication: {test_app_name}\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"⚠️  SparkApplication '{test_app_name}' not found (may already be deleted)\")\n",
    "\n",
    "# Delete ConfigMap\n",
    "try:\n",
    "    result = run_kubectl([\"-n\", NAMESPACE, \"delete\", \"configmap\", f\"{test_app_name}-job\"])\n",
    "    print(f\"✓ Deleted ConfigMap: {test_app_name}-job\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"⚠️  ConfigMap '{test_app_name}-job' not found (may already be deleted)\")\n",
    "\n",
    "print(f\"\\n✅ Clean up complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "- ✅ Spark Operator deployment and health\n",
    "- ✅ SparkApplication CRD availability\n",
    "- ✅ Creating and submitting SparkApplications\n",
    "- ✅ Monitoring application lifecycle\n",
    "- ✅ Retrieving logs from driver and executors\n",
    "- ✅ RBAC configuration\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "If the test passed, you can now:\n",
    "1. Run the Kronodroid pipeline with `--transform-runner spark-operator`\n",
    "2. Create custom SparkApplications for your data processing needs\n",
    "3. Integrate with MinIO and LakeFS for data versioning\n",
    "\n",
    "### Useful Documentation\n",
    "\n",
    "- Spark Operator docs: https://github.com/kubeflow/spark-operator\n",
    "- SparkApplication examples: https://github.com/kubeflow/spark-operator/tree/master/examples\n",
    "- Local setup: `infra/k8s/kind/addons/spark-operator/README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
