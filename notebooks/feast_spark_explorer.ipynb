{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feast Feature Store Explorer with Spark Backend\n",
    "\n",
    "This notebook demonstrates how to query and explore the Feast feature store configured with a Spark offline backend and Iceberg tables on LakeFS.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "dlt (Kaggle) ‚Üí Avro ‚Üí MinIO ‚Üí Spark ‚Üí Iceberg (LakeFS) ‚Üí Feast\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/benjaminbrown/Documents/GitHub/mlops\n",
      "LakeFS endpoint: http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set environment variables for local development (adjust as needed)\n",
    "os.environ.setdefault(\"LAKEFS_ENDPOINT_URL\", \"http://localhost:8000\")\n",
    "os.environ.setdefault(\"LAKEFS_ACCESS_KEY_ID\", \"AKIAIOSFOLQUICKSTART\")\n",
    "os.environ.setdefault(\"LAKEFS_SECRET_ACCESS_KEY\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\n",
    "os.environ.setdefault(\"LAKEFS_REPOSITORY\", \"kronodroid\")\n",
    "os.environ.setdefault(\"LAKEFS_BRANCH\", \"main\")\n",
    "os.environ.setdefault(\"REDIS_CONNECTION_STRING\", \"redis://localhost:16379\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"LakeFS endpoint: {os.environ['LAKEFS_ENDPOINT_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session\n",
    "\n",
    "Create a Spark session configured for Iceberg + LakeFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/14 15:41:20 WARN Utils: Your hostname, Benjamins-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.197 instead (on interface en0)\n",
      "25/12/14 15:41:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/benjaminbrown/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/benjaminbrown/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6dbedf92-0178-493d-8405-eea7bacdb1bd;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/benjaminbrown/Documents/GitHub/mlops/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.5.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.0/iceberg-spark-runtime-3.5_2.12-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0!iceberg-spark-runtime-3.5_2.12.jar (1224ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.5.0/iceberg-aws-bundle-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-aws-bundle;1.5.0!iceberg-aws-bundle.jar (873ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.5.0/spark-avro_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.5.0!spark-avro_2.12.jar (97ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (82ms)\n",
      ":: resolution report :: resolve 1458ms :: artifacts dl 2277ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.5.0 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6dbedf92-0178-493d-8405-eea7bacdb1bd\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (70738kB/49ms)\n",
      "25/12/14 15:41:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.7\n",
      "Spark app name: feast_explorer\n"
     ]
    }
   ],
   "source": [
    "from engines.spark_engine.dfp_spark.session import get_spark_session, SparkConfig\n",
    "\n",
    "# Create Spark session with Iceberg + LakeFS configuration\n",
    "spark_config = SparkConfig(\n",
    "    app_name=\"feast_explorer\",\n",
    "    driver_memory=\"2g\",\n",
    "    executor_memory=\"2g\",\n",
    ")\n",
    "\n",
    "spark = get_spark_session(config=spark_config)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark app name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Feast Feature Store\n",
    "\n",
    "Connect to the Feast feature store with Spark offline store configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flags are no longer necessary in Feast. Experimental features will log warnings instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast project: dfp\n",
      "Registry path: registry_type='file' registry_store_type=None path='data/registry.db' cache_ttl_seconds=60 s3_additional_kwargs=None purge_feast_metadata=False\n",
      "Offline store type: spark\n"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# Path to feast feature_store.yaml\n",
    "feast_repo_path = project_root / \"feature_stores\" / \"feast_store\"\n",
    "\n",
    "# Initialize the feature store\n",
    "store = FeatureStore(repo_path=str(feast_repo_path))\n",
    "\n",
    "print(f\"Feast project: {store.project}\")\n",
    "print(f\"Registry path: {store.config.registry}\")\n",
    "print(f\"Offline store type: {store.config.offline_store.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List All Feature Views\n",
    "\n",
    "Display all registered feature views in the Feast registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Feature Views Summary\n",
      "==================================================\n",
      "Regular Feature Views: 2\n",
      "Batch Feature Views: 2\n",
      "On-Demand Feature Views: 1\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all feature views\n",
    "feature_views = store.list_feature_views()\n",
    "batch_feature_views = store.list_batch_feature_views()\n",
    "on_demand_feature_views = store.list_on_demand_feature_views()\n",
    "\n",
    "print(f\"\\nüìä Feature Views Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Regular Feature Views: {len(feature_views)}\")\n",
    "print(f\"Batch Feature Views: {len(batch_feature_views)}\")\n",
    "print(f\"On-Demand Feature Views: {len(on_demand_feature_views)}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Views Details\n",
    "\n",
    "Display detailed information about each feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "REGULAR FEATURE VIEWS\n",
      "============================================================\n",
      "\n",
      "üîπ Feature View: malware_batch_features\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Entities: malware_sample\n",
      "   TTL: 365 days, 0:00:00\n",
      "   Online: False\n",
      "   Tags: {'usage': 'training', 'dataset': 'kronodroid', 'team': 'dfp'}\n",
      "   Source: kronodroid_training_source\n",
      "   Features (4):\n",
      "      ‚Ä¢ is_malware: Int64 - Target label\n",
      "      ‚Ä¢ dataset_split: String\n",
      "      ‚Ä¢ data_source: String\n",
      "      ‚Ä¢ sample_id: String\n",
      "\n",
      "üîπ Feature View: malware_sample_features\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Entities: malware_sample\n",
      "   TTL: 365 days, 0:00:00\n",
      "   Online: True\n",
      "   Tags: {'dataset': 'kronodroid', 'team': 'dfp'}\n",
      "   Source: kronodroid_training_source\n",
      "   Features (5):\n",
      "      ‚Ä¢ sample_id: String\n",
      "      ‚Ä¢ data_source: String - emulator or real_device\n",
      "      ‚Ä¢ dataset_split: String - train/validation/test\n",
      "      ‚Ä¢ is_malware: Int64 - 1=malware, 0=benign\n",
      "      ‚Ä¢ app_package: String - Android app package name\n"
     ]
    }
   ],
   "source": [
    "def display_feature_view_details(fv):\n",
    "    \"\"\"Display detailed information about a feature view.\"\"\"\n",
    "    print(f\"\\nüîπ Feature View: {fv.name}\")\n",
    "    print(f\"   {'‚îÄ'*45}\")\n",
    "    \n",
    "    # Entities\n",
    "    entity_names = [e.name if hasattr(e, 'name') else str(e) for e in fv.entities]\n",
    "    print(f\"   Entities: {', '.join(entity_names)}\")\n",
    "    \n",
    "    # TTL\n",
    "    print(f\"   TTL: {fv.ttl}\")\n",
    "    \n",
    "    # Online serving\n",
    "    online = getattr(fv, 'online', 'N/A')\n",
    "    print(f\"   Online: {online}\")\n",
    "    \n",
    "    # Tags\n",
    "    tags = getattr(fv, 'tags', {})\n",
    "    if tags:\n",
    "        print(f\"   Tags: {tags}\")\n",
    "    \n",
    "    # Source\n",
    "    source = getattr(fv, 'batch_source', getattr(fv, 'source', None))\n",
    "    if source:\n",
    "        source_name = getattr(source, 'name', type(source).__name__)\n",
    "        print(f\"   Source: {source_name}\")\n",
    "        if hasattr(source, 'table'):\n",
    "            print(f\"   Table: {source.table}\")\n",
    "    \n",
    "    # Schema/Features\n",
    "    schema = getattr(fv, 'schema', [])\n",
    "    if schema:\n",
    "        print(f\"   Features ({len(schema)}):\")\n",
    "        for field in schema:\n",
    "            desc = getattr(field, 'description', '')\n",
    "            desc_str = f\" - {desc}\" if desc else \"\"\n",
    "            print(f\"      ‚Ä¢ {field.name}: {field.dtype}{desc_str}\")\n",
    "\n",
    "# Display regular feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGULAR FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for fv in feature_views:\n",
    "    display_feature_view_details(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BATCH FEATURE VIEWS\n",
      "============================================================\n",
      "\n",
      "üîπ Feature View: malware_batch_features\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Entities: malware_sample\n",
      "   TTL: 365 days, 0:00:00\n",
      "   Online: False\n",
      "   Tags: {'usage': 'training', 'dataset': 'kronodroid', 'team': 'dfp'}\n",
      "   Source: kronodroid_training_source\n",
      "   Features (4):\n",
      "      ‚Ä¢ is_malware: Int64 - Target label\n",
      "      ‚Ä¢ dataset_split: String\n",
      "      ‚Ä¢ data_source: String\n",
      "      ‚Ä¢ sample_id: String\n",
      "\n",
      "üîπ Feature View: malware_sample_features\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Entities: malware_sample\n",
      "   TTL: 365 days, 0:00:00\n",
      "   Online: True\n",
      "   Tags: {'dataset': 'kronodroid', 'team': 'dfp'}\n",
      "   Source: kronodroid_training_source\n",
      "   Features (5):\n",
      "      ‚Ä¢ sample_id: String\n",
      "      ‚Ä¢ data_source: String - emulator or real_device\n",
      "      ‚Ä¢ dataset_split: String - train/validation/test\n",
      "      ‚Ä¢ is_malware: Int64 - 1=malware, 0=benign\n",
      "      ‚Ä¢ app_package: String - Android app package name\n"
     ]
    }
   ],
   "source": [
    "# Display batch feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for fv in batch_feature_views:\n",
    "    display_feature_view_details(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ON-DEMAND FEATURE VIEWS\n",
      "============================================================\n",
      "\n",
      "üî∏ On-Demand Feature View: malware_derived_features\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Source FVs: malware_sample_features\n",
      "   Computed Features (2):\n",
      "      ‚Ä¢ is_emulator_sample: Int64\n",
      "      ‚Ä¢ __dummy_id: String\n"
     ]
    }
   ],
   "source": [
    "# Display on-demand feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ON-DEMAND FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for odfv in on_demand_feature_views:\n",
    "    print(f\"\\nüî∏ On-Demand Feature View: {odfv.name}\")\n",
    "    print(f\"   {'‚îÄ'*45}\")\n",
    "    \n",
    "    # Source feature views\n",
    "    sources = list(odfv.source_feature_view_projections.keys())\n",
    "    print(f\"   Source FVs: {', '.join(sources)}\")\n",
    "    \n",
    "    # Schema\n",
    "    schema = getattr(odfv, 'schema', [])\n",
    "    if schema:\n",
    "        print(f\"   Computed Features ({len(schema)}):\")\n",
    "        for field in schema:\n",
    "            print(f\"      ‚Ä¢ {field.name}: {field.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entities\n",
    "\n",
    "List all entities defined in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENTITIES\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Entity' object has no attribute 'join_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m entity_data = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[32m     10\u001b[39m     entity_data.append({\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mName\u001b[39m\u001b[33m\"\u001b[39m: entity.name,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mJoin Keys\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43mentity\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin_keys\u001b[49m),\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mValue Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(entity.value_type),\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m\"\u001b[39m: entity.description \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     })\n\u001b[32m     17\u001b[39m entities_df = pd.DataFrame(entity_data)\n\u001b[32m     18\u001b[39m display(entities_df)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Entity' object has no attribute 'join_keys'"
     ]
    }
   ],
   "source": [
    "# List all entities\n",
    "entities = store.list_entities()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "entity_data = []\n",
    "for entity in entities:\n",
    "    entity_data.append({\n",
    "        \"Name\": entity.name,\n",
    "        \"Join Keys\": \", \".join(entity.join_keys),\n",
    "        \"Value Type\": str(entity.value_type),\n",
    "        \"Description\": entity.description or \"N/A\"\n",
    "    })\n",
    "\n",
    "entities_df = pd.DataFrame(entity_data)\n",
    "display(entities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Sources\n",
    "\n",
    "List all data sources configured in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SOURCES\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Timestamp Field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kronodroid_categories_source</td>\n",
       "      <td>FileSource</td>\n",
       "      <td>_dbt_loaded_at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kronodroid_training_source</td>\n",
       "      <td>FileSource</td>\n",
       "      <td>event_timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kronodroid_samples_source</td>\n",
       "      <td>FileSource</td>\n",
       "      <td>event_timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kronodroid_push_source</td>\n",
       "      <td>PushSource</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name        Type  Timestamp Field\n",
       "0  kronodroid_categories_source  FileSource   _dbt_loaded_at\n",
       "1    kronodroid_training_source  FileSource  event_timestamp\n",
       "2     kronodroid_samples_source  FileSource  event_timestamp\n",
       "3        kronodroid_push_source  PushSource                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List all data sources\n",
    "data_sources = store.list_data_sources()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SOURCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "source_data = []\n",
    "for source in data_sources:\n",
    "    source_info = {\n",
    "        \"Name\": source.name,\n",
    "        \"Type\": type(source).__name__,\n",
    "    }\n",
    "    \n",
    "    # Add table info for SparkSource\n",
    "    if hasattr(source, 'table'):\n",
    "        source_info[\"Table\"] = source.table\n",
    "    \n",
    "    # Add timestamp field\n",
    "    if hasattr(source, 'timestamp_field'):\n",
    "        source_info[\"Timestamp Field\"] = source.timestamp_field\n",
    "    \n",
    "    source_data.append(source_info)\n",
    "\n",
    "sources_df = pd.DataFrame(source_data)\n",
    "display(sources_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Views Summary Table\n",
    "\n",
    "Create a summary table of all feature views with their key attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Feature Views Summary Table\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Entities</th>\n",
       "      <th># Features</th>\n",
       "      <th>TTL</th>\n",
       "      <th>Online</th>\n",
       "      <th>Source</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malware_batch_features</td>\n",
       "      <td>FeatureView</td>\n",
       "      <td>malware_sample</td>\n",
       "      <td>4</td>\n",
       "      <td>365 days, 0:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>kronodroid_training_source</td>\n",
       "      <td>usage=training, dataset=kronodroid, team=dfp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malware_sample_features</td>\n",
       "      <td>FeatureView</td>\n",
       "      <td>malware_sample</td>\n",
       "      <td>5</td>\n",
       "      <td>365 days, 0:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>kronodroid_training_source</td>\n",
       "      <td>dataset=kronodroid, team=dfp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>malware_batch_features</td>\n",
       "      <td>BatchFeatureView</td>\n",
       "      <td>malware_sample</td>\n",
       "      <td>4</td>\n",
       "      <td>365 days, 0:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>kronodroid_training_source</td>\n",
       "      <td>usage=training, dataset=kronodroid, team=dfp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>malware_sample_features</td>\n",
       "      <td>BatchFeatureView</td>\n",
       "      <td>malware_sample</td>\n",
       "      <td>5</td>\n",
       "      <td>365 days, 0:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>kronodroid_training_source</td>\n",
       "      <td>dataset=kronodroid, team=dfp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>malware_derived_features</td>\n",
       "      <td>OnDemandFeatureView</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>True</td>\n",
       "      <td>malware_sample_features</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name                 Type        Entities  # Features  \\\n",
       "0    malware_batch_features          FeatureView  malware_sample           4   \n",
       "1   malware_sample_features          FeatureView  malware_sample           5   \n",
       "2    malware_batch_features     BatchFeatureView  malware_sample           4   \n",
       "3   malware_sample_features     BatchFeatureView  malware_sample           5   \n",
       "4  malware_derived_features  OnDemandFeatureView             N/A           2   \n",
       "\n",
       "                 TTL  Online                      Source  \\\n",
       "0  365 days, 0:00:00   False  kronodroid_training_source   \n",
       "1  365 days, 0:00:00    True  kronodroid_training_source   \n",
       "2  365 days, 0:00:00   False  kronodroid_training_source   \n",
       "3  365 days, 0:00:00    True  kronodroid_training_source   \n",
       "4                N/A    True     malware_sample_features   \n",
       "\n",
       "                                           Tags  \n",
       "0  usage=training, dataset=kronodroid, team=dfp  \n",
       "1                  dataset=kronodroid, team=dfp  \n",
       "2  usage=training, dataset=kronodroid, team=dfp  \n",
       "3                  dataset=kronodroid, team=dfp  \n",
       "4                                           N/A  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_fv_summary(fv, fv_type=\"FeatureView\"):\n",
    "    \"\"\"Extract summary info from a feature view.\"\"\"\n",
    "    schema = getattr(fv, 'schema', [])\n",
    "    entities = [e.name if hasattr(e, 'name') else str(e) for e in fv.entities] if hasattr(fv, 'entities') else []\n",
    "    tags = getattr(fv, 'tags', {})\n",
    "    source = getattr(fv, 'batch_source', getattr(fv, 'source', None))\n",
    "    source_name = getattr(source, 'name', 'N/A') if source else 'N/A'\n",
    "    \n",
    "    return {\n",
    "        \"Name\": fv.name,\n",
    "        \"Type\": fv_type,\n",
    "        \"Entities\": \", \".join(entities),\n",
    "        \"# Features\": len(schema),\n",
    "        \"TTL\": str(getattr(fv, 'ttl', 'N/A')),\n",
    "        \"Online\": getattr(fv, 'online', 'N/A'),\n",
    "        \"Source\": source_name,\n",
    "        \"Tags\": \", \".join(f\"{k}={v}\" for k, v in tags.items()) if tags else \"N/A\"\n",
    "    }\n",
    "\n",
    "# Collect all feature views\n",
    "all_fv_data = []\n",
    "\n",
    "for fv in feature_views:\n",
    "    all_fv_data.append(get_fv_summary(fv, \"FeatureView\"))\n",
    "\n",
    "for fv in batch_feature_views:\n",
    "    all_fv_data.append(get_fv_summary(fv, \"BatchFeatureView\"))\n",
    "\n",
    "for odfv in on_demand_feature_views:\n",
    "    sources = list(odfv.source_feature_view_projections.keys())\n",
    "    schema = getattr(odfv, 'schema', [])\n",
    "    all_fv_data.append({\n",
    "        \"Name\": odfv.name,\n",
    "        \"Type\": \"OnDemandFeatureView\",\n",
    "        \"Entities\": \"N/A\",\n",
    "        \"# Features\": len(schema),\n",
    "        \"TTL\": \"N/A\",\n",
    "        \"Online\": True,\n",
    "        \"Source\": \", \".join(sources),\n",
    "        \"Tags\": \"N/A\"\n",
    "    })\n",
    "\n",
    "fv_summary_df = pd.DataFrame(all_fv_data)\n",
    "print(\"\\nüìã Feature Views Summary Table\")\n",
    "print(\"=\"*80)\n",
    "display(fv_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Query Feature View with Spark (Example)\n",
    "\n",
    "Demonstrate how to fetch historical features using the Spark offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entity DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_001</td>\n",
       "      <td>2025-12-13 15:44:17.676131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_002</td>\n",
       "      <td>2025-12-12 15:44:17.676154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_003</td>\n",
       "      <td>2025-12-11 15:44:17.676156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id            event_timestamp\n",
       "0  sample_001 2025-12-13 15:44:17.676131\n",
       "1  sample_002 2025-12-12 15:44:17.676154\n",
       "2  sample_003 2025-12-11 15:44:17.676156"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a sample entity DataFrame for historical feature retrieval\n",
    "# This would typically come from your application data\n",
    "entity_df = pd.DataFrame({\n",
    "    \"sample_id\": [\"sample_001\", \"sample_002\", \"sample_003\"],\n",
    "    \"event_timestamp\": [\n",
    "        datetime.now() - timedelta(days=1),\n",
    "        datetime.now() - timedelta(days=2),\n",
    "        datetime.now() - timedelta(days=3),\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Sample entity DataFrame:\")\n",
    "display(entity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Uncomment to fetch historical features (requires running infrastructure)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This uses the Spark offline store configured in feature_store.yaml\u001b[39;00m\n\u001b[32m      4\u001b[39m feature_refs = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmalware_sample_features:app_package\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmalware_sample_features:is_malware\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmalware_sample_features:data_source\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmalware_sample_features:dataset_split\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m training_df = \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_historical_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m.to_df()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHistorical features retrieved via Spark:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m display(training_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mlops/.venv/lib/python3.13/site-packages/feast/feature_store.py:1163\u001b[39m, in \u001b[36mFeatureStore.get_historical_features\u001b[39m\u001b[34m(self, entity_df, features, full_feature_names)\u001b[39m\n\u001b[32m   1160\u001b[39m utils._validate_feature_refs(_feature_refs, full_feature_names)\n\u001b[32m   1161\u001b[39m provider = \u001b[38;5;28mself\u001b[39m._get_provider()\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m job = \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_historical_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_views\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_feature_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_feature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mlops/.venv/lib/python3.13/site-packages/feast/infra/passthrough_provider.py:428\u001b[39m, in \u001b[36mPassthroughProvider.get_historical_features\u001b[39m\u001b[34m(self, config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_historical_features\u001b[39m(\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    420\u001b[39m     config: RepoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m    426\u001b[39m     full_feature_names: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    427\u001b[39m ) -> RetrievalJob:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     job = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moffline_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_historical_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_views\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_views\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_feature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_feature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mlops/.venv/lib/python3.13/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:130\u001b[39m, in \u001b[36mSparkOfflineStore.get_historical_features\u001b[39m\u001b[34m(config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config.offline_store, SparkOfflineStoreConfig)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fv \u001b[38;5;129;01min\u001b[39;00m feature_views:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fv.batch_source, SparkSource)\n\u001b[32m    132\u001b[39m warnings.warn(\n\u001b[32m    133\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe spark offline store is an experimental feature in alpha development. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSome functionality may still be unstable so functionality can change in the future.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    135\u001b[39m     \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[32m    136\u001b[39m )\n\u001b[32m    138\u001b[39m spark_session = get_spark_session_or_start_new_with_repoconfig(\n\u001b[32m    139\u001b[39m     store_config=config.offline_store\n\u001b[32m    140\u001b[39m )\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Uncomment to fetch historical features (requires running infrastructure)\n",
    "# This uses the Spark offline store configured in feature_store.yaml\n",
    "\n",
    "feature_refs = [\n",
    "    \"malware_sample_features:app_package\",\n",
    "    \"malware_sample_features:is_malware\",\n",
    "    \"malware_sample_features:data_source\",\n",
    "    \"malware_sample_features:dataset_split\",\n",
    "]\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_refs,\n",
    ").to_df()\n",
    "\n",
    "print(\"Historical features retrieved via Spark:\")\n",
    "display(training_df)\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Historical feature retrieval is commented out.\")\n",
    "print(\"   Uncomment the code above when infrastructure (LakeFS, Spark, Iceberg) is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Registry Inspection\n",
    "\n",
    "Inspect the Feast registry directly for additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "REGISTRY INFORMATION\n",
      "============================================================\n",
      "\n",
      "Project: dfp\n",
      "Provider: local\n",
      "\n",
      "Offline Store Configuration:\n",
      "  Type: spark\n",
      "\n",
      "Spark Configuration:\n",
      "    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "    spark.sql.catalog.lakefs_catalog: org.apache.iceberg.spark.SparkCatalog\n",
      "    spark.sql.catalog.lakefs_catalog.type: hadoop\n",
      "    spark.sql.catalog.lakefs_catalog.warehouse: ${LAKEFS_WAREHOUSE:-s3a://kronodroid/main/iceberg}\n",
      "    spark.hadoop.fs.s3a.endpoint: ${LAKEFS_ENDPOINT_URL:-http://localhost:8000}\n",
      "    spark.hadoop.fs.s3a.access.key: ***\n",
      "    spark.hadoop.fs.s3a.secret.key: ***\n",
      "    spark.hadoop.fs.s3a.path.style.access: true\n",
      "    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "    spark.hadoop.fs.s3a.connection.ssl.enabled: false\n",
      "    spark.sql.iceberg.write.format.default: avro\n",
      "    spark.driver.memory: 2g\n",
      "    spark.executor.memory: 2g\n",
      "\n",
      "Online Store Configuration:\n",
      "  Type: redis\n"
     ]
    }
   ],
   "source": [
    "# Get registry information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGISTRY INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nProject: {store.project}\")\n",
    "print(f\"Provider: {store.config.provider}\")\n",
    "print(f\"\\nOffline Store Configuration:\")\n",
    "print(f\"  Type: {store.config.offline_store.type}\")\n",
    "\n",
    "# Show Spark configuration from the offline store\n",
    "if hasattr(store.config.offline_store, 'spark_conf'):\n",
    "    print(f\"\\nSpark Configuration:\")\n",
    "    for key, value in store.config.offline_store.spark_conf.items():\n",
    "        # Mask sensitive values\n",
    "        if 'secret' in key.lower() or 'password' in key.lower() or 'key' in key.lower():\n",
    "            print(f\"    {key}: ***\")\n",
    "        else:\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "print(f\"\\nOnline Store Configuration:\")\n",
    "print(f\"  Type: {store.config.online_store.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "# Uncomment if you want to stop the session\n",
    "spark.stop()\n",
    "\n",
    "print(\"\\n‚úÖ Notebook complete!\")\n",
    "print(\"   Spark session is still active. Call spark.stop() when finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
