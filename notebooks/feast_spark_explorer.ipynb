{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feast Feature Store Explorer with Spark Backend\n",
    "\n",
    "This notebook demonstrates how to query and explore the Feast feature store configured with a Spark offline backend and Iceberg tables on LakeFS.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "dlt (Kaggle) ‚Üí Avro ‚Üí MinIO ‚Üí Spark ‚Üí Iceberg (LakeFS) ‚Üí Feast\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set environment variables for local development (adjust as needed)\n",
    "os.environ.setdefault(\"LAKEFS_ENDPOINT_URL\", \"http://localhost:8000\")\n",
    "os.environ.setdefault(\"LAKEFS_ACCESS_KEY_ID\", \"AKIAIOSFOLQUICKSTART\")\n",
    "os.environ.setdefault(\"LAKEFS_SECRET_ACCESS_KEY\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\n",
    "os.environ.setdefault(\"LAKEFS_REPOSITORY\", \"kronodroid\")\n",
    "os.environ.setdefault(\"LAKEFS_BRANCH\", \"main\")\n",
    "os.environ.setdefault(\"REDIS_CONNECTION_STRING\", \"redis://localhost:16379\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"LakeFS endpoint: {os.environ['LAKEFS_ENDPOINT_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session\n",
    "\n",
    "Create a Spark session configured for Iceberg + LakeFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engines.spark_engine.dfp_spark.session import get_spark_session, SparkConfig\n",
    "\n",
    "# Create Spark session with Iceberg + LakeFS configuration\n",
    "spark_config = SparkConfig(\n",
    "    app_name=\"feast_explorer\",\n",
    "    driver_memory=\"2g\",\n",
    "    executor_memory=\"2g\",\n",
    ")\n",
    "\n",
    "spark = get_spark_session(config=spark_config)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark app name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Feast Feature Store\n",
    "\n",
    "Connect to the Feast feature store with Spark offline store configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# Path to feast feature_store.yaml\n",
    "feast_repo_path = project_root / \"feature_stores\" / \"feast_store\"\n",
    "\n",
    "# Initialize the feature store\n",
    "store = FeatureStore(repo_path=str(feast_repo_path))\n",
    "\n",
    "print(f\"Feast project: {store.project}\")\n",
    "print(f\"Registry path: {store.config.registry}\")\n",
    "print(f\"Offline store type: {store.config.offline_store.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List All Feature Views\n",
    "\n",
    "Display all registered feature views in the Feast registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all feature views\n",
    "feature_views = store.list_feature_views()\n",
    "batch_feature_views = store.list_batch_feature_views()\n",
    "on_demand_feature_views = store.list_on_demand_feature_views()\n",
    "\n",
    "print(f\"\\nüìä Feature Views Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Regular Feature Views: {len(feature_views)}\")\n",
    "print(f\"Batch Feature Views: {len(batch_feature_views)}\")\n",
    "print(f\"On-Demand Feature Views: {len(on_demand_feature_views)}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Views Details\n",
    "\n",
    "Display detailed information about each feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_view_details(fv):\n",
    "    \"\"\"Display detailed information about a feature view.\"\"\"\n",
    "    print(f\"\\nüîπ Feature View: {fv.name}\")\n",
    "    print(f\"   {'‚îÄ'*45}\")\n",
    "    \n",
    "    # Entities\n",
    "    entity_names = [e.name if hasattr(e, 'name') else str(e) for e in fv.entities]\n",
    "    print(f\"   Entities: {', '.join(entity_names)}\")\n",
    "    \n",
    "    # TTL\n",
    "    print(f\"   TTL: {fv.ttl}\")\n",
    "    \n",
    "    # Online serving\n",
    "    online = getattr(fv, 'online', 'N/A')\n",
    "    print(f\"   Online: {online}\")\n",
    "    \n",
    "    # Tags\n",
    "    tags = getattr(fv, 'tags', {})\n",
    "    if tags:\n",
    "        print(f\"   Tags: {tags}\")\n",
    "    \n",
    "    # Source\n",
    "    source = getattr(fv, 'batch_source', getattr(fv, 'source', None))\n",
    "    if source:\n",
    "        source_name = getattr(source, 'name', type(source).__name__)\n",
    "        print(f\"   Source: {source_name}\")\n",
    "        if hasattr(source, 'table'):\n",
    "            print(f\"   Table: {source.table}\")\n",
    "    \n",
    "    # Schema/Features\n",
    "    schema = getattr(fv, 'schema', [])\n",
    "    if schema:\n",
    "        print(f\"   Features ({len(schema)}):\")\n",
    "        for field in schema:\n",
    "            desc = getattr(field, 'description', '')\n",
    "            desc_str = f\" - {desc}\" if desc else \"\"\n",
    "            print(f\"      ‚Ä¢ {field.name}: {field.dtype}{desc_str}\")\n",
    "\n",
    "# Display regular feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGULAR FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for fv in feature_views:\n",
    "    display_feature_view_details(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for fv in batch_feature_views:\n",
    "    display_feature_view_details(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display on-demand feature views\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ON-DEMAND FEATURE VIEWS\")\n",
    "print(\"=\"*60)\n",
    "for odfv in on_demand_feature_views:\n",
    "    print(f\"\\nüî∏ On-Demand Feature View: {odfv.name}\")\n",
    "    print(f\"   {'‚îÄ'*45}\")\n",
    "    \n",
    "    # Source feature views\n",
    "    sources = list(odfv.source_feature_view_projections.keys())\n",
    "    print(f\"   Source FVs: {', '.join(sources)}\")\n",
    "    \n",
    "    # Schema\n",
    "    schema = getattr(odfv, 'schema', [])\n",
    "    if schema:\n",
    "        print(f\"   Computed Features ({len(schema)}):\")\n",
    "        for field in schema:\n",
    "            print(f\"      ‚Ä¢ {field.name}: {field.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entities\n",
    "\n",
    "List all entities defined in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all entities\n",
    "entities = store.list_entities()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "entity_data = []\n",
    "for entity in entities:\n",
    "    entity_data.append({\n",
    "        \"Name\": entity.name,\n",
    "        \"Join Keys\": \", \".join(entity.join_keys),\n",
    "        \"Value Type\": str(entity.value_type),\n",
    "        \"Description\": entity.description or \"N/A\"\n",
    "    })\n",
    "\n",
    "entities_df = pd.DataFrame(entity_data)\n",
    "display(entities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Sources\n",
    "\n",
    "List all data sources configured in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data sources\n",
    "data_sources = store.list_data_sources()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SOURCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "source_data = []\n",
    "for source in data_sources:\n",
    "    source_info = {\n",
    "        \"Name\": source.name,\n",
    "        \"Type\": type(source).__name__,\n",
    "    }\n",
    "    \n",
    "    # Add table info for SparkSource\n",
    "    if hasattr(source, 'table'):\n",
    "        source_info[\"Table\"] = source.table\n",
    "    \n",
    "    # Add timestamp field\n",
    "    if hasattr(source, 'timestamp_field'):\n",
    "        source_info[\"Timestamp Field\"] = source.timestamp_field\n",
    "    \n",
    "    source_data.append(source_info)\n",
    "\n",
    "sources_df = pd.DataFrame(source_data)\n",
    "display(sources_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Views Summary Table\n",
    "\n",
    "Create a summary table of all feature views with their key attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fv_summary(fv, fv_type=\"FeatureView\"):\n",
    "    \"\"\"Extract summary info from a feature view.\"\"\"\n",
    "    schema = getattr(fv, 'schema', [])\n",
    "    entities = [e.name if hasattr(e, 'name') else str(e) for e in fv.entities] if hasattr(fv, 'entities') else []\n",
    "    tags = getattr(fv, 'tags', {})\n",
    "    source = getattr(fv, 'batch_source', getattr(fv, 'source', None))\n",
    "    source_name = getattr(source, 'name', 'N/A') if source else 'N/A'\n",
    "    \n",
    "    return {\n",
    "        \"Name\": fv.name,\n",
    "        \"Type\": fv_type,\n",
    "        \"Entities\": \", \".join(entities),\n",
    "        \"# Features\": len(schema),\n",
    "        \"TTL\": str(getattr(fv, 'ttl', 'N/A')),\n",
    "        \"Online\": getattr(fv, 'online', 'N/A'),\n",
    "        \"Source\": source_name,\n",
    "        \"Tags\": \", \".join(f\"{k}={v}\" for k, v in tags.items()) if tags else \"N/A\"\n",
    "    }\n",
    "\n",
    "# Collect all feature views\n",
    "all_fv_data = []\n",
    "\n",
    "for fv in feature_views:\n",
    "    all_fv_data.append(get_fv_summary(fv, \"FeatureView\"))\n",
    "\n",
    "for fv in batch_feature_views:\n",
    "    all_fv_data.append(get_fv_summary(fv, \"BatchFeatureView\"))\n",
    "\n",
    "for odfv in on_demand_feature_views:\n",
    "    sources = list(odfv.source_feature_view_projections.keys())\n",
    "    schema = getattr(odfv, 'schema', [])\n",
    "    all_fv_data.append({\n",
    "        \"Name\": odfv.name,\n",
    "        \"Type\": \"OnDemandFeatureView\",\n",
    "        \"Entities\": \"N/A\",\n",
    "        \"# Features\": len(schema),\n",
    "        \"TTL\": \"N/A\",\n",
    "        \"Online\": True,\n",
    "        \"Source\": \", \".join(sources),\n",
    "        \"Tags\": \"N/A\"\n",
    "    })\n",
    "\n",
    "fv_summary_df = pd.DataFrame(all_fv_data)\n",
    "print(\"\\nüìã Feature Views Summary Table\")\n",
    "print(\"=\"*80)\n",
    "display(fv_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Query Feature View with Spark (Example)\n",
    "\n",
    "Demonstrate how to fetch historical features using the Spark offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a sample entity DataFrame for historical feature retrieval\n",
    "# This would typically come from your application data\n",
    "entity_df = pd.DataFrame({\n",
    "    \"sample_id\": [\"sample_001\", \"sample_002\", \"sample_003\"],\n",
    "    \"event_timestamp\": [\n",
    "        datetime.now() - timedelta(days=1),\n",
    "        datetime.now() - timedelta(days=2),\n",
    "        datetime.now() - timedelta(days=3),\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Sample entity DataFrame:\")\n",
    "display(entity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to fetch historical features (requires running infrastructure)\n",
    "# This uses the Spark offline store configured in feature_store.yaml\n",
    "\n",
    "# feature_refs = [\n",
    "#     \"malware_sample_features:app_package\",\n",
    "#     \"malware_sample_features:is_malware\",\n",
    "#     \"malware_sample_features:data_source\",\n",
    "#     \"malware_sample_features:dataset_split\",\n",
    "# ]\n",
    "\n",
    "# training_df = store.get_historical_features(\n",
    "#     entity_df=entity_df,\n",
    "#     features=feature_refs,\n",
    "# ).to_df()\n",
    "\n",
    "# print(\"Historical features retrieved via Spark:\")\n",
    "# display(training_df)\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Historical feature retrieval is commented out.\")\n",
    "print(\"   Uncomment the code above when infrastructure (LakeFS, Spark, Iceberg) is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Registry Inspection\n",
    "\n",
    "Inspect the Feast registry directly for additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get registry information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGISTRY INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nProject: {store.project}\")\n",
    "print(f\"Provider: {store.config.provider}\")\n",
    "print(f\"\\nOffline Store Configuration:\")\n",
    "print(f\"  Type: {store.config.offline_store.type}\")\n",
    "\n",
    "# Show Spark configuration from the offline store\n",
    "if hasattr(store.config.offline_store, 'spark_conf'):\n",
    "    print(f\"\\nSpark Configuration:\")\n",
    "    for key, value in store.config.offline_store.spark_conf.items():\n",
    "        # Mask sensitive values\n",
    "        if 'secret' in key.lower() or 'password' in key.lower() or 'key' in key.lower():\n",
    "            print(f\"    {key}: ***\")\n",
    "        else:\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "print(f\"\\nOnline Store Configuration:\")\n",
    "print(f\"  Type: {store.config.online_store.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "# Uncomment if you want to stop the session\n",
    "# spark.stop()\n",
    "\n",
    "print(\"\\n‚úÖ Notebook complete!\")\n",
    "print(\"   Spark session is still active. Call spark.stop() when finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
