# Spark image with Kronodroid Iceberg job
#
# Build:
#   docker build -t dfp-spark:latest -f tools/docker/Dockerfile.spark .
#
# For Kind clusters:
#   kind load docker-image dfp-spark:latest --name dfp-kind

# NOTE: `apache/spark-py` is not a published image on Docker Hub.
# This repo already standardizes on the official Spark image tag below.
FROM apache/spark:3.5.0-python3

USER root

# Create Ivy cache directory for Maven dependency resolution at runtime
# The spark user needs write access to this directory
RUN mkdir -p /home/spark/.ivy2/cache /home/spark/.ivy2/jars && \
    chown -R spark:spark /home/spark/.ivy2

# Pre-download Iceberg and S3 dependencies to avoid version mismatches between driver/executor
# All pods will use the same JARs from the image
RUN curl -L -o /opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar \
    https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar && \
    curl -L -o /opt/spark/jars/hadoop-aws-3.3.4.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    curl -L -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

# Install uv for fast, reproducible Python package installation
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Copy project files for dependency resolution
COPY pyproject.toml /opt/spark/work-dir/

# Install additional Python dependencies using uv
RUN uv pip install --system --no-cache \
    pyspark==3.5.0 \
    "pyarrow>=14.0.0" \
    "requests>=2.31.0"

# Copy the Spark job and supporting modules
COPY engines/spark_engine/dfp_spark/kronodroid_iceberg_job.py /opt/spark/work-dir/
COPY engines/spark_engine/dfp_spark/session.py /opt/spark/work-dir/

# Set working directory
WORKDIR /opt/spark/work-dir

# Switch back to spark user
USER spark

# Default entrypoint from base image handles spark-submit
