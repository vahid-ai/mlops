# KFP Training Component Image
#
# Pre-built image for Kubeflow Pipeline training components with all
# dependencies installed. Eliminates runtime pip installs for faster
# startup and better log visibility.
#
# Build:
#   docker build -t dfp-kfp-training:latest -f tools/docker/Dockerfile.kfp_training .
#
# For Kind clusters:
#   kind load docker-image dfp-kfp-training:latest --name dfp-kind
#
# Multi-arch build (for CI/registry):
#   docker buildx build --platform linux/amd64,linux/arm64 \
#     -t dfp-kfp-training:latest -f tools/docker/Dockerfile.kfp_training --push .

FROM apache/spark:3.5.0-python3

USER root

# Install system dependencies for building Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create directories with correct permissions
RUN mkdir -p /home/spark/.ivy2/cache /home/spark/.ivy2/jars \
    /home/spark/.cache/pip /home/spark/.local \
    && chown -R spark:spark /home/spark

# Pre-download Iceberg and S3 JARs for Spark
RUN curl -L -o /opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.4.3.jar \
    https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.4.3/iceberg-spark-runtime-3.5_2.12-1.4.3.jar && \
    curl -L -o /opt/spark/jars/hadoop-aws-3.3.4.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    curl -L -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

# Install Python dependencies for KFP training component
# These match the packages installed at runtime in the KFP component
# Note: kfp>=2.9.0 is compatible with Python 3.8 in the Spark base image
RUN pip install --no-cache-dir \
    'kfp>=2.9.0' \
    'mlflow>=2.0,<3.0' \
    'torch>=2.0,<3.0' \
    'lightning>=2.0,<3.0' \
    'feast[redis]>=0.32,<1.0' \
    'pyspark==3.5.0' \
    'pandas>=2.0,<3.0' \
    'numpy>=1.24,<2.0' \
    'pyarrow>=14.0,<18.0' \
    'requests>=2.31,<3.0' \
    'psutil>=5.9,<6.0' \
    'pyyaml>=6.0,<7.0' \
    'tensorboard>=2.10,<2.18' \
    'rich>=13.0,<14.0'

# Create feast data directory (for registry.db)
RUN mkdir -p /feast/data && chown -R spark:spark /feast

# Set working directory
WORKDIR /opt/spark/work-dir

# Switch back to spark user for security
USER spark

# Environment variables for MLflow S3 artifact storage
ENV MLFLOW_S3_IGNORE_TLS=true
ENV MLFLOW_S3_UPLOAD_EXTRA_ARGS='{"ACL": "bucket-owner-full-control"}'

# Default command (overridden by KFP)
CMD ["python3"]
